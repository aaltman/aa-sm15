Abstract<BR>Linearizable datastores are desirable because they provide users with the illusion that the datastore is run on a single ma- chine that performs client operations one at a time. To reduce the performance cost of providing this illusion, many special- ized algorithms for linearizable reads have been proposed which significantly improve read performance compared to write performance. The main difference between these specialized algorithms is their performance under different workloads. Unfortunately, since a datastore&#8217;s workload is often unknown or changes over time and system designers must decide on a single read algorithm to implement ahead of time, a datastore&#8217;s performance is often suboptimal as it cannot adapt to workload changes.<BR>In this paper, we lay the groundwork for addressing this problem by proposing Chameleon, an algorithm for linearizable reads that provides a principled approach for datastores to switch between existing read algorithms at runtime. The key observation that enables this generalization is that all existing algorithms are specific read-write quorum systems. Chameleon constructs a generic read-write quorum system, by using tokens that are included to complete write and read operations. This token quorum system enables Chameleon to mimic existing read algorithms and switch between them by transferring these tokens between processes. 
<P></P>
<P>&nbsp;&nbsp;&nbsp; 1 Introduction<BR>A datastore is linearizable if it provides users with the illu- sion that it is run on a single machine that processes client operations one at a time [15]. This property is desirable, as ap- plication developers do not need to reason about side effects that can arise from concurrent datastore operations [14, 21]. The de facto standard for providing linearizability is by use of a state machine replication algorithm [27]. These al- gorithms provide clients with procedures for both writing and reading the application&#8217;s state. While both write and read operations can be processed with the same procedure, the non-mutating property of read operations allows them to be processed concurrently without coordination between them. This observation has led to the development of many specialized read algorithms which significantly improve read<BR>These algorithms are of particular practical importance as many real-world workloads are read dominant [3, 9]. For example, Google reported that in their ads workload, read operations outnumber write operations by three orders of magnitude [10]. This prevalence makes read operation per- formance the primary determinant of end-user latency and overall system throughput.<BR>Existing read algorithms can be classified into one of four categories: leader [6], majority quorum [8, 31], flexible quo-<BR>rum [13, 30], and local [4, 5, 7, 17, 24]. The main difference between algorithms in these categories is their performance in different workloads. For example, in geo-distributed de- ployments, if most read operations are performed near a distinguished process known as the leader, average read la- tency will be significantly lower using leader reads compared to majority quorum reads. However, majority quorum reads can achieve higher peak throughput comparatively as the leader is not a bottleneck [2, 8]. Moreover, local reads provide the lowest read latency compared to algorithms in all other categories at the cost of increased write latency [24]. Finally, flexible quorum reads increase read latency but reduce write latency in the presence of failures or network partitions.<BR>These differences pose a serious challenge during the de- sign of a linearizable datastore when the expected workload is unknown ahead of time. Even in the unlikely scenario where it is known, designers can only implement a single read algorithm, and as such, performance will degrade if the workload changes. Consequently, to optimize performance when the workload is unknown ahead of time and to adapt to workload changes, a datastore needs to be able to switch between existing read algorithms at run-time.<BR>In this paper, we lay the groundwork for switching be- tween different read algorithms at run-time by presenting Chameleon, a generalized and reconfigurable algorithm for linearizable reads. The key observation that enables this generalization is that all existing algorithms are specific read- write quorum systems &#8212; their correctness is solely based on read and write operations contacting overlapping sets of pro- cesses. To guarantee read-write quorum intersection, each process is provided with a set of tokens that are included to complete both read and write operations. This token quorum</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Leader reads.&nbsp;(b) Majority quorum reads.&nbsp;(c) Flexible quorum reads.&nbsp;(d) Local reads.</P>
<P>Figure 1. Example read (dashed) and write (solid) quorums of existing algorithms for linearizable reads.</P>
<P>system enables Chameleon to mimic existing read algorithms and switch between them by strategically transferring these tokens between processes.<BR>In the remainder of this paper, we describe Chameleon in detail. Section 2 argues that all existing algorithms are specific read-write quorum systems. Section 3 describes the token quorum system and shows how it can mimic all exist- ing algorithms. Section 4 discusses reconfiguring the token quorum system and tolerating failures. Section 5 discusses related work, and Section 6 presents concluding remarks.<BR>&nbsp;&nbsp;&nbsp; 2 Preliminaries<BR>In this section, we discuss the system model, state machine replication, existing algorithms for linearizable reads, and their relationship to read-write quorum systems.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Model<BR>We assume an asynchronous system consisting of &#55349;&#56411; pro- cesses of which at most &#55349;&#56403; may crash where &#55349;&#56403; &lt; &#55349;&#56411; . In this system, messages passed between processes may be arbi- trarily delayed, reordered, or lost entirely. In addition to this model, we require processes to be able to grant leases in order to tolerate failures without sacrificing liveliness [12]. Specif- ically, we require that processes can grant leases correctly, that is, the lease granter&#8217;s perception of the lease&#8217;s expira- tion is guaranteed to occur after the holder&#8217;s perception of the lease&#8217;s expiration in real-time. This can be done by ei- ther assuming processes have access to synchronized clocks with bounded clock skew [12] or by assuming processes are equipped with hardware clocks with bounded drift [22].<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 State Machine Replication<BR>The de facto standard for providing linearizable, fault-tolerant replication of client operations across a set of processes is by use of a state machine replication (SMR) algorithm. Linearizability is a consistency model that ensures client operations are executed in some total order that respects their real-time ordering, i.e., if operation &#55349;&#56412;1 completes before operation &#55349;&#56412;2 begins, then &#55349;&#56412;1 must be ordered before &#55349;&#56412;2 [15].<BR>SMR algorithms provide fault-tolerance by maintaining a copy of the application&#8217;s state at each process known as their replica. Replicas are assumed to atomically apply client operations and be deterministic, as in, if two replicas apply the same sequence of operations, they reach the same state [27]. Clients perform operations by submitting them to processes within the system. Once a process receives such a request, it performs either the write or read procedure of the SMR algorithm depending on the submitted operation, and returns the result to the client. Intuitively, the result for a write operation signifies the operation has taken effect and is durable whereas the result of a read operation is the latest version of the application&#8217;s state.</P>
<P><BR><FONT class=extract>To provide the real-time ordering guarantee of linearizability, SMR algorithms often rely on a distinguished process known as the leader to sequence write operations. The leader does so in two phases: prepare and commit. The prepare phase consists of the leader proposing that a write operation &#55349;&#56420; be assigned to the &#55349;&#56406;-th location in the log. The prepare phase completes once a majority of processes accept this proposal ensuring it is decided. After this, &#55349;&#56420; can be committed, that is, a process can apply &#55349;&#56420; to their replica after they apply all write operations assigned up to index &#55349;&#56406;.<BR></FONT></P>
<P><FONT class=extract>Read operations are processed by assigning them to the index number of either the latest committed write operation or some concurrent write operation. Once a read operation has been assigned to some index number &#55349;&#56406;, it is completed by executing it against some replica that has applied all write operations with index numbers up to and including &#55349;&#56406;.</FONT></P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Algorithms for Linearizable Reads<BR>We now describe the four categories of read algorithms mentioned previously. </P>
<P><FONT class=extract>In leader reads all read operations are forwarded to the network&#8217;s leader for processing. When the leader receives a read request it assigns it to the highest index that it has sent a commit request for. This guarantees that read operations always observe the latest complete write operations so long as a new leader has not been elected in the meantime. To ensure this doesn&#8217;t happen, the leader is granted a leader lease, guaranteeing at most one process be- lieves they are the leader at any given time. Unlike leader reads, in majority quorum reads, all processes can perform read operations. They do so by assigning read operations to the maximum index a simple majority of processes have received a prepare request for. </FONT></P>
<P><FONT class=extract>Flexible quorum reads are similar to majority quorum reads except reads are assigned to the maximum index of an arbitrary read quorum of pro- cesses. To enable this flexibility and guarantee the real-time ordering requirement of linearizability, the leader waits to receive acknowledgments from at least one process in every read quorum before committing a write. Similarly to leader reads, to not reduce fault-tolerance, processes that are mem- bers of a read quorum are granted read leases. Finally, in local reads, all processes assign read operations to an index based on their local perception of the latest assigned write index. Similar to flexible quorum reads, the leader in local reads must receive acknowledgments from all processes that hold a read lease before a write is committed.</FONT></P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 <FONT class=extract>Read-Write Quorum Systems<BR>We now discuss the relationship between existing algorithms for linearizable reads and read-write quorum systems. We do so with the aid of the illustration in Figure 1. A read-write quorum system is defined by a set of read quorums and write quorums such that every read and write quorum intersects. In the context of specialized algorithms for linearizable reads, each read and write quorum is a subset of the &#55349;&#56411; processes in the system. In leader reads, the only read quorum is the leader and any simple majority of processes including the leader is a write quorum. Since at most one process is the leader at any time, every read and write quorum intersects. In majority quorum reads, read and write quorums are any simple majority of processes. Similarly, since any two simple majorities intersect so does every read and write quorum. Flexible quorum reads define the quorum system explicitly. Finally, in local reads, every process is a read quorum and there is a single write quorum containing all processes. Consequently, all existing algorithms for linearizable reads reduce to specific read-write quorum systems.</FONT></P>
<P><BR>&nbsp;&nbsp;&nbsp; 3 Chameleon<BR>We now explain how Chameleon constructs a read-write quorum system using tokens, followed by showing how this</P>
<P>enables Chameleon to mimic all existing specialized read al- gorithms. We then describe how Chameleon performs write and read operations assuming that messages aren&#8217;t lost, all processes are non-faulty, and the quorum system and leader are fixed followed by sketching why Chameleon is correct. We discuss relaxing these assumptions in Section 4.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 The Token Quorum System<BR>Each token is owned by one process and held by at most one process at any time. Note, a token&#8217;s owner never changes but its holder can. We say that a token is transferred from one process to another. A token is defined as a tuple &#55349;&#56409;, &#55349;&#56415; where<BR>&#55349;&#56409; is the token&#8217;s owner and &#55349;&#56415; is some unique integer used<BR>to differentiate between tokens owned by the same process. Using these tokens a read quorum is then defined by a set of processes that hold at least one token owned by some simple majority of processes. Similarly, a write quorum is defined by a set of processes which is at least a simple majority of processes that collectively hold every token owned by some (potentially different) simple majority of processes. Finally, we say that an operation included a token if it completes by contacting a quorum that held it.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Mimicking Existing Algorithms<BR>We now show how strategically assigning which processes hold which tokens enables Chameleon to mimic each of the existing read algorithms discussed in Section 2.<BR>To mimic leader reads, each process owns a single token which is held by the leader. This is illustrated in Figure 2a where the arrows represent the transferring of tokens owned by processes B, C, D, and E to A (the leader), and the number beside each process represents how many tokens it holds. In this scenario, the leader itself is a read quorum as it holds at least one token from a simple majority of processes. Further- more, a write quorum is any simple majority of processes that includes the leader &#8212; the only process that holds every token from a majority of processes.<BR>To mimic majority quorum reads each process owns a single token which is not transferred. Consequently, any simple majority of processes is both a read and write quorum<BR>&#8212; the same quorum system produced by majority quorum reads. We illustrate an example of this in Figure 2b.<BR>Chameleon mimics flexible quorum reads through a com- bination of the previous two approaches, as only one process needs to hold more than one token to achieve a read quorum smaller than a simple majority. We illustrate such an example in Figure 2c where process D holds two tokens. This enables D and processes A, C, or E to perform read operations. Fur- thermore, in this example, since a write must include every token owned by a simple majority of processes, it is bound to intersect D or A, C, and E. In either case, this guarantees read-write quorum intersection.<BR>Finally, Chameleon mimics local reads by each process owning &#55349;&#56411; tokens and transferring one token to all other pro- cesses. Consequently, each process will hold onto one token owned by every process in the system as we illustrate in Figure 2d. This results in each process being a read quorum and the only valid write quorum being all processes. Conse-<BR>quently, the token quorum system can mimic every existing specialized algorithm for linearizable reads.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Performing Client Operations<BR>We now discuss how Chameleon performs operations using the token quorum system.<BR>Writes (Algorithm 1). When a process wishes to perform a write operation, it first forwards it to the leader. Once the leader receives this request, it assigns the write operation to the next index in the log and sends a prepare request to all processes including the write operation and its assigned index number. Upon receipt of this request, a process records this index number if it is the highest they have received a prepare request for. It then sends an acknowledgment to the leader including a description of the current set of tokens they hold. In the system illustrated in Figure 2c, the set of tokens returned per process differs. For example, processes A, C, and E will inform the leader they hold their own token, process B will return saying they do not hold any tokens, and process D will return saying they hold both their own and B&#8217;s token. Once the leader has sent these prepare re- quests, it waits to receive acknowledgments from at least a simple majority of processes that collectively hold every to- ken owned by some (potentially different) simple majority of processes. In the system illustrated in Figure 2c, valid sets of acknowledging processes include (A, C, E), (A, D, E), and (C, D, E). Following this, the leader sends commit requests to all processes for this write and returns to the client signifying that the write is completed and durable.<BR>Reads (Algorithm 2). Similarly to write operations, clients submit read operations to any process. Once a process &#55349;&#56413; receives this request, it first computes the closest read quo- rum to it (&#55349;&#56389;) based on the tokens each process holds. If this quorum happens to be &#55349;&#56413; itself, then &#55349;&#56413; assigns the read to the highest index it has received a prepare request for. Otherwise,<BR>&#55349;&#56413; sends a read request to all processes in &#55349;&#56389;. For example, if<BR>1&nbsp;&nbsp;&nbsp; procedure write(&#55349;&#56412;):<BR>2&nbsp;&#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; &#10143; &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; + 1&nbsp;/* Local op counter initially 0 */<BR>/* &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; on lines 3 and 4 is the same as line 2&nbsp;*/<BR>3&nbsp;send &#10216;&#55349;&#56394; &#55349;&#56389;&#55349;&#56380;&#55349;&#56391; &#55349;&#56376;, &#55349;&#56412;, &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; &#10217; to &#8467;<BR>4&nbsp;wait to receive &#10216;&#55349;&#56394; &#55349;&#56389;&#55349;&#56380;&#55349;&#56391; &#55349;&#56376;_&#55349;&#56372;&#55349;&#56374;&#55349;&#56382;, &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; &#10217; from &#8467;<BR>5&nbsp; upon receiving &#10216;&#55349;&#56394; &#55349;&#56389;&#55349;&#56380;&#55349;&#56391; &#55349;&#56376;, &#55349;&#56412;, &#55349;&#56400;&#10217; from &#55349;&#56413;:<BR>6&nbsp;&#55349;&#56406; &#8788; &#55349;&#56406; + 1&nbsp;/* Latest log index initially 0 */<BR>7&nbsp;send &#10216;&#55349;&#56387;&#55349;&#56389;&#55349;&#56376;&#55349;&#56387;&#55349;&#56372;&#55349;&#56389;&#55349;&#56376;, &#55349;&#56406;&#10217; to all<BR>/* Processes that have acknowledged, the tokens they have returned, and the current include.&nbsp;*/<BR>8&nbsp;&#55349;&#56372; &#10143; &#55349;&#56391; &#55349;&#56389; &#10143; &#55349;&#56391; &#55349;&#56380; &#10143; &#8709;<BR>9&nbsp;upon receiving &#10216;&#55349;&#56387;_&#55349;&#56372;&#55349;&#56374;&#55349;&#56382;, &#55349;&#56406;, &#55349;&#56391; &#8242;&#10217; from &#55349;&#56414;:<BR>10&nbsp;&#55349;&#56372; &#10143; &#55349;&#56372; &#8746; {&#55349;&#56414;} and &#55349;&#56391; &#55349;&#56389; &#8788; &#55349;&#56391; &#55349;&#56389; &#8746; &#55349;&#56391; &#8242;<BR>11&nbsp;foreach process &#55349;&#56415; in the system do<BR>12&nbsp;if &#55349;&#56391; &#55349;&#56389; contains &#55349;&#56408; tokens owned by &#55349;&#56415; then<BR>13&nbsp;&#55349;&#56391; &#55349;&#56380; &#10143; &#55349;&#56391; &#55349;&#56380; &#8746; {&#55349;&#56415; }<BR>14&nbsp;wait for |&#55349;&#56372;|&nbsp; &#8805;&nbsp; &#8968;&#55349;&#56411;2+1 &#8969; and |&#55349;&#56391; &#55349;&#56380; |&nbsp; &#8805;&nbsp; &#8968;&#55349;&#56411;2+1 &#8969;<BR>15&nbsp;send &#10216;&#55349;&#56374;&#55349;&#56386;&#55349;&#56384;&#55349;&#56384;&#55349;&#56380;&#55349;&#56391; , &#55349;&#56406;, &#55349;&#56412;&#10217; to all<BR>16&nbsp;send &#10216;&#55349;&#56394; &#55349;&#56389;&#55349;&#56380;&#55349;&#56391; &#55349;&#56376;_&#55349;&#56372;&#55349;&#56374;&#55349;&#56382;, &#55349;&#56400;&#10217; to &#55349;&#56413;<BR>17&nbsp; upon receiving &#55349;&#56387;&#55349;&#56389;&#55349;&#56376;&#55349;&#56387;&#55349;&#56372;&#55349;&#56389;&#55349;&#56376;, &#55349;&#56406; from &#8467;:<BR>/* Maximum prepare index received initially 0&nbsp;*/<BR>18&nbsp;&#55349;&#56384;&#55349;&#56398;&#55349;&#56421;&#55349;&#56387; &#10143; max(&#55349;&#56384;&#55349;&#56398;&#55349;&#56421;&#55349;&#56387;, &#55349;&#56406;)<BR>/* &#55349;&#56391; is the set of held tokens.&nbsp;*/<BR>19&nbsp;send &#10216;&#55349;&#56387;_&#55349;&#56372;&#55349;&#56374;&#55349;&#56382;, &#55349;&#56406;, &#55349;&#56391; &#10217; to &#8467;<BR>20&nbsp;&nbsp; upon receiving &#55349;&#56374;&#55349;&#56386;&#55349;&#56384;&#55349;&#56384;&#55349;&#56380;&#55349;&#56391; , &#55349;&#56406;, &#55349;&#56412; from &#8467;:<BR>/* Applies the write &#55349;&#56412; against the local replica once all writes up to &#55349;&#56406; &#8722; 1 have been applied&nbsp;*/<BR>21&nbsp;Write(&#55349;&#56406;, &#55349;&#56412;)<BR>Algorithm 1: Write pseudocode for Chameleon assum- ing no message loss, all processes are non-faulty, a fixed leader &#8467;, each process holds a fixed set of tokens &#55349;&#56391; , and each process owns &#55349;&#56408; tokens.</P>
<P>process &#55349;&#56413; is process A in Figure 2c possible &#55349;&#56389;&#8217;s include (A, C, E), (A, D), (C, D), and (D, E). Once a process receives a read request it returns the highest index it has received a prepare request for and the set of tokens it currently holds. Once &#55349;&#56413; has sent these read requests, it waits to receive ac- knowledgments from a set of processes that collectively hold at least one token owned by some simple majority of pro- cesses. Following this, &#55349;&#56413; assigns the read operation to the maximum index returned from this set of processes. In either case, &#55349;&#56413; then completes the read operation by (1) waiting for its replica to commit all write operations up to and including the assigned index, (2) executing the read operation against its local replica, and (3) returning the result to the client.<BR>Sending read requests to the closest read quorum is similar to the thrifty optimization of Paxos [20] which only sends prepare messages to a majority of processes. This reduces the number of messages sent by the leader but increases tail latency when processes don&#8217;t respond. To avoid this<BR>trade-off, &#55349;&#56389; can be replaced with all processes in the system.<BR>1&nbsp;&nbsp; procedure read(&#55349;&#56412;):<BR>2&nbsp;&#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; &#10143; &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415;&nbsp;1 and &#55349;&#56406;&#55349;&#56411;&#55349;&#56401;&#55349;&#56402;&#55349;&#56421; &#10143; 0<BR>3&nbsp;&#55349;&#56389; &#10143; closest_read_quorum()<BR>4&nbsp;if &#55349;&#56389; is only the current process then<BR>5&nbsp;&#55349;&#56406;&#55349;&#56411;&#55349;&#56401;&#55349;&#56402;&#55349;&#56421; &#10143; &#55349;&#56384;&#55349;&#56398;&#55349;&#56421;&#55349;&#56387;<BR>6&nbsp;else<BR>/* &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; on lines 7 and 9 is the same as line 2&nbsp;*/<BR>7&nbsp;send &#10216;&#55349;&#56389;&#55349;&#56376;&#55349;&#56372;&#55349;&#56375;, &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415; &#10217; to all processes in &#55349;&#56389;<BR>8&nbsp;&#55349;&#56391; &#55349;&#56380; &#10143; &#8709;<BR>9&nbsp;upon receiving &#10216;&#55349;&#56389;_&#55349;&#56372;&#55349;&#56374;&#55349;&#56382;, &#55349;&#56400;&#55349;&#56411;&#55349;&#56417;&#55349;&#56415;, &#55349;&#56391; &#8242;, &#55349;&#56384;&#55349;&#56398;&#55349;&#56421;&#55349;&#56387; &#8242;&#10217;:<BR>10&nbsp;&#55349;&#56406;&#55349;&#56411;&#55349;&#56401;&#55349;&#56402;&#55349;&#56421; &#10143; max(&#55349;&#56406;&#55349;&#56411;&#55349;&#56401;&#55349;&#56402;&#55349;&#56421;, &#55349;&#56384;&#55349;&#56398;&#55349;&#56421;&#55349;&#56387; &#8242;)<BR>11&nbsp;foreach process &#55349;&#56413; in the system do<BR>12&nbsp;if (&#55349;&#56413;, &#8727;) &#8712; &#55349;&#56391; &#8242; then &#55349;&#56391; &#55349;&#56380; &#10143; &#55349;&#56391; &#55349;&#56380; &#8746; {&#55349;&#56413;}<BR>13&nbsp;wait for&nbsp; &#55349;&#56391; &#55349;&#56380;&nbsp;&#55349;&#56411;2+1<BR>/* Executes the read request &#55349;&#56412; against the local<BR>replica once all writes up to and including &#55349;&#56406;&#55349;&#56411;&#55349;&#56401;&#55349;&#56402;&#55349;&#56421;<BR>have been applied and return the result&nbsp;*/<BR>14&nbsp;return Read(&#55349;&#56406;&#55349;&#56411;&#55349;&#56401;&#55349;&#56402;&#55349;&#56421; , &#55349;&#56412;).<BR>15&nbsp;&nbsp; upon receiving &#55349;&#56389;&#55349;&#56376;&#55349;&#56372;&#55349;&#56375;, &#55349;&#56400; from &#55349;&#56413;:<BR>16&nbsp;send &#55349;&#56389;_&#55349;&#56372;&#55349;&#56374;&#55349;&#56382;, &#55349;&#56400;, &#55349;&#56391; , &#55349;&#56384;&#55349;&#56398;&#55349;&#56421;&#55349;&#56387; to &#55349;&#56413;<BR>Algorithm 2: Read pseudocode for Chameleon under the same assumptions as Algorithm 1.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Correctness Sketch<BR>Chameleon is similar to MultiPaxos [19] and Raft [24] in the sense that the leader processes writes in two phases and as- signs writes to log indexes in increasing order. Consequently, the argument for Chameleon&#8217;s correctness is fundamentally similar to these algorithms and as such we focus on its sole difference &#8212; how reads are ordered with respect to writes. Specifically, Chameleon must guarantee that if a write op- eration &#55349;&#56420; assigned to index &#55349;&#56406; completes before some read operation &#55349;&#56415; begins then &#55349;&#56415; is assigned to an index at least &#55349;&#56406;. This follows from our assumption that the token quorum system is fixed and the fact that every read and write quorum has at least one token in their intersection. These together imply that some process &#55349;&#56413; was in the intersection of &#55349;&#56420;&#8217;s write quorum and &#55349;&#56415; &#8217;s read quorum. Consequently, since &#55349;&#56420; completed before &#55349;&#56415; began, &#55349;&#56413; processed the prepare request for &#55349;&#56420; before the read request for &#55349;&#56415; , and therefore &#55349;&#56413; would have returned a maximum prepare index of at least &#55349;&#56406; in re- sponse to the read request for &#55349;&#56415; . This in turn implies that &#55349;&#56415; is assigned to at least &#55349;&#56406; as required.<BR>&nbsp;&nbsp;&nbsp; 4 Discussion<BR>We now discuss mechanisms for reconfiguring the token quorum system and tolerating message loss and failures.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Reconfiguring the Token Quorum System<BR>Chameleon&#8217;s novelty lies in its ability to switch between existing algorithms for linearizable reads at runtime. This<BR>is achieved by transferring tokens between processes &#8212; re- configuring the token quorum system in the process. To maintain correctness Chameleon must ensure that all read operations observe all previously completed write opera- tions. This can be achieved by ensuring that if a process holds some token &#55349;&#56417; it has a maximum prepare index at least as large as the maximum index number of any completed write operation that included &#55349;&#56417; . We now discuss a synchro- nous centralized approach to guarantee this. For future work, we plan to explore how to minimize reconfiguration times by designing an asynchronous approach for reconfiguration. In the centralized approach, the leader appends special entries to the log known as token configurations that describe what processes should hold which tokens. Processes then determine the current set of tokens they hold based on the lat- est configuration. To not impact performance, each process knows the latest configuration locally. To ensure this local perception of the token configuration is consistent, processes mark their local perception as invalid once they receive a prepare request for a new token configuration. At this point, processes do not know which tokens they hold and as such, their processing of prepare and read requests is stalled until they know. This occurs after they receive a commit request for the new token configuration, which is only sent once the<BR>leader receives acknowledgments from all processes.<BR>To guarantee that all read operations observe all previ- ously completed write operations we need to make two addi- tional changes. First, when the leader processes a new token configuration, it (1) waits for all outstanding write operations to complete before proposing the token configuration and (2) stalls the processing of new write operations until the new token configuration has been acknowledged by all processes. Second, when acknowledging read requests, processes re- turn the index of the token configuration it used to compute the set of tokens it holds. The process performing the read operation then keeps track of the highest index returned and counts tokens only from processes that return that index. In the event this is insufficient to cover a read quorum, the process resends read requests until it covers a read quorum. These changes ensure that each token holder has a maxi- mum prepare index of at least as large as the maximum index number of any completed write operation. This is because the new token configuration&#8217;s index number is higher than all previously completed writes since the leader waited for all outstanding writes to be completed. Furthermore, read operations are bound to observe all previously completed write operations since readers only include tokens from the latest token configuration.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Tolerating Message Loss &amp; Failures<BR>Like in existing algorithms, message loss can be tolerated by supporting request re-transmission and detecting duplicate operations. Consequently, Chameleon can be adapted to do so using existing techniques such as the ones proposed by<BR>CHT [7]. Similarly, tolerating failures in Chameleon is func- tionally equivalent to how algorithms for local reads tolerate failures. This is because they both have to guarantee liveli- ness even when a read quorum is partitioned from the rest of the network. To handle this case, algorithms for local reads grant processes read leases which are time-based contracts that while valid allow processes to perform read operations against their local replica. When these leaseholders crash or are partitioned from the rest of the network, the leader will wait for the expiration of their read lease before completing ongoing write operations. This guarantees they are no longer performing reads locally and thus it is safe for the rest of the processes to commit said write. We now discuss how this idea can be adapted to Chameleon to tolerate failures.<BR>To enable fault-tolerant local reading of the token con- figuration, each process is granted a read lease which is revoked when a process doesn&#8217;t respond to token configura- tion changes promptly. Similarly, to tolerate the failure of a token holder, holding a token is attached to a lease. Specif-<BR>ically, if process &#55349;&#56413; holds a token &#55349;&#56417; owned by process &#55349;&#56414;, &#55349;&#56414;<BR>will grant &#55349;&#56413; the right to hold &#55349;&#56417; for some lease duration. This enables the leader to revoke all tokens held by some process<BR>&#55349;&#56413; by contacting all processes and requesting them to wait<BR>for their leases granted to &#55349;&#56413; to expire. Once this occurs, the leader includes these revoked tokens until the write quorum condition is met. Like in algorithms for local reads, this en- sures that any read quorum that &#55349;&#56413; was a part of is no longer performing read operations after its leases are revoked.<BR>In addition to this revocation mechanism, once a token has been revoked, the token needs to be associated with a valid prepare index to ensure read operations observe all com- pleted write operations. Like in our reconfiguration mecha- nism, this can be accomplished by relying on the leader to provide their latest assigned log index to the process which is revoking the token. However, unlike the reconfiguration mechanisms, this requires that at most one process is the leader at any time. This can be accomplished by granting the leader a leader lease. To also handle the failure of the leader, we can adopt the enhanced leader election service proposed by CHT [7] that takes care of both granting the leader a leader lease and electing a leader. For future work, we will formalize the mechanism required to tolerate failures.</P>
<P>&nbsp;&nbsp;&nbsp; 5 Related Work<BR>SMR algorithms. The most well known SMR algorithms are MultiPaxos [19] and Raft [26]. Like Chameleon, they are leader-based and process writes in two phases. An alterna- tive to these leader-based algorithms is leaderless algorithms such as EPaxos [23, 28] and Nezha [11]. These algorithms allow all processes to perform write operations without con- tacting the leader at the expense of needing a super quorum of acknowledgments instead of a simple majority. We believe Chameleon can be adapted to work with these algorithms.<BR>Quorum systems. Flexible [16], Wan [1], and Dynamic Paxos [25] use non-majority quorum systems to ensure that leader election quorums always intersect with prepare quo- rums. This guarantees that writes are consistent across lead- ers. In Flexible and Wan Paxos, the quorum system is config- urable but must be known a priori. Dynamic Paxos allows for the quorum system to be reconfigured at runtime but is specific to leader election quorums. Specifically, Dynamic Paxos enables the leader election zone to move at runtime to tolerate failures or for access locality. Due to the specific use case of this mechanism, it is not capable of mimicking existing algorithms for linearizable reads. Chameleon, on the other hand, can because of its use of the token quorum system. EdgePQR [13] extends Wan Paxos with the ability to perform low-latency reads at edge data centers. To do so, they introduce the idea of edge quorums which enables reads to be processed by contacting any majority of processes within an edge datacenter. To guarantee correctness, all write oper- ations are also required to contact a majority of processes in each edge data center. EdgePQR tolerates the failure of edge datacenters without stalling write operations by enabling the addition and removal of edge datacenters at runtime. This is similar to Chameleon&#8217;s ability to reconfigure the read-write quorum system but only applies to edge quorums and not the entire read-write quorum system. Quoracle [30] quantifies the trade-offs between different read-write quorum systems but does not consider reconfiguration at runtime.<BR>Token virtualization. An alternative interpretation of the token quorum system presented in this paper is that each token is a virtual process which is mapped to a single phys- ical process through token holding. Quorum intersection in Chameleon&#8217;s case is then guaranteed by intersecting suf- ficiently many virtual processes for both reads and writes. This idea of token virtualization has parallels to works in the late 80s and early 90s done on fair resource schedul- ing [18, 29]. In these works, tokens were known as tickets or shares which were given to users to fairly schedule their programs on shared CPU resources. Chameleon differs from these works in its use of tokens as a means to guarantee read-write quorum intersection instead of fair scheduling.</P>
<P>&nbsp;&nbsp;&nbsp; 6 Conclusion<BR>In this paper, we presented Chameleon, an algorithm for linearizable reads that lays out a principled approach for datastores to switch between existing read algorithms at runtime. To do so, Chameleon constructs a read-write quo- rum system using tokens that are included to complete read and write operations. This token quorum system enables Chameleon to switch between existing read algorithms by transferring them between processes. For future work, we plan to develop a complete description of Chameleon along with an evaluation that demonstrates the benefits of switch- ing between existing read algorithms at runtime.