<H3 class="post-title entry-title" style='WHITE-SPACE: normal; WORD-SPACING: 0px; POSITION: relative; TEXT-TRANSFORM: none; COLOR: rgb(102,102,102); FONT: 22px "Trebuchet MS", Trebuchet, Verdana, sans-serif; ORPHANS: 2; WIDOWS: 2; MARGIN: 0.75em 0px 0px; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial' itemprop="name"><FONT class=extract>The 4 laws of Durability</FONT></H3>
<DIV class=post-header style='FONT-SIZE: 10px; FONT-FAMILY: "Trebuchet MS", Trebuchet, Verdana, sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(102,102,102); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; MARGIN: 0px 0px 1.5em; LETTER-SPACING: normal; LINE-HEIGHT: 1.6; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-variant-ligatures: normal; font-variant-caps: normal'>
<DIV class=post-header-line-1><FONT class=extract></FONT></DIV></DIV>
<DIV id=post-body-6552651021562621124 class="post-body entry-content" style='FONT-SIZE: 13px; FONT-FAMILY: "Trebuchet MS", Trebuchet, Verdana, sans-serif; WIDTH: 546px; WHITE-SPACE: normal; WORD-SPACING: 0px; POSITION: relative; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(102,102,102); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; LETTER-SPACING: normal; LINE-HEIGHT: 1.4; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-variant-ligatures: normal; font-variant-caps: normal' itemprop="description articleBody">
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>When it comes to having durable data, there are four ways to do it:<SPAN>&nbsp;</SPAN><B>undo log</B>,<SPAN>&nbsp;</SPAN><B>redo log</B>,<SPAN>&nbsp;</SPAN><B>shadow copy</B><SPAN>&nbsp;</SPAN>and<SPAN>&nbsp;</SPAN><B>shadow data</B>.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Let's start with the preliminaries.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>So what do we mean by "<I>durable</I>"?</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Well, durable means that whatever data you're trying to save, has reached your storage device in a consistent way. It means that when you write to storage you want it to be "permanent", whether that storage device is a USB key, a CD ROM, an hard drive, and SSD, or a non-volatile memory DIMM like Intel's Optane DC PM.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>For any of these storage devices, the algorithms are always the same: you have to use one of the four mentioned above.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Keep in mind that these are needed if the data needs to be consistent, i.e. you want to see the whole data before the storage or none of the data. I mean, if we were ok with having garbled data, then why would we bother saving it in permanent storage? The whole point of making data durable is because it has important information and therefore, it implies consistency.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Now that the basics are out of the way, what are exactly these four algorithms?</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>I'm going to focus on these in the context of transactions, but they don't have to be necessarily about that.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract><B>Undo log</B><SPAN>&nbsp;</SPAN>is technique where we write to durable storage a log entry before each write is done to storage. It allows multiple independent (non-atomic) writes to become durable in an all-or-nothing way, like a transaction, or a checkpoint.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>In the context of persistent memory,<SPAN>&nbsp;</SPAN></FONT><A style="TEXT-DECORATION: none; COLOR: rgb(34,136,187)" href="https://pmem.io/pmdk/libpmemobj/"><FONT class=extract>libpmemobj<SPAN>&nbsp;</SPAN></FONT></A><FONT class=extract>in PMDK is an example of a transactional system that uses undo log.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>In<SPAN>&nbsp;</SPAN><B>Redo log</B><SPAN>&nbsp;</SPAN>we write the log with multiple entries to storage before writing the actual data. The difference between redo and undo is that undo log does one entry in the log at a time followed by one modification, while the redo log does all entries in the log in one shot and then all the modifications in one shot.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><A style="TEXT-DECORATION: none; COLOR: rgb(34,136,187)" href="https://dl.acm.org/doi/10.1145/1950365.1950379"><FONT class=extract>Mnemosyne<SPAN>&nbsp;</SPAN></FONT></A><FONT class=extract>and<SPAN>&nbsp;</SPAN></FONT><A style="TEXT-DECORATION: none; COLOR: rgb(34,136,187)" href="https://github.com/pramalhe/OneFile/"><FONT class=extract>OneFile<SPAN>&nbsp;</SPAN></FONT></A><FONT class=extract>are examples of transactional systems that utilize redo log.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract><B>Shadow copy</B>, sometimes called Copy-On-Write (<B>COW</B>) creates a new replica of the data and writes the new data along with the unchanged contents to durable storage, before swapping some kind of pointer to indicate the this is the new object/data and the old one can be discarded. COW can't really be used by itself for transactions over multiple objects, but it can be combined with redo log to make it more efficient.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>One example is SAP HANA which uses redo log with COW.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract><B>Shadow data</B><SPAN>&nbsp;</SPAN>can sometimes be confused with COW but it is not the same thing. In shadow data two (or more) replicas of the entire data are kept in durable storage and they both are updated with the modifications, one at a time. First one replica, then a logical pointer and then the second replica. On the next set of atomic writes the recently updated replica is the first to be updated.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Examples of shadow data transactional systems are<SPAN>&nbsp;</SPAN></FONT><A style="TEXT-DECORATION: none; COLOR: rgb(34,136,187)" href="https://dl.acm.org/doi/10.1145/3210377.3210392"><FONT class=extract>Romulus</FONT></A><FONT class=extract>,<SPAN>&nbsp;</SPAN></FONT><A style="TEXT-DECORATION: none; COLOR: rgb(34,136,187)" href="https://github.com/pramalhe/RedoDB"><FONT class=extract>RedoDB<SPAN>&nbsp;</SPAN></FONT></A><FONT class=extract>and<SPAN>&nbsp;</SPAN></FONT><A style="TEXT-DECORATION: none; COLOR: rgb(34,136,187)" href="https://dl.acm.org/doi/10.1145/3437801.3441586"><FONT class=extract>Trinity<SPAN>&nbsp;</SPAN></FONT></A><FONT class=extract>to some extent.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>We though long and hard at the similarities and differences between these four algorithms for durable transactions, and we found they possess four common characteristics, regardless of the underlying storage media for which they are intended.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Each one of these characteristics reveals an important insight into the concept of durability and we believe these to be empirical rules to which all durable techniques abide. These rules are:</FONT></P>
<OL style="TEXT-ALIGN: left">
<LI style="PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 0.25em; PADDING-RIGHT: 0px; TEXT-INDENT: 0px"><FONT class=extract><SPAN>&nbsp;</SPAN>There must be a<SPAN>&nbsp;</SPAN><B>replica</B><SPAN>&nbsp;</SPAN>of the data; </FONT>
<LI style="PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 0.25em; PADDING-RIGHT: 0px; TEXT-INDENT: 0px"><FONT class=extract><SPAN>&nbsp;</SPAN>There must be a durable<SPAN>&nbsp;</SPAN><B>state</B><SPAN>&nbsp;</SPAN>indicating which of the replicas is consistent; </FONT>
<LI style="PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 0.25em; PADDING-RIGHT: 0px; TEXT-INDENT: 0px"><FONT class=extract>All algorithms require at least one<SPAN>&nbsp;</SPAN><B>ordering</B><SPAN>&nbsp;</SPAN>constraint of the writes to durable storage;<SPAN>&nbsp;</SPAN> </FONT>
<LI style="PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 0.25em; PADDING-RIGHT: 0px; TEXT-INDENT: 0px"><FONT class=extract>A modification is durable only after a<SPAN>&nbsp;</SPAN><B>round-trip</B><SPAN>&nbsp;</SPAN>fence to the storage hardware;</FONT></LI></OL>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>The first key insight regarding durable transactions is that a consistent and durable<SPAN>&nbsp;</SPAN><B>replica<SPAN>&nbsp;</SPAN></B>of the data must exist at all times. This replica may be a full copy of the data, such as on shadow data, or it may be a logical replica, such as on undo log and redo log.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Intuitively, there has to be a consistent replica of the data, so that there is a way to recover data to its original consistent state in the event of a failure. Shadow data keeps a full replica of the data thus incurring a high permanent usage of the durable media (space amplification), while the undo log and redo log approaches have to write in durable storage, not just the new data but also, encoded information about the location and size of the modification (write amplification).</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>There's clearly an important trade-off here: log-based algorithms will increase (amortized) write amplification but shadow-data-based algorithms will increase space amplification.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>The second empirical rule implies that the algorithm must ensure that, irrespective of when a failure occurs, there is a way for the recovery procedure to determine which of the replicas is<SPAN>&nbsp;</SPAN><B>consistent</B>.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Shadow data like Romulus uses a two-bit variable to determine which of the two replicas is consistent, while redo log and undo log can use the size of the log (zero or non-zero) to indicate if the log is consistent.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>By itself, there is no significant difference in any of the approaches however, the exact mechanics, will influence the number of ordering constraints in the algorithm.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>This leads us to the third insight, that data consistency is possible only through<SPAN>&nbsp;</SPAN><B>ordering<SPAN>&nbsp;</SPAN></B>of some of the writes.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>For shadow-copying, the modifications on the new block must be made durable<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the pointer swap, otherwise a failure occurring after the pointer swap is made durable, would leave the pointer referencing an inconsistent block. This means that apart from block allocation and de-allocation details, shadow-copying has a single ordering constraint, or in other words, a single ordering fence.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Shadow data like Romulus uses a two-bit state (though one bit would suffice) to indicate which of the two replicas is the consistent one, or whether both are consistent. If the state variable indicating which replica is the consistent one becomes durable before or after the modifications on either replica and a crash occurs, upon recovery it may be referencing the inconsistent replica. For this algorithm, three ordering constraints exist: one to prevent the state from changing to COPYING<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the modifications in main replica are done; another to prevent the modifications in back replica from being done<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the state changes to COPYING; and another one to prevent the state change to IDLE<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the changes on back replica are durable.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>The undo log technique has two constraints per modified object/range: the log entry must contain the old value<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the entry is added to the log; and the entry must be added to the log<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the modification is done on the data. Undo log has one extra constraint per transaction, requiring the last modification to be durable<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the log is reset.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>The Redo log technique has three constraints per transaction: all the log entries must be durable<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the log size is set; the log size must be set<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the modifications are done on the data; the modifications on the data must be durable<SPAN>&nbsp;</SPAN><I>before</I><SPAN>&nbsp;</SPAN>the log is reset.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>The fourth and final rule addresses the need for a<SPAN>&nbsp;</SPAN><B>round-trip</B><SPAN>&nbsp;</SPAN>synchronization mechanism to the storage domain, such that the hardware can guarantee that it contains, in stable durable storage, all the previously written data. The cost of such a fence is typically of the order of the storage device's latency.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Fast devices like PM implement this round-trip fence orders of magnitude faster than slower devices, like hard drives.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Without such a mechanism, it is not possible to have durable operations, even if ordering constraints are set: in the event of a failure, the ordering constraints impose a temporal sequence of which the writes will be made durable, but there is no guarantee on durability.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>A corollary of this is that all algorithms require one and only one such fence, strategically placed.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Notice that the ordering constraints may be replaced by such synchronous fences, at the detriment of performance, and in fact, many storage systems make no distinction between the two. Ordering is typically achieved with an asynchronrous fence and it relates to the order to which certain writes will be made durable in the storage media.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>On block based storage, this is typically implemented with<SPAN>&nbsp;</SPAN><SPAN style="FONT-FAMILY: courier">fsync()</SPAN><SPAN>&nbsp;</SPAN>or<SPAN>&nbsp;</SPAN><SPAN style="FONT-FAMILY: courier">fdatasync()</SPAN>.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>In Persistent Memory (PM) ordering can be achieved through the combination of flushes (clwb) and fences (sfence) or by writing to the same cache line. The round-trip guarantee of durability is given by a synchronous fence, either<SPAN>&nbsp;</SPAN><SPAN style="FONT-FAMILY: courier">fsync()/fdatasync()</SPAN><SPAN>&nbsp;</SPAN>on block storage, or<SPAN>&nbsp;</SPAN><SPAN style="FONT-FAMILY: courier">sfence</SPAN><SPAN>&nbsp;</SPAN>on PM storage.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>In case you haven't noticed, the fact that all algorithms require one round-trip fence to the device (psync), but may require multiple ordering fences (pfence) has implications in performance. This is specially true given that the psync has inescapable physical implications:<SPAN>&nbsp;</SPAN><B>it is not possible to have all-or-nothing consistent durability without a psync that physically does a round trip to the storage device</B><SPAN>&nbsp;</SPAN>(or at least the storage domain) and therefore the latency cost of this single round trip is inescapable.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>However, different algorithms may have different ordering constraints (pfences) and these may have different costs.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Yes, fsync() is used for both sync and ordering on block devices, and the sfence instruction is also used for both in PM, however, there are tricks. In PM, writes to the same cache line are guaranteed to be ordered and therefore, no sfence is needed to order them, as long as store with memory_order_release is used.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Seen as these round trips are typically the bottleneck when doing random writes to PM, the fact that we can create an algorithm with a lower number of psyncs means we can have a performance gain that is nearly proportional to the reduction in the number of such fences.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>&nbsp;</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>This is exactly what we've done with Trinity.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Trinity is a novel durability technique that needs just two fences per transaction and reduces the number of flushes when doing random writes. It consumes more memory than the other previous techniques but it has significant higher performance.</FONT></P>
<P style="FONT-SIZE: 11pt; FONT-FAMILY: Calibri; MARGIN: 0in"><FONT class=extract>Moreover, we combined it with our own variant of TL2 for highly scalable durable linearizable transactions, and we used that to make a K/V store, which is likely that fastest K/V store on the planet with full transactions (though you need Optane Persistent Memory to be able to run it).</FONT></P></DIV>