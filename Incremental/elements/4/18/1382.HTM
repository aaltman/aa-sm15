<STRONG>: Curse of dimensionality </STRONG>in machine learning
<H3></H3><SPAN class=mw-editsection style="FONT-SIZE: small; FONT-FAMILY: sans-serif; VERTICAL-ALIGN: baseline; FONT-WEIGHT: normal; MARGIN-LEFT: 1em; LINE-HEIGHT: 0; MARGIN-RIGHT: 0px; user-select: none"><SPAN class=mw-editsection-bracket style="COLOR: ; MARGIN-RIGHT: 0.25em">[</SPAN><A title="Edit section: Machine learning" style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; WHITE-SPACE: nowrap; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/w/index.php?title=Curse_of_dimensionality&amp;action=edit&amp;section=5"><SPAN>edit</SPAN></A><SPAN class=mw-editsection-bracket style="COLOR: ; MARGIN-LEFT: 0.25em">]</SPAN></SPAN>
<DIV></DIV>
<P style="FONT-SIZE: 16px; FONT-FAMILY: sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(32,33,34); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; MARGIN: 0.5em 0px 1em; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">In<SPAN>&nbsp;</SPAN><A title="Machine learning" style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</A><SPAN>&nbsp;</SPAN>problems that involve learning a "state-of-nature" from a finite number of data samples in a high-dimensional<SPAN>&nbsp;</SPAN><A title="Feature space" class=mw-redirect style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Feature_space">feature space</A><SPAN>&nbsp;</SPAN>with each feature having a range of possible values, typically an enormous amount of training data is required to ensure that there are several samples with each combination of values. In an abstract sense, as the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.<SUP id=cite_ref-4 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-4"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[4]</FONT></A></SUP></P>
<P style="FONT-SIZE: 16px; FONT-FAMILY: sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(32,33,34); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; MARGIN: 0.5em 0px 1em; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">A typical rule of thumb is that there should be at least 5 training examples for each dimension in the representation.<SUP id=cite_ref-Pattern_recog_5-0 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Pattern_recog-5"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[5]</FONT></A></SUP><SPAN>&nbsp;</SPAN>In<SPAN>&nbsp;</SPAN><A title="Machine learning" style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</A><SPAN>&nbsp;</SPAN>and insofar as predictive performance is concerned, the<SPAN>&nbsp;</SPAN><I>curse of dimensionality</I><SPAN>&nbsp;</SPAN>is used interchangeably with the<SPAN>&nbsp;</SPAN><I>peaking phenomenon</I>,<SUP id=cite_ref-Pattern_recog_5-1 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-Pattern_recog-5"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[5]</FONT></A></SUP><SPAN>&nbsp;</SPAN>which is also known as<SPAN>&nbsp;</SPAN><I>Hughes phenomenon</I>.<SUP id=cite_ref-6 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-6"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[6]</FONT></A></SUP><SPAN>&nbsp;</SPAN>This phenomenon states that with a fixed number of training samples, the average (expected) predictive power of a classifier or regressor first increases as the number of dimensions or features used is increased but beyond a certain dimensionality it starts deteriorating instead of improving steadily.<SUP id=cite_ref-7 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-7"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[7]</FONT></A></SUP><SUP id=cite_ref-8 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-8"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[8]</FONT></A></SUP><SUP id=cite_ref-McLachlan:2004_9-0 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-McLachlan:2004-9"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[9]</FONT></A></SUP></P>
<P style="FONT-SIZE: 16px; FONT-FAMILY: sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(32,33,34); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; MARGIN: 0.5em 0px 1em; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">Nevertheless, in the context of a<SPAN>&nbsp;</SPAN><I>simple</I><SPAN>&nbsp;</SPAN>classifier (e.g.,<SPAN>&nbsp;</SPAN><A title="Linear discriminant analysis" style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis</A><SPAN>&nbsp;</SPAN>in the multivariate Gaussian model under the assumption of a common known covariance matrix), Zollanvari,<SPAN>&nbsp;</SPAN><I>et al.</I>, showed both analytically and empirically that as long as the relative cumulative efficacy of an additional feature set (with respect to features that are already part of the classifier) is greater (or less) than the size of this additional feature set, the expected error of the classifier constructed using these additional features will be less (or greater) than the expected error of the classifier constructed without them. In other words, both the size of additional features and their (relative) cumulative discriminatory effect are important in observing a decrease or increase in the average predictive power.<SUP id=cite_ref-zollanvari_10-0 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-zollanvari-10"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[10]</FONT></A></SUP></P>
<P style="FONT-SIZE: 16px; FONT-FAMILY: sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(32,33,34); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; MARGIN: 0.5em 0px 1em; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">In<SPAN>&nbsp;</SPAN><A title="Similarity learning" style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Similarity_learning">metric learning</A>, higher dimensions can sometimes allow a model to achieve better performance. After normalizing embeddings to the surface of a hypersphere, FaceNet achieves the best performance using 128 dimensions as opposed to 64, 256, or 512 dimensions in one ablation study.<SUP id=cite_ref-11 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-11"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[11]</FONT></A></SUP><SPAN>&nbsp;</SPAN>A loss function for unitary-invariant dissimilarity between word embeddings was found to be minimized in high dimensions.<SUP id=cite_ref-12 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-12"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[12]</FONT></A></SUP></P>
<DIV class="mw-heading mw-heading3" style="FONT-SIZE: 1.2em; OVERFLOW: hidden; FONT-FAMILY: sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: bold; COLOR: ; PADDING-BOTTOM: 0px; FONT-STYLE: normal; PADDING-TOP: 0.5em; ORPHANS: 2; WIDOWS: 2; MARGIN: 0.25em 0px; LETTER-SPACING: normal; LINE-HEIGHT: 1.6; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial"></DIV>