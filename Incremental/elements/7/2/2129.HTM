<b> : </b>Paxos background</H2>Before we discuss Paxos bottlenecks, here is a brief refresher for the Paxos protocol and variants.<BR><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243); DISPLAY: inline-block" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9ndTHOkQFGO0-ThK8HQWpwsLwGOQ8XQjJGRjcPaAPRynUS11Xtq2aSVoTfeh-AyYTjCU_VxKbcdfIqA-r94_vE2PqHupySPqNoox_iV2tZsuVOmof68kExkMQV10124Fss5t-9q3jgM/s1600/paxos.png" imageanchor="1"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" border=0 src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9ndTHOkQFGO0-ThK8HQWpwsLwGOQ8XQjJGRjcPaAPRynUS11Xtq2aSVoTfeh-AyYTjCU_VxKbcdfIqA-r94_vE2PqHupySPqNoox_iV2tZsuVOmof68kExkMQV10124Fss5t-9q3jgM/s400/paxos.png" width=400 height=167></A><BR>The figure illustrates a single leader (vanilla) Paxos protocol. Yes, three phases look expensive, but in Multi-Paxos, things get a lot better because Phase1 is skipped in the presence of a stable leader. In Multi-Paxos, for upcoming slots (i.e., consensus instances), the leader skips Phase1 and just goes with Phase2. As another optimization, Phase3 messages are piggybacked to the Phase2 messages of upcoming slots rather than being sent separately.<BR><BR>But, even in Multi-Paxos (which we consider henceforth), there is an obvious bottleneck at the leader. The leader is doing a disproportionately large amount of the work, while the followers are slacking off. The followers receive one message and send one message back for each slot. In contrast, the poor leader needs to send N messages in Phase2a, and receive at least a quorum of messages from followers in Phase2b. It turns out, in practice, the overhead of Phase2b is worse than that of Phase2a. For sending Phase1a messages, the leader serializes the message once, and the network card takes care of sending them. For receiving the messages in Phase2b, the leader node needs to deserialize and process each message separately.<BR><BR><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243); DISPLAY: inline-block" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjF3-wqBO0OgQZcGmatL_a3eW2-2zPjEUWHlhEdY9MsffUiFjLcLrqkagKoT89f5MSdaeQjdBwmxlwmaHJv8yOy2Dw5FEerI8hi_ugAl-CX4ind72Mw2aby4V7T_h8bfgAH9zBbgM61te8/s1600/paxos_architectures_single_leader.png" imageanchor="1"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" border=0 src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjF3-wqBO0OgQZcGmatL_a3eW2-2zPjEUWHlhEdY9MsffUiFjLcLrqkagKoT89f5MSdaeQjdBwmxlwmaHJv8yOy2Dw5FEerI8hi_ugAl-CX4ind72Mw2aby4V7T_h8bfgAH9zBbgM61te8/s200/paxos_architectures_single_leader.png" width=200 height=124>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</A><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243); DISPLAY: inline-block" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLOFpQ8GeDmriJdxRTlVXTIvQrpxu1p7xiWUl2AV5H6rN-S-_lfj2Zo3VVUOhu_8L9EcoTUneUEPACDzD2q0pcr70rsfBh8YLZLI-Rj5DEOuaR-s2JHba4zJMnE3RntK0sfn1ICHFbYvc/s1600/paxos_architectures_leaderless.png" imageanchor="1"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" border=0 src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLOFpQ8GeDmriJdxRTlVXTIvQrpxu1p7xiWUl2AV5H6rN-S-_lfj2Zo3VVUOhu_8L9EcoTUneUEPACDzD2q0pcr70rsfBh8YLZLI-Rj5DEOuaR-s2JHba4zJMnE3RntK0sfn1ICHFbYvc/s200/paxos_architectures_leaderless.png" width=200 height=125></A><BR><BR>Of course many researchers noticed this bottleneck at the leader, and they proposed Paxos flavors to alleviate this issue. EPaxos used opportunistic leaders: any node becomes a leader when it receives a request, and tries to get a ~3/4ths quorum of nodes accept the request to finalize it. In EPaxos, a conflict is possible with concurrent and noncommutative commands, and that requires another round to resolve.<BR><BR><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243); DISPLAY: inline-block" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhikNVm0Z5-Ri-2t7dyoEi3MmXEZ6jpW-1zF4BdF8QVzU6UKhayidYS80PQNxF8K_CdQIN_zOvHnebc5ndZArv5KYVfvhpOcuhjwYW6qm3DUzH4Lq_v5NlQ6T5SvBSM91EvhIzDUgYVNc8/s1600/paxos_architectures_hierarchichal.png" imageanchor="1"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" border=0 src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhikNVm0Z5-Ri-2t7dyoEi3MmXEZ6jpW-1zF4BdF8QVzU6UKhayidYS80PQNxF8K_CdQIN_zOvHnebc5ndZArv5KYVfvhpOcuhjwYW6qm3DUzH4Lq_v5NlQ6T5SvBSM91EvhIzDUgYVNc8/s200/paxos_architectures_hierarchichal.png" width=200 height=124></A>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243); DISPLAY: inline-block" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2GLD1Hg0cXpUAqWZ97Ptm9l_XUvBN0aMp94mguB4ittvB2n27tJJWpxeT8shozRPKwhUKEqAeMVB6RBo3gR2ogCwAr99SgSxUda2LUAdmDc2VIcQCbfbEulQIki0akxTyVABQLODKE64/s1600/paxos_architectures_multi_quorum.png" imageanchor="1"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" border=0 src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2GLD1Hg0cXpUAqWZ97Ptm9l_XUvBN0aMp94mguB4ittvB2n27tJJWpxeT8shozRPKwhUKEqAeMVB6RBo3gR2ogCwAr99SgSxUda2LUAdmDc2VIcQCbfbEulQIki0akxTyVABQLODKE64/s200/paxos_architectures_multi_quorum.png" width=200 height=125></A><BR>WanKeeper deploys Paxos groups hierarchically. This helps for scalability because key-ranges are sharded to Paxos groups. Using a simpler versions of this idea, Spanner and CockroachDB statically assign keyranges to Paxos groups, and use another service (such as Movedir) to modify the assignments.<BR><BR><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243)" href="http://muratbuffalo.blogspot.com/2017/12/wpaxos-wide-area-network-paxos-protocol.html">WPaxos provides a more decentralized version of sharding.</A>&nbsp; It uses multileaders, and partitions the object-space among these multileaders. Unlike statically partitioned multiple Paxos deployments, WPaxos is able to adapt to the changing access locality through object stealing. Multiple concurrent leaders coinciding in different zones steal ownership of objects from each other using Phase1 of Paxos, and then use Phase2 to commit update-requests on these objects locally until they are stolen by other leaders. To achieve fast Phase2 commits, WPaxos adopts<SPAN>&nbsp;</SPAN><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243)" href="http://muratbuffalo.blogspot.com/2016/11/modeling-paxos-and-flexible-paxos-in.html">the flexible quorums idea</A><SPAN>&nbsp;</SPAN>in a novel manner, and appoints Phase2 acceptors to be close to their respective leaders.<SPAN>&nbsp;</SPAN><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(33,150,243)" href="https://arxiv.org/pdf/1703.08905.pdf">Here is the link to the journal version of our WPaxos paper for more details.</A>