<b> : </b>Some notes relevant to the history of the discussion of bisection bandwidth in High Performance Computing</H2>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>There has not been a lot of recent discussion of high-bisection-bandwidth computers, but this was a topic that came up frequently in the late 1990&#8217;s and early 2000&#8217;s. A good reference is the 2002 Report on High End Computing for the National Security Community, which I participated in as the representative of IBM (<A style='BOX-SIZING: border-box; TEXT-DECORATION: none; FONT-FAMILY: "Libre Franklin", sans-serif; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(160,68,0); transition: 0.1s ease-in-out' href="https://sites.utexas.edu/jdm4372/2022/04/11/why-dont-we-talk-about-bisection-bandwidth-any-more/#refs">ref</A><SPAN>&nbsp;</SPAN>1). The section starting on page 35 (&#8220;Systems, Architecture, Programmability, and Components Working Group&#8221;) discusses two approaches to &#8220;supercomputing&#8221; &#8211; one focusing on aggregating cost-effective peak performance (the &#8220;type T&#8221; (Transistor) systems), and the other focusing on providing the tightest integration and interconnect performance (&#8220;type C&#8221; (Communication) systems). A major influence on this distinction was Burton Smith, founder of Tera Computer Company and developer of the Tera MTA system. The Tera MTA was a unique architecture with no caches, with memory distributed/interleaved/hashed across the entire system, and with processors designed to tolerate the memory latency and to effectively synchronize on memory accesses (using &#8220;full/empty&#8221; metadata bits in the memory).</P>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>The 2002 report led fairly directly to the DARPA High Productivity Computing Systems (HPCS) project (2002-2010), which provided direct funding to several companies to develop hardware and software technologies to make supercomputers significantly easier to use. Phase 1 was just some seed money to write proposals, and included about 7 companies. Phase 2 was a much larger ($50M over 3 years to each recipient, if I recall correctly) set of grants for the companies to do significant high-level design work. Phase 3 grants ($200M-$250M over 5 years to each recipient) were awarded to Cray and IBM (<A style='BOX-SIZING: border-box; TEXT-DECORATION: none; FONT-FAMILY: "Libre Franklin", sans-serif; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(160,68,0); transition: 0.1s ease-in-out' href="https://sites.utexas.edu/jdm4372/2022/04/11/why-dont-we-talk-about-bisection-bandwidth-any-more/#refs">ref</A><SPAN>&nbsp;</SPAN>2). I was (briefly) the large-scale systems architecture team lead on the IBM Phase 2 project in 2004.</P>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>Both the Cray and IBM projects were characterized by a desire to improve effective bisection bandwidth, and both used hierarchical all-to-all interconnect topologies.</P>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>(If I recall correctly), the Cray project funded the development of the &#8220;Cascade&#8221; interconnect which eventually led to the Cray XC30 series of supercomputers (<A style='BOX-SIZING: border-box; TEXT-DECORATION: none; FONT-FAMILY: "Libre Franklin", sans-serif; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(160,68,0); transition: 0.1s ease-in-out' href="https://sites.utexas.edu/jdm4372/2022/04/11/why-dont-we-talk-about-bisection-bandwidth-any-more/#refs">ref</A><SPAN>&nbsp;</SPAN>3). Note that the Cray project funded only the interconnect development, while standard AMD and/or Intel processors were used for compute. The inability to influence the processor design limited what Cray was able to do with the interconnect. The IBM grant paid for the development of the &#8220;Torrent&#8221; bridge/switch chip for an alternate version of an IBM Power7-based server. Because IBM was developing both the processor and the interconnect chip, there was more opportunity to innovate.</P>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>I left IBM at the end of 2005 (near the end of Phase 2). IBM did eventually complete the implementation funded by DARPA, but backed out of the &#8220;Blue Waters&#8221; system at NCSA (<A style='BOX-SIZING: border-box; TEXT-DECORATION: none; FONT-FAMILY: "Libre Franklin", sans-serif; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: rgb(160,68,0); transition: 0.1s ease-in-out' href="https://sites.utexas.edu/jdm4372/2022/04/11/why-dont-we-talk-about-bisection-bandwidth-any-more/#refs">ref</A><SPAN>&nbsp;</SPAN>4) &#8211; an NSF-funded supercomputer that was very deliberately pitched as a &#8220;High Productivity Computing System&#8221; to benefit from the DARPA HPCS development projects. I have no inside information from the period that IBM backed out, but it is easy to suspect that IBM wanted/needed more money than was available in the budget to deliver the full-scale system. The &#8220;Blue Waters&#8221; system was replaced by a Cray &#8211; but since the DARPA-funded &#8220;Cascade&#8221; interconnect was not ready, Blue Waters used an older implementation (and older AMD processors). IBM sold only a handful of small Power7 systems with the &#8220;Torrent&#8221; interconnect, mostly to weather forecasting centers. As far as I can tell, none of the systems were large enough for the Torrent interconnect to show off its (potential) usefulness.</P>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>So after about 10 years of effort and a DARPA HPCS budget of over $600M, the high performance computing community got the interconnect for the Cray XC30. I think it was a reasonably good interconnect, but I never used it. TACC just retired our Cray XC40 and its interconnect performance was fine, but not revolutionary. After this unimpressive return on investment, it is perhaps not surprising that there has been a lack of interest in<SPAN>&nbsp;</SPAN><STRONG style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: 700'>funding</STRONG><SPAN>&nbsp;</SPAN>for high-bisection-bandwidth systems.</P>
<P style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: normal; COLOR: rgb(40,40,40); PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 1px; PADDING-RIGHT: 0px'>That is not to say that there is an unambiguous<SPAN>&nbsp;</SPAN><STRONG style='BOX-SIZING: border-box; FONT-FAMILY: "Libre Franklin", sans-serif; FONT-WEIGHT: 700'>market</STRONG><SPAN>&nbsp;</SPAN>for high-bisection-bandwidth systems! It is easy enough to identify application areas and user communities who will claim to &#8220;need&#8221; increased bandwidth, but historically they have failed to follow up with actual purchases of the few high-bandwidth systems that have been offered over the years. A common factor in the history of the HPC market has been that given enough time, most users who &#8220;demand&#8221; special characteristics will figure out how to work around the lack with more software optimization efforts, or with changes to their strategy for computing. A modest fraction will give up on scaling their computations to larger sizes and will just make do with the more modest ongoing improvements in single-node performance to advance their work.