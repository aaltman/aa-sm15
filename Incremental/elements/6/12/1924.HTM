<b> : </b>Job and Data Organization</H2>
<P style="MARGIN: 0px 0px 20px">Load testing determined that our backend server can handle about 100 queries per second (QPS). Trials performed with a limited set of users lead us to expect a peak load of about 3,470 QPS, so we need at least 35 tasks. However, the following considerations mean that we need at least 37 tasks in the job, or<SPAN>&nbsp;</SPAN><SPAN data-type="tex">N+2</SPAN>:</P>
<UL style="PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 0px 55px; PADDING-RIGHT: 0px">
<LI style="PADDING-BOTTOM: 25px">During updates, one task at a time will be unavailable, leaving 36 tasks.</LI>
<LI style="PADDING-BOTTOM: 25px">A machine failure might occur during a task update, leaving only 35 tasks, just enough to serve peak load.<SUP style="FONT-SIZE: 11px; TEXT-DECORATION: none; COLOR: rgb(67,136,255); LINE-HEIGHT: 0"><A id=id-0vYuXSpSqF0IzCmUg-marker class=jumptarget style="TEXT-DECORATION: none; COLOR: rgb(67,136,255); OUTLINE-WIDTH: 0px; OUTLINE-STYLE: none; OUTLINE-COLOR: invert" href="https://sre.google/sre-book/production-environment/#id-0vYuXSpSqF0IzCmUg" data-type="noteref">13</A></SUP></LI></UL>
<P style="MARGIN: 0px 0px 20px">A closer examination of user traffic shows our peak usage is distributed globally: 1,430 QPS from North America, 290 from South America, 1,400 from Europe and Africa, and 350 from Asia and Australia. Instead of locating all backends at one site, we distribute them across the USA, South America, Europe, and Asia. Allowing for<SPAN>&nbsp;</SPAN><SPAN data-type="tex">N+2</SPAN><SPAN>&nbsp;</SPAN>redundancy per region means that we end up with 17 tasks in the USA, 16 in Europe, and 6 in Asia. However, we decide to use 4 tasks (instead of 5) in South America, to lower the overhead of<SPAN>&nbsp;</SPAN><SPAN data-type="tex">N+2</SPAN><SPAN>&nbsp;</SPAN>to<SPAN>&nbsp;</SPAN><SPAN data-type="tex">N+1</SPAN>. In this case, we&#8217;re willing to tolerate a small risk of higher latency in exchange for lower hardware costs: if GSLB redirects traffic from one continent to another when our South American datacenter is over capacity, we can save 20% of the resources we&#8217;d spend on hardware. In the larger regions, we&#8217;ll spread tasks across two or three clusters for extra resiliency.</P>
<P style="MARGIN: 0px 0px 20px">Because the backends need to contact the Bigtable holding the data, we need to also design this storage element strategically. A backend in Asia contacting a Bigtable in the USA adds a significant amount of latency, so we replicate the Bigtable in each region. Bigtable replication helps us in two ways: it provides resilience should a<SPAN>&nbsp;</SPAN><SPAN class=keep-together>Bigtable</SPAN><SPAN>&nbsp;</SPAN>server fail, and it lowers data-access latency. While Bigtable only offers eventual consistency, it isn&#8217;t a major problem because we don&#8217;t need to update the contents often.</P>
<P style="MARGIN: 0px 0px 20px">We&#8217;ve introduced a lot of terminology here; while you don&#8217;t need to remember it all, it&#8217;s useful for framing many of the other systems we&#8217;ll refer to later.</P></SECTION></SECTION>