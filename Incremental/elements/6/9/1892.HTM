<b> : </b>Summary of Metastable Failures in the Wild</H5>
<P>In<SPAN>&nbsp;</SPAN><A style="TEXT-DECORATION: none; COLOR: rgb(3,57,216); BACKGROUND-COLOR: transparent" href="https://www.usenix.org/sites/default/files/metastability_in_the_wild.pdf">Table 1</A>, we provide a breakdown of metastable failure incidents we have found. The examples include instances from both major cloud providers (e.g., Microsoft, Amazon, Google, IBM) and smaller companies and projects (e.g., Spotify, Elasticsearch, Apache Cassandra). Our summary table describes high-level aspects of these failures: duration of the incident, impacted services, triggers leading to the outage, the sustaining effect mechanism, and corrective actions taken by the engineers.&nbsp;</P>
<P>Due to the often limited scope of provided information, we use our best judgment in identifying metastable failures. The most important criteria we use is the sustaining effect mechanism. We highlight several instances in gray color when the incident description is not clear on the presence of such a sustaining effect, but metastable failure is plausible depending on the interpretation and given the rest of the information provided. Additionally, we assign each incident a unique identifier to refer to each incident later.&nbsp;</P>
<P>Triggers are the starting events in the chain leading to metastable failures. Around 45% of observed triggers in<SPAN>&nbsp;</SPAN><A style="TEXT-DECORATION: none; COLOR: rgb(3,57,216); BACKGROUND-COLOR: transparent" href="https://www.usenix.org/sites/default/files/metastability_in_the_wild.pdf">Table 1</A><SPAN>&nbsp;</SPAN>are due to engineer errors, such as buggy configuration or code deployments, and latent bugs (i.e., undetected pre-existing bugs). These can be observed in incidents GGL1, GGL2, GGL3, GGL4, AWS1, AWS3, AZR3, ELC1, SPF1. Load spikes are another prominent trigger category, with around 35% of incidents reporting it. A significant number of cases (45%) have more than one trigger.&nbsp;</P>
<P>Handling and recovering from metastable failures is not easy, with our data suggesting that incidents cause significant outages. For instance, the IBM1 incident lasted over three days. More generally, we have observed outages in a range of 1.5 to 73.53 hours, with 4 to 10 hours of outages being the most common (35% of incidents reporting the outage period).</P>
<P>While triggers initiate the failure, the sustaining effect mechanisms prevent the system from recovering. We observed a variety of different sustaining effects, such as load increase due to retries, expensive error handling, lock contention, or performance degradation due to leader election churn. By far, the most common sustaining effect is due to the retry policy, affecting more than 50% of the studied incidents &#8212; GGL2, GGL3, AWS1, AWS2, AWS3, AZR2, AZR4, IBM1, SPF1, SPF2, and CAS2 incidents are all sustained by retries.</P>
<P>Recovery from a metastable failure is challenging and often requires reducing load. Direct load shedding, such as throttling, dropping requests, or changing workload parameters, was used in over 55% of the cases. Some indirect mechanisms were also popular, such as reboots to clean the queues or operation backlogs, or policy changes. An example of such a policy change is the CAS1 incident where a feature was turned off to allow the servers to join the cluster.&nbsp;</P></DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class="field-item odd" style="FONT-SIZE: 18px; FONT-FAMILY: Muli, sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(0,0,0); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">
<DIV class="entity entity-paragraphs-item paragraphs-item-single-column-text view-mode-full view-mode-full--paragraphs_item view-mode-full--paragraphs_item--single_column_text" style="PADDING-BOTTOM: 20px; PADDING-TOP: 20px">
<DIV class=content>
<DIV class="field field-name-field-single-column-sub field-type-text field-label-hidden" style="FONT-SIZE: 40px; FONT-FAMILY: Klavika, sans-serif; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; PADDING-TOP: 20px; MARGIN: 0px; LINE-HEIGHT: 50px">
<DIV class=field-items>
<DIV class="field-item odd">Metastability at Twitter</DIV></DIV></DIV>
<DIV class="field field-name-field-single-column-text field-type-text-long field-label-hidden">
<DIV class=field-items>
<DIV class="field-item odd">
<P>While publicly available incident reports provide enough high-level information to identify the metastable failures, they lack the depth and detail to understand the complex interactions between components in large systems. In this case study, we use insider information to describe in detail one specific metastable failure occurring at Twitter, a large internet company, due to garbage collection (GC). We identify a sustaining loop where high queueing increases memory pressure and mark-and-sweep processing during GC, causing job slowdowns and thus higher queueing. The effect is more pronounced at high system loads, where the system is more vulnerable to spikes. Specifically, we see that a peak load test during a busy day triggers the system to enter a metastable failure state where jobs start to fail, and it is only after sufficient load shedding that the success rate stops dropping.</P></DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class="field-item even" style="FONT-SIZE: 18px; FONT-FAMILY: Muli, sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(0,0,0); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">
<DIV class="entity entity-paragraphs-item paragraphs-item-article-image view-mode-full view-mode-full--paragraphs_item view-mode-full--paragraphs_item--article_image" style="BORDER-BOTTOM: rgb(20,53,147) 3px solid; PADDING-BOTTOM: 20px; PADDING-TOP: 20px">
<DIV class=content>
<DIV class="field field-name-field-article-image field-type-image field-label-hidden">
<DIV class=field-items>
<DIV class="field-item odd"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; HEIGHT: auto; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" alt="" src="https://www.usenix.org/sites/default/files/styles/article_embedded/public/case_study_timeseries_0.png?itok=kcT-i7YN" width=1440 height=285></DIV></DIV></DIV>
<DIV class="field field-name-field-article-image-caption field-type-text field-label-hidden" style="FONT-SIZE: 14px; FONT-WEIGHT: 600; COLOR: rgb(0,0,0); FONT-STYLE: normal">
<DIV class=field-items>
<DIV class="field-item odd">Figure 1: Timeseries of a core service under a peak load test at Twitter. Metrics are normalized except for the success rate, which is scaled to show the trend dropping below the SLO.</DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class="field-item odd" style="FONT-SIZE: 18px; FONT-FAMILY: Muli, sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(0,0,0); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">
<DIV class="entity entity-paragraphs-item paragraphs-item-single-column-text view-mode-full view-mode-full--paragraphs_item view-mode-full--paragraphs_item--single_column_text" style="PADDING-BOTTOM: 20px; PADDING-TOP: 20px">
<DIV class=content>
<DIV class="field field-name-field-single-column-text field-type-text-long field-label-hidden">
<DIV class=field-items>
<DIV class="field-item odd">
<P>Peak load tests are one of the common types of tests used regularly in industry to expose potential problems and highlight the necessary steps to prevent incidents from happening. Figure 1 shows the timeseries of system metrics at a core service during a peak load test where we see a metastable failure. System load, GC duration, and queue length have been normalized to show only the trend, while success rate (SR) is scaled to demonstrate it dropping sharply below the SLO. All metrics are measured using the standard observability tools at Twitter, except for the (average) queue length, which is inferred using Little&#8217;s Law. By queue length, we mean the count of all the requests in the system. The service is a mature production service that's well-tuned and has been running for several years, under all the usual operating practices of frequent deployments, regular stress tests, and continuous monitoring and alerting.</P>
<P>In this incident, the peak load occurs around the 48-minute mark, and the SR starts to drop over time. Once the SR of this service drops below a critical threshold (i.e., the SLO), service operators are alerted to mitigate the problem. In this incident, the operators start load shedding at around the 83-minute mark and continue with more load shedding at 106 minutes. This had the desired effect of lowering the load, which also lowers GC and queue length. However, the SR still continues to drop and does not start to recover even when the load is back down to the level before the test. SR remains below the SLO until the service is restarted by operators. This is because even after the load shedding, a sustaining effect is still slowing down the system and causing it to remain in a metastable failure state.</P>
<P>Studying the internal system metrics from the test has shed some light on the problem. We find that the changes to GC duration are highly correlated with load fluctuations, as more load brings more memory allocation, thus requiring more GC. However, the GC is busier than normal during the peak load test. During the second load-shedding period between 106-118 minute marks, the load is more than 20% lower than that at the 40-minute offset, yet the GC is busier and SR is still dropping. At the same time, the queue length is also more than 50% higher, which implies that there are more jobs stuck in the system exacerbating GC. Thus, there is contention between arriving traffic and GC consuming resources, suggesting the metastability sustaining effect.</P>
<P>Specifically, the incident is caused by the sustaining effect in the following steps: (i) a load spike caused by peak load test introduces initial high queue length in the system; (ii) high queue length results in high GC behaviors; (iii) high GC behaviors slow job processing; (iv) more jobs get stuck in the system, which leads to higher queue length.</P>
<P>To demonstrate each of these steps, we further study data from this test as well as non-test data as a baseline. For (i), we can see the initial trigger in Figure 1 at around minute 48 where the load spike causes a sharp increase in queue length. For (ii), we see that queue length and GC duration are correlated over time in Figure 1. &nbsp;Additionally, we plot queue length vs. GC duration (see Figure 2(a)) under 3 normal days without the test to show these metrics generally exhibit a positive correlation. One might wonder whether the system load affects these metrics, and we find that it is correlated to both queue length and GC duration. But to eliminate the impact of system load, we also filtered the data to only include results with approximately the same system load, and we still see a correlation between queue length and GC duration, which suggests that high queue length leads to high GC. Correlation does not imply causation, so we validate and reproduce these effects in the next section via a simple example. For (iii), we plot GC duration vs. latency (see Figure 2(b)) during the same period without peak load testing and observe that the latency increases with GC duration. As GC consumes CPU cycles, there is CPU contention with job processing, which causes slowdowns to jobs as evidenced by the higher latencies. Naturally, job slowdowns will cause additional congestion and queueing, which completes the sustaining loop (iv).</P>
<P>Similar incidents recur many times, and engineers take different approaches to mitigate/fix this issue. For example, (i) observing unusually high latency spikes in backend services resulted in work to improve their performance to lower queue lengths, (ii) observing higher GC duration than normal resulted in adjusting the JVM memory configuration (e.g., increasing max heap size) to tweak GC behavior, and (iii) observing high resource utilization (e.g., CPU) resulted in adding more servers to lower per-server load. These approaches decrease system vulnerabilities and make it more robust to the trigger at the magnitude of the peak load test level.</P></DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class="field-item even" style="FONT-SIZE: 18px; FONT-FAMILY: Muli, sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(0,0,0); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">
<DIV class="entity entity-paragraphs-item paragraphs-item-article-image view-mode-full view-mode-full--paragraphs_item view-mode-full--paragraphs_item--article_image" style="BORDER-BOTTOM: rgb(20,53,147) 3px solid; PADDING-BOTTOM: 20px; PADDING-TOP: 20px">
<DIV class=content>
<DIV class="field field-name-field-article-image field-type-image field-label-hidden">
<DIV class=field-items>
<DIV class="field-item odd"><IMG style="MAX-WIDTH: 100%; BORDER-TOP: 0px; HEIGHT: auto; BORDER-RIGHT: 0px; BORDER-BOTTOM: 0px; BORDER-LEFT: 0px" alt="" src="https://www.usenix.org/sites/default/files/styles/article_embedded/public/case_study_scatter.png?itok=DiDkAK9p" width=1440 height=545></DIV></DIV></DIV>
<DIV class="field field-name-field-article-image-caption field-type-text field-label-hidden" style="FONT-SIZE: 14px; FONT-WEIGHT: 600; COLOR: rgb(0,0,0); FONT-STYLE: normal">
<DIV class=field-items>
<DIV class="field-item odd">Figure 2: Correlation between metrics during 3 normal days at Twitter.</DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class="field-item odd" style="FONT-SIZE: 18px; FONT-FAMILY: Muli, sans-serif; WHITE-SPACE: normal; WORD-SPACING: 0px; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; COLOR: rgb(0,0,0); FONT-STYLE: normal; ORPHANS: 2; WIDOWS: 2; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(255,255,255); TEXT-INDENT: 0px; font-variant-ligatures: normal; font-variant-caps: normal; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial">
<DIV class="entity entity-paragraphs-item paragraphs-item-single-column-text view-mode-full view-mode-full--paragraphs_item view-mode-full--paragraphs_item--single_column_text" style="PADDING-BOTTOM: 20px; PADDING-TOP: 20px">
<DIV class=content>
<DIV class="field field-name-field-single-column-sub field-type-text field-label-hidden" style="FONT-SIZE: 40px; FONT-FAMILY: Klavika, sans-serif; TEXT-TRANSFORM: none; FONT-WEIGHT: 400; PADDING-TOP: 20px; MARGIN: 0px; LINE-HEIGHT: 50px">
<DIV class=field-items>
<DIV class="field-item odd">Replicating Metastability</DIV></DIV></DIV>
<DIV class="field field-name-field-single-column-text field-type-text-long field-label-hidden">
<DIV class=field-items>
<DIV class="field-item odd">
<P>In this section, we develop a small-scale reproduction of the GC metastable failure seen in the Twitter case study. This allows us to perform controlled experiments to validate the sustaining effect and study the factors that affect vulnerability. Our code is open-sourced<SPAN>&nbsp;</SPAN><A title=GC_Metastability style="TEXT-DECORATION: none; COLOR: rgb(3,57,216); BACKGROUND-COLOR: transparent" href="https://github.com/lexiangh/Metastability/tree/main/GC_Metastability" rel=nofollow>here</A>. We confirm that GC can cause metastability and that the vulnerability increases with load. Since the sustaining effect is due to a high queue length causing memory pressure and GC slowdowns, we find that the memory size also impacts the degree of vulnerability.