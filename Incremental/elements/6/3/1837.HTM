<b> : </b><SPAN class=section-number style="BOX-SIZING: border-box">5.2.1.<SPAN>&nbsp;</SPAN></SPAN>Application Level<A title="Permalink to this headline" class=headerlink style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: ; FONT: 14px/1 FontAwesome; MARGIN-LEFT: 0.5em; DISPLAY: inline-block; text-rendering: auto; -webkit-font-smoothing: antialiased; opacity: 0" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-level">&#61633;</A></H3>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">At a high level, the application should maximize parallel execution between the host, the devices, and the bus connecting the host to the devices, by using asynchronous functions calls and streams as described in<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution"><FONT color=#0066cc size=3>Asynchronous Concurrent Execution</FONT></A>. It should assign to each processor the type of work it does best: serial workloads to the host; parallel workloads to the devices.</P>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">For the parallel workloads, at points in the algorithm where parallelism is broken because some threads need to synchronize in order to share data with each other, there are two cases: Either these threads belong to the same block, in which case they should use<SPAN>&nbsp;</SPAN><CODE class="docutils literal notranslate" style="BOX-SIZING: border-box; FONT-SIZE: 12px; MAX-WIDTH: 100%; BORDER-TOP: rgb(225,228,229) 1px solid; FONT-FAMILY: var(--nv-font-face-mono); BORDER-RIGHT: rgb(225,228,229) 1px solid; BACKGROUND: rgb(255,255,255); WHITE-SPACE: normal; OVERFLOW-X: auto; BORDER-BOTTOM: rgb(225,228,229) 1px solid; COLOR: rgb(231,76,60); PADDING-BOTTOM: 2px; PADDING-TOP: 2px; PADDING-LEFT: 5px; BORDER-LEFT: rgb(225,228,229) 1px solid; PADDING-RIGHT: 5px"><SPAN class=pre style="BOX-SIZING: border-box">__syncthreads()</SPAN></CODE><SPAN>&nbsp;</SPAN>and share data through shared memory within the same kernel invocation, or they belong to different blocks, in which case they must share data through global memory using two separate kernel invocations, one for writing to and one for reading from global memory. The second case is much less optimal since it adds the overhead of extra kernel invocations and global memory traffic. Its occurrence should therefore be minimized by mapping the algorithm to the CUDA programming model in such a way that the computations that require inter-thread communication are performed within a single thread block as much as possible.</P></SECTION><SECTION id=device-level style="BOX-SIZING: border-box; DISPLAY: block">