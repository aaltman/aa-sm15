<b> : </b><SPAN class=section-number style="BOX-SIZING: border-box">5.3.<SPAN>&nbsp;</SPAN></SPAN>Maximize Memory Throughput<A title="Permalink to this headline" class=headerlink style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: ; FONT: 14px/1 FontAwesome; MARGIN-LEFT: 0.5em; DISPLAY: inline-block; text-rendering: auto; -webkit-font-smoothing: antialiased; opacity: 0" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-memory-throughput">&#61633;</A></H2>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth.</P>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">That means minimizing data transfers between the host and the device, as detailed in<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#data-transfer-between-host-and-device"><FONT color=#0066cc size=3>Data Transfer between Host and Device</FONT></A>, since these have much lower bandwidth than data transfers between global memory and the device.</P>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">That also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i.e., L1 cache and L2 cache available on devices of compute capability 2.x and higher, texture cache and constant cache available on all devices).</P>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it. As illustrated in<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-c-runtime"><FONT color=#0066cc size=3>CUDA Runtime</FONT></A>, a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each thread of a block:</P>
<UL class=simple style="LIST-STYLE-TYPE: disc; BOX-SIZING: border-box; PADDING-BOTTOM: 0px; PADDING-TOP: 0px; PADDING-LEFT: 0px; MARGIN: 0px 0px 24px; LINE-HEIGHT: 24px; PADDING-RIGHT: 0px">
<LI style="LIST-STYLE-TYPE: disc; BOX-SIZING: border-box; MARGIN-LEFT: 24px">
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 0px 5px 0px 0px">Load data from device memory to shared memory,</P>
<LI style="LIST-STYLE-TYPE: disc; BOX-SIZING: border-box; MARGIN-LEFT: 24px">
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 0px 5px 0px 0px">Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were populated by different threads,</P>
<LI style="LIST-STYLE-TYPE: disc; BOX-SIZING: border-box; MARGIN-LEFT: 24px">
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 0px 5px 0px 0px">Process the data in shared memory,</P>
<LI style="LIST-STYLE-TYPE: disc; BOX-SIZING: border-box; MARGIN-LEFT: 24px">
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 0px 5px 0px 0px">Synchronize again if necessary to make sure that shared memory has been updated with the results,</P>
<LI style="LIST-STYLE-TYPE: disc; BOX-SIZING: border-box; MARGIN-LEFT: 24px">
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 0px 5px 0px 0px">Write the results back to device memory.</P></LI></UL>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">For some applications (for example, for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality. As mentioned in<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x"><FONT color=#0066cc size=3>Compute Capability 7.x</FONT></A>,<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-8-x"><FONT color=#0066cc size=3>Compute Capability 8.x</FONT></A><SPAN>&nbsp;</SPAN>and<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-9-0"><FONT color=#0066cc size=3>Compute Capability 9.0</FONT></A>, for devices of compute capability 7.x, 8.x and 9.0, the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus shared memory is configurable for each kernel call.</P>
<P style="BOX-SIZING: border-box; COLOR: ; MARGIN: 15px 5px 15px 0px">The throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory. The next step in maximizing memory throughput is therefore to organize memory accesses as optimally as possible based on the optimal memory access patterns described in<SPAN>&nbsp;</SPAN><A class="reference external" style="BOX-SIZING: border-box; CURSOR: pointer; TEXT-DECORATION: none; COLOR: " href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses"><FONT color=#0066cc size=3>Device Memory Accesses</FONT></A>. This optimization is especially important for global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact on performance.</P><SECTION id=data-transfer-between-host-and-device style="BOX-SIZING: border-box; DISPLAY: block">