In<SPAN>&nbsp;</SPAN><A title="Machine learning" style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</A><SPAN>&nbsp;</SPAN>problems that involve learning a "state-of-nature" from a finite number of data samples in a high-dimensional<SPAN>&nbsp;</SPAN><A title="Feature space" class=mw-redirect style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Feature_space">feature space</A><SPAN>&nbsp;</SPAN>with each feature having a range of possible values, typically an enormous amount of training data is required to ensure that there are several samples with each combination of values. In an abstract sense, as the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.<SUP id=cite_ref-4 class=reference style="FONT-SIZE: 12px; WHITE-SPACE: nowrap; FONT-WEIGHT: normal; FONT-STYLE: normal; LINE-HEIGHT: 1"><A style="TEXT-DECORATION: none; BACKGROUND: none transparent scroll repeat 0% 0%; COLOR: ; border-radius: 2px; overflow-wrap: break-word" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#cite_note-4"><FONT style="BACKGROUND-COLOR: #ffffff" face=Arial>[4]</FONT></A></SUP>