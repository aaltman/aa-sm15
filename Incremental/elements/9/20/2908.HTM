<H3><STRONG><FONT class=extract>1.1 The purpose of caching</FONT></STRONG></H3>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>Caching is based on the&nbsp;<B data-stringify-type="bold">principle of locality</B>, which means that programs repeatedly access data located close to each other.&nbsp;There are two kinds of locality:</FONT></SPAN></P>
<UL>
<LI><FONT class=extract><STRONG>Temporal locality</STRONG>, where data that has been referenced recently is likely to be referenced again (i.e. <EM>time-based</EM> locality). </FONT>
<LI><FONT class=extract><STRONG>Spatial locality</STRONG>, where data that is stored near recently referenced data is also likely to be referenced (i.e. <EM>space-based&nbsp;</EM>locality).</FONT></LI></UL>
<P><FONT class=extract>We can speed up&nbsp;a system by making it faster to retrieve this locally relevant data. Caching does this by storing a &#8220;cached&#8221; copy of the data, in a smaller, faster data store. For example a CPU has a cache in specialized SRAM memory on the processor so it doesn&#8217;t have to go all the way to RAM or Disk.&nbsp;</FONT></P>
<H3><STRONG><FONT class=extract>1.2 How a cache is used by an application</FONT></STRONG></H3>
<P><FONT class=extract>When a data request goes through a cache, a copy of the data is either already in the cache, which is called a <STRONG>cache hit</STRONG>, or it&#8217;s not and the data has to be fetched from the primary store and copied into the cache, which is called a <STRONG>cache miss</STRONG>. A cache is performing well when there are many more cache hits than cache misses.&nbsp;</FONT></P>
<P><FONT class=extract><IMG alt="Cache miss and cache hit" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_1_2x_1_1024x1024/eec4f3e7daf8307cfe1ddcacfa3a1a21.webp" width=660 height=495 loading="lazy"></FONT></P>
<H3><STRONG><FONT class=extract>1.3 When to use caching</FONT></STRONG></H3>
<UL>
<LI style="FONT-WEIGHT: 400"><FONT class=extract><SPAN style="FONT-WEIGHT: 400">Caching is most helpful when the data you need is slow to access, possibly because of slow hardware, having to go over the network, or complex computation.</SPAN> </FONT>
<LI style="FONT-WEIGHT: 400"><FONT class=extract><SPAN style="FONT-WEIGHT: 400">Caching is helpful in systems where there are many requests to static or slow to change data, because repeated requests to the cache will be up to date.&nbsp;</SPAN> </FONT>
<LI style="FONT-WEIGHT: 400"><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>Caching can also reduce load on primary data stores, which has the downstream effect of reducing service costs to reach performant response times.&nbsp;</FONT></SPAN></LI></UL>
<P><FONT class=extract><SPAN style="FONT-WEIGHT: 400">Ultimately the features of a successful caching layer are highly situation-dependent. Creating and optimizing a cache requires tracking </SPAN><STRONG>latency</STRONG> and <STRONG>throughput</STRONG> metrics, and tuning parameters to the particular data access patterns of the system.&nbsp;</FONT></P>
<H3><STRONG><FONT class=extract>1.4 When not to use caching</FONT></STRONG></H3>
<UL>
<LI style="FONT-WEIGHT: 400"><FONT class=extract><SPAN style="FONT-WEIGHT: 400">Caching isn&#8217;t helpful when it takes just as long to access the cache as it does to access the primary data.</SPAN> </FONT>
<LI style="FONT-WEIGHT: 400"><FONT class=extract><SPAN style="FONT-WEIGHT: 400">Caching doesn&#8217;t work as well when requests have low repetition (higher randomness), because caching performance comes from repeated memory access patterns.&nbsp;</SPAN> </FONT>
<LI style="FONT-WEIGHT: 400"><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>Caching isn&#8217;t helpful when the data changes frequently, as the cached version gets out of sync and the primary data store must be accessed every time.&nbsp;</FONT></SPAN></LI></UL>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>It&#8217;s important to note that caches should not be used as permanent data storage - they are almost always implemented in volatile memory because it is faster, and should thus be considered transient. In section 2 below, we&#8217;ll talk more about strategies for writing to a caching layer that don&#8217;t cause data loss.</FONT></SPAN></P>
<H3><STRONG><FONT class=extract>1.5 Cache performance</FONT></STRONG></H3>
<P><FONT class=extract>Cache performance is affected by the way data is mapped into the cache. Data is located the fastest in a cache if it&#8217;s mapped to a single cache entry, which is called a <STRONG>direct-mapped cache</STRONG>. But if too many pieces of data are mapped to the same location, the resulting <STRONG>contention</STRONG> increases the number of cache misses because relevant data is replaced too soon.&nbsp;</FONT></P>
<P><FONT class=extract><IMG alt="Direct mapped cache" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_2_2x_1_1024x1024/b73efa1bc324a76948fd6ac980499f31.webp" width=660 height=495 loading="lazy"></FONT></P>
<P><FONT class=extract>The widely accepted solution is to use a <STRONG>set-associative cache</STRONG> design. Each piece of data is mapped to a set of cache entries, which are all checked to determine a cache hit or miss. This is slightly slower than only having to check a single entry, but gives flexibility around choosing what data to keep in the cache. Below, we&#8217;ll talk more about cache replacement algorithms and their design tradeoffs.&nbsp;</FONT></P>
<P><FONT class=extract><IMG alt="Set-associative cache" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_3_2x_1_1024x1024/539982ead7ef0eb9e029853f86cdc0e9.webp" width=660 height=495 loading="lazy"></FONT></P>
<P><FONT class=extract>One final thing to note is the performance of a cache on startup. Since caching improves repeated data access, there&#8217;s no performance benefit on a <STRONG>cold start</STRONG>. A cache can be <STRONG>warmed up</STRONG> to make sure it doesn&#8217;t lag on startup by <STRONG>seeding</STRONG> it with the right data before any operations occur. This can be a set of static data that is known beforehand to be relevant, or predicted based on previous usage patterns.&nbsp;</FONT></P>
<P><FONT class=extract>Now that we&#8217;ve covered the basics of caching, let&#8217;s look more closely at different ways of implementing a cache. First, we&#8217;ll look at &#8220;policies&#8221; for writing to a cache.&nbsp;</FONT></P>
<H2><SPAN style="FONT-WEIGHT: 400"><A name=writing-policies></A><STRONG><FONT class=extract>2. Cache writing policies</FONT></STRONG></SPAN></H2>
<P><FONT class=extract>There are interesting design tradeoffs between the speed of writing and the consistency with persisted data. A cache is made of copies of data, and is thus transient storage, so when writing we need to decide when to write to the cache and when to write to the primary data store.</FONT></P>
<P><FONT class=extract>A <STRONG>write-behind cache</STRONG> writes first to the cache, and then to the primary datastore. This can either mean that the primary data store is updated almost immediately, or that the data is not persisted until the cache entry is replaced, in which case it&#8217;s tagged with a <STRONG>dirty bit</STRONG> to keep track that the data is out of sync.</FONT></P>
<P><FONT class=extract>It is possible that at the moment a write-behind cache fails that some data has not been persisted, and is lost. Ultimately writes will be fastest in a write-behind cache, and it can be a reasonable choice if the cached data tolerates possible data loss.</FONT></P>
<P><FONT class=extract>In a&nbsp;<STRONG>write-around cache</STRONG> design, the application writes directly to the primary datastore, and the cache checks with the primary datastore to keep the cached data valid. If the application is accessing the newest data, the cache might be behind, but the write doesn&#8217;t have to wait on two systems being updated and the primary datastore is always up to date.&nbsp;</FONT></P>
<P><FONT class=extract>Finally, a&nbsp;<STRONG>write-through cache</STRONG> updates both the cache data and the backing data at the same time. If the cache layer fails, then the update isn&#8217;t lost because it&#8217;s been persisted. In exchange, the write takes longer to succeed because it needs to update the slower memory.&nbsp;</FONT></P>
<P><FONT class=extract><IMG alt="Cache writing policies" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_4_2x_1_1024x1024/36bccfda8bccee11deca1341630c2c4a.webp" width=660 height=680 loading="lazy"></FONT></P>
<P><FONT class=extract>A cache has a limited amount of available memory, so at some point we need to clear out the old data and make space for more relevant data. The choice of which data to remove is made by a cache replacement policy.</FONT></P>
<H2><SPAN style="FONT-WEIGHT: 400"><A name=replacement-policies></A><STRONG><FONT class=extract>3. Cache replacement policies</FONT></STRONG></SPAN></H2>
<P><FONT class=extract>The<STRONG> cache replacement policy</STRONG> is an essential part of the success of a caching layer. The replacement policy (also called <STRONG>eviction policy</STRONG>) decides what memory to free when the cache is full.</FONT></P>
<P><FONT class=extract>A good replacement policy will ensure that the cached data is as relevant as possible to the application, that is, it utilizes the principle of locality to optimize for cache hits. Replacement policies are fine-tuned to particular use cases, so there are many different algorithms and implementations to choose from. But they&#8217;re all based on a few fundamental policies:</FONT></P>
<H3><STRONG><FONT class=extract>3.1 LRU - least recently used</FONT></STRONG></H3>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>In an LRU replacement policy, the entry in the cache that is the oldest will be freed. LRU performs well and is fairly simple to understand, so it&#8217;s a good default choice of replacement policy. </FONT></SPAN></P>
<P><FONT class=extract><SPAN style="FONT-WEIGHT: 400">To implement LRU the cache keeps track of recency with </SPAN><STRONG>aging bits</STRONG> that need to get updated on every entry every time data is accessed. Although LRU makes efficient decisions about what data to remove, the computational overhead of keeping track of the aging bits leads to approximations like the clock replacement policy.&nbsp;</FONT></P>
<P><FONT class=extract>A clock replacement policy approximates LRU with a clock pointer that cycles sequentially through the cache looking for an available entry. As the pointer passes an in-use entry, it marks the <STRONG>stale bit</STRONG>, which gets reset whenever the entry is accessed again. If the clock pointer ever finds an entry with a positive stale bit, it means the cached data hasn&#8217;t been accessed for an entire cycle of the clock, so the entry is freed and used as the next available entry.&nbsp;</FONT></P>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>LRU-based algorithms are well suited for applications where the oldest data are the least likely to be used again. For example, a local news outlet where users mostly access today&#8217;s news could use a CDN with LRU replacement to make today&#8217;s news faster to load. After that day has passed, it becomes less likely that the news article is accessed again, so it can be freed from the cache to make space for the next day&#8217;s news.&nbsp;</FONT></SPAN></P>
<H3><STRONG><FONT class=extract>3.2 LFU - least frequently used</FONT></STRONG></H3>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>In an LFU replacement policy, the entry in the cache that has been used the least frequently will be freed. Frequency is tracked with a simple access count in the entry metadata.&nbsp;</FONT></SPAN></P>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>LFU replacement policies are especially helpful for applications where infrequently accessed data is the least likely to be used again. For example, an encyclopedia-like service could have certain articles that are popular (e.g. about celebrities) and others that are more obscure. </FONT></SPAN></P>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>With a CDN running LFU replacement would mean the popular articles are persisted in the cache and faster to load, while the obscure articles are freed quickly.&nbsp;</FONT></SPAN></P>
<H3><STRONG><FONT class=extract>3.3 Balancing LRU and LFU</FONT></STRONG></H3>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>Both LRU and LFU have advantages for particular data access patterns, so it&#8217;s common to see them combined in various ways to optimize performance. An LFRU (Least Frequently Recently Used) replacement policy is one such example. It takes into account both recency and frequency by starting with LFU, and then moving to LRU if the data is used frequently enough.&nbsp;</FONT></SPAN></P>
<P><FONT class=extract><SPAN style="FONT-WEIGHT: 400">To respond to </SPAN><EM><SPAN style="FONT-WEIGHT: 400">changing</SPAN></EM><SPAN style="FONT-WEIGHT: 400"> data access patterns, an ARC (Adaptive Replacement Cache) takes the LFRU approach and then dynamically adjusts the amount of LFU and LRU cache space based on recent replacement events. In practice, ARC will outperform LFU and LRU, but requires the system to tolerate much higher complexity which isn&#8217;t always feasible. </SPAN></FONT></P>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>ARC&#8217;s are valuable when access patterns vary, for example a search engine where sometimes requests are a stable set of popular websites, and sometimes requests cluster around particular news events.&nbsp;</FONT></SPAN></P>
<H3><STRONG><FONT class=extract>3.4 Expiration policies</FONT></STRONG></H3>
<P><FONT class=extract><SPAN style="FONT-WEIGHT: 400">In distributed systems there is often an explicit </SPAN><STRONG>expiration policy</STRONG> or <STRONG>retention policy</STRONG> for making sure there is always space in the cache for new data. This parameter specifies an amount of time after which a resource is freed if it hasn&#8217;t been used, called the <STRONG>Time To Live (TTL)</STRONG>.</FONT></P>
<P><FONT class=extract>Finding the right TTL is one way to optimize a caching layer. Sometimes explicit removal policies are <STRONG>event-driven</STRONG>, for example freeing an entry when it is written to.&nbsp;</FONT></P>
<H2><SPAN style="FONT-WEIGHT: 400"><A name=distributed-cache></A><STRONG><FONT class=extract>4. Distributed cache&nbsp;</FONT></STRONG></SPAN></H2>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract>In a distributed system a caching layer can reside on each machine in the application service cluster, or it can be implemented in a cluster isolated from the application service.</FONT></SPAN></P>
<P><FONT class=extract>A <STRONG>private</STRONG> cache exists in the memory of each application computer in the service cluster. Since each machine stores its own cache, the cached data will get out of sync based on what requests have been directed to that machine, and at what time.</FONT></P>
<P><FONT class=extract>The main advantage of in-memory distributed caches is speed - since the cache is in memory, it&#8217;s going to function a lot faster than a shared caching layer that requires network requests to operate. <STRONG>Redis</STRONG> and <STRONG>Memcached</STRONG> are two common choices for private cache implementations.&nbsp;</FONT></P>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract><IMG alt="Private cache" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_5_2x_1_1024x1024/c572e7f7703c73640e86d636cfa1954f.webp" width=660 height=495 loading="lazy"></FONT></SPAN></P>
<P><FONT class=extract>A <STRONG>shared</STRONG> cache exists in an isolated layer. In fact, the application might not be aware that there&#8217;s a cache. This allows the cache layer and application layer to be scaled independently, and for many different types of application services to use the same cache.</FONT></P>
<P><FONT class=extract>The application layer needs to be able to detect the availability of the shared cache, and to switch over to the primary data store if the shared cache ever goes down. A shared cache implemented on its own cluster has the same resiliency considerations as any distributed system - including failovers, recovery, partitioning, rebalancing, and concurrency.&nbsp;</FONT></P>
<P><FONT class=extract><IMG alt="Shared cache" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_6_2x_1_1024x1024/65b08eb3d3ef16f551985531dd16935e.webp" width=660 height=660 loading="lazy"></FONT></P>
<P><FONT class=extract>Private and shared caches can be used together, just like CPU architectures have different levels of caching. The private cache can also be a fallback that retains the advantages of caching in case the shared cache layer fails.</FONT></P>
<P><SPAN style="FONT-WEIGHT: 400"><FONT class=extract><IMG alt="Examples of caching" src="https://d3no4ktch0fdq4.cloudfront.net/public/CACHE/images/blog-articles/content-images/Diagram_7_2x_1_1024x1024/030541fe2ddea77a3dc7c21e9aa33af0.webp" width=660 height=495 loading="lazy"></FONT></SPAN>