Viewstamped Replication is the underdog of consensus algorithms: invented just before Paxos, and revisited in 2012, it so far missed the hype train. Similar to Raft, it is a batteries included algorithm: leader election(primary change), recovery, reconfiguration, and client interaction are among the topics that, besides consensus(replication), are dealt with in the research paper. But, it comes with an advantage over Raft: its ability to run without the need for stable storage &#8212; an important advantage given how hard it is to use stable storage safely.</P>
<P>The protocol is used in industry, forming the backbone of the replication layer of Tigerbeetle; in fact, the enthusiastic introduction to the algorithm by Joran, the founder of Tigerbeetle, is what motivated me to look further into it. And so, similar to what I did for Paxos and the bakery algorithm, this article will describe my experience with specifying an implementation of Viewstamped Replication, using TLA+, and implementing it using Rust and Automerge.</P>
<P>Full spec and code available at <A href="https://github.com/gterzian/understand-vsr">https://github.com/gterzian/understand-vsr</A></P>
<P>The specification<BR>&#8220;Viewstamped Replication Revisited&#8221; is a breeze to read. So much so that I don&#8217;t feel the need to add any kind of informal description of the protocol here: just read the paper. But, the algorithm described is more subtle then it appears. Going through the exercise of specifying it using TLA+ proved to be a valuable experience: I made many errors, and the TLC model checker caught them all(although I can&#8217;t be sure, as we&#8217;ll see below). If I had gone straight to the code, these errors would have taken more time to surface, or worse: they could have remained hidden.</P>
<P>An example is in order: the paper contains the following sentence in Part 3 Overview:</P>
<P>The primary is chosen round-robin, starting with replica 1, as the system moves to new views.</P>
<P>The safety of view changes is discussed later, in part 8.1, and it does not mention the round-robin approach as being key to it. So, for some unknown reason, I initially modeled the choice of primary as deterministic, but not one that follows the round-robin approach. TLA+ has a nice CHOOSE operator, and &#8212; perhaps in an example of the hammer fallacy &#8212; that&#8217;s what I used.</P>
<P>It took TLC about seven minutes to catch the error: two replicas would consider themselves primary for a given view, and due to re-delivery of messages could also consider themselves &#8220;elected&#8221;. In other words, messages meant for one replica were re-delivered to the other replica. My initial fix was to add a destination primary for each message, thereby linking a given &#8220;vote&#8221; message(known as DOVIEWCHANGE) to not only a view, but also to a primary for that view.</P>
<P>Re-reading the paper later, I realized that the round-robin was achieving exactly that: it links each view to a given replica as primary. Realizing my error, I removed all those redundant &#8220;destination primary&#8221; fields on messages, and moved to a proper round-robin approach to choosing primaries: the modulo operator coming to the rescue.</P>
<P>Now, when I write &#8220;It took TLC about seven minutes to catch the error,&#8221; I really mean: TLC ran the model for seven minutes, at which point one of the invariants I had defined in the spec were violated. TLC by default only checks for deadlocks; any other invariant to check is up to the author of the spec to define.</P>
<P>The invariants applied to the spec are expressed in the theorem at the bottom: THEOREM Spec =&gt; [](TypeOk /\ ViewChangeOk /\ IViewChangeOk /\ RecoveryOk). But for TLC to apply them when running the model, one needs to configure their use: either via the UI or with a .cfg file.</P>
<P>See Chapter 14 of &#8220;Specifying Systems&#8221; for a primer on the model checker.</P>
<P>This bring me to my only real, as opposed to aesthetic, reservation about the paper: it contains only an informal discussion of its safety properties. I have tried to translate those into invariants in TLA+, and also tried to formulate the inductive invariants that imply those. But, it is not clear to me that they cover the same ground as the informal safety properties described in the paper.</P>
<P><BR><A href="https://github.com/gterzian/understand-vsr/blob/cd0c690ad68eadc839c00d6d4bbeb5c4767337bf/VSR.tla#L126">https://github.com/gterzian/understand-vsr/blob/cd0c690ad68eadc839c00d6d4bbeb5c4767337bf/VSR.tla#L126</A><BR>If you read the first part of my series on Paxos, you may remember that I was also not sure whether the invariants I defined, especially the inductive invariant, were truly expressing the safety properties of the protocol. But this was a self-imposed restriction: I wanted to go through the exercise of defining them for myself. The original Paxos paper does in fact contain an inductive invariant, at A2 Proof of Consistency, and it would have been trivial for me to translate it into TLA+.</P>
<P>For a discussion of invariants, inductive invariants, and why they are superior to behavioral proofs, see the first part of my series on Paxos.</P>
<P>So, even though reading &#8220;ViewStamped Replication Revisited&#8221; felt easy, I have only a moderate amount of confidence in my spec, and only a vague idea of what the safety properties of the protocol are.</P>
<P>As I moved to the Rust implementation, I rarely looked at the paper again, instead looking at my TLA+ spec as a blueprint. The spec &#8212; defining all pre- and post-condition of each actions &#8212; is clearer than the paper: when describing a sub-protocol in prose, the paper does not include conditions imposed by other sub-protocols. But, having confidence in a spec requires having confidence in the safety properties implied by it&#8212; a spec is only as good as its invariants.</P>
<P>Another example of the usefulness of a spec and the precise description of logical conditions it provides is found at <A href="https://ahelwer.ca/post/2023-04-05-checkpoint-coordination/">https://ahelwer.ca/post/2023-04-05-checkpoint-coordination/</A>.</P>
<P>The Implementation<BR>Once again, I&#8217;ve used Automerge wrapped by Automerge-repo, so that I could focus on a kind of &#8220;syncable semi-shared state&#8221; instead of network messages. That state is shown below.</P>
<P><BR><A href="https://github.com/gterzian/understand-vsr/blob/cd0c690ad68eadc839c00d6d4bbeb5c4767337bf/src/main.rs#L581">https://github.com/gterzian/understand-vsr/blob/cd0c690ad68eadc839c00d6d4bbeb5c4767337bf/src/main.rs#L581</A><BR>Vsr is the shared document, but each replica owns a region of the document: only a given replica will write to its own Replica data; all others will only read it. Since the document will sync in the background, a replica can:</P>
<P>Write to Replica as a way to send a message.<BR>Watch for changes to the document, and read other&#8217;s Replica as a way to receive a message.<BR>This does require translating &#8212; or, one might say: implementing under a refinement &#8212; our spec into the corresponding concepts using Automerge.</P>
<P>A replica runs several tasks, five of them implementing the respective sub-algorithms:</P>
<P>State transfer: to bring a lagging replica up-to-date.<BR>Recovery: to allow a crashed replica to restart.<BR>Viewchange: switching primaries.<BR>Primary: which implements the primary part of normal operations, and also executes the state machine.<BR>Backup: which implements the backup part of normal operations.<BR>In addition, each node also acts as a client to the system, running two tasks:</P>
<P>Increment: requesting an monotonic increment of the current state.<BR>Read: requesting a read of the current state.<BR>This implementation is essentially the equivalent of my previous multi-decree Paxos implementation, and sure enough, both come in at around 850 lines of code. But, that is not a good yardstick for complexity.</P>
<P>Due to its reduced complexity, I feel more confident about the correctness of my Paxos implementation. Why is the complexity reduced? Because multi-decree Paxos builds on a simple core algorithm: the synod. While understanding that algorithm requires some effort, once you do, it&#8217;s like having learned to ride a bike.</P>
<P>Multi-decree consists of giving numbers to a sequence of synod instances &#8212; a simple matter. The fact that these instances can run in parallel and complete in any order &#8212; allowing the infamous gaps in the log &#8212; allows the algorithm to run and &#8220;just do its thing&#8221;. Adding leader election is the kind of work I look forward to: a bit of thinking, a bit of modelling in TLA+, and finally a bit of coding(perhaps with an iteration or two) &#8212; all with the knowledge that the safety of the protocol is not at stake.</P>
<P>In comparison, ViewStamped Replication consists of several sub-protocols &#8212; normal operation, view change, state transfer, recovery, and reconfiguration &#8212; each of whom are interdependent and together form one large protocol. An error in implementing any of these sub-protocols can compromise the safety of the whole.</P>
<P>Besides these differences&#8212; rather aesthetic in nature &#8212; Viewstamped Replication is a practical algorithm: it comes with the benefit of being able to run entirely in memory(something one could do with Paxos too, if you added a kind of message-based recovery workflow), and the &#8220;batteries included&#8221; aspect seems to be popular(this is reminiscent of the endless Rails/Django vs micro web frameworks debate of old). I will only say that when the algorithm was revisited in 2012, formal safety properties would have been a useful addition to the paper. Perhaps something for the next visit?</P>
<P>Thank you for reading.