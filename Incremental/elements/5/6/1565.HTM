High Performance Concurrency Control and Commit Protocols in OLTP<BR>Databases</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Jack Waudby</P>
<P>School of Computing Newcastle University</P>
<P><BR>This dissertation is submitted for the degree of<BR>Doctor of Philosophy</P>
<P>&nbsp;</P>
<P><BR>December 2023</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>For Snowy &amp; Jessie,<BR>who possessed Beauty without Vanity, Strength without Insolence, Courage without Ferocity,<BR>and all the virtues of Man without his Vices.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Declaration</P>
<P><BR>I hereby declare that except where specific reference is made to the work of others, the contents of this dissertation are original and have not been submitted in whole or in part for consideration for any other degree or qualification in this, or any other university. This dissertation is my own work and contains nothing which is the outcome of work done in collaboration with others, except as specified in the text and Acknowledgements.</P>
<P>Jack Waudby December 2023</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Acknowledgements</P>
<P><BR>Over the course of my PhD, I was fortunate to have received support from a number of colleagues, collaborators, and friends.</P>
<P>Advisors I would like to start by thanking my supervisor, Dr Paul Ezhilchelvan, for his guidance and patience. Thank you for taking a chance on a student with little Computer Science background, it has been a pleasure to work with you over these past years. I would also think to thank Dr Jim Webber for his advice and continued support across my PhD, I have always taken so much from our discussions. I&#8217;ll be forever grateful to you both for introducing me to databases and distributed systems.</P>
<P>Colleagues My thanks to the staff of the Newcastle University Centre for Doctoral Training (CDT) in Cloud Computing for Big Data: to Professors Paul Watson and Darren Wilkinson for leading the CDT; thanks especially to Jen Wood and Andrew Turnbull, who literally keep the whole show on the road. Thanks to my fellow CDT PhD students for sharing their knowledge, experience and friendship. In particular thanks go to the members of Cohort 4, Georgia Atkinson, Julian Austin, Matthew Fisher, Rob Geada, Konstantinos (Sam) Georgopoulos, Carlos Vladimiro Gonz&#225;lez Zelaya, Benjamin Lam, Cameron Trotter, and Joshua Barney it has been a pleasure to share this journey with you all, I wish you all the best whatever you do next. Thanks to the other members of the (unofficial) Ez Labs, Thomas Cooper, George Stamatiadis, and Chris Johnson.<BR>viii&nbsp;</P>
<P>Neo4j Clustering Team During my internship at Neo4j I was fortunate to work with many outstanding people. Thanks to Hugo Firth, Aleksey Karasavov, Antony Butterfield, Bal&#225;zs Lendvai, Tselmeg Baasan, Ragnar Wernersson, Aur&#233;lien Arena, F&#225;bio Botelho, and Marie Gaillard.</P>
<P>Research visits and collaborations I would like to acknowledge the support of institutions which hosted my research visits: the Budapest University of Technology and Economics and Centrum Wiskunde &amp; Informatica. I would like to express my gratitude to G&#225;bor Sz&#225;rnyas and Ben Steer, with whom I worked closely through the Linked Data Benchmark Council&#8217;s Social Network Benchmark task force. Ben, thank you for sharing your knowledge, experience and friendship with me, I wish you all the best in your future endeavors. G&#225;bor you are a force of nature, I can not express in words how much you have helped me across my PhD, I will be forever grateful. Thanks to Professor Isi Mitrani, with whom I got the opportunity to work on several projects.</P>
<P>Friends and Family Thank you to all my friends for providing an invaluable support network. Also, thank you to the latest addition to the family, Freddie, for bringing a smile back to my face. Finally, thank you to my father, Stuart, to my mother, Heather, and my sister Holly. Not just for the past four years, but for the 24 years before that as well. You have given me endless support, opportunity, and encouragement over the years. Through the highs and lows your support has never wavered. This achievement is yours as much as it is mine. Thank you.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>A man with new ideas is a madman, until his ideas triumph.<BR>Marcelo Bielsa</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Abstract</P>
<P><BR>Depending on the required scale, modern data-orientated applications are built on top of either many-core or distributed OLTP databases. In both architectures, database concurrency control is a performance critical component. Additionally, in distributed OLTP databases, where a transaction can span across multiple data partitions, distributed atomic commitment is a necessity; unless the latter is designed judiciously, it can significantly degrade performance. This thesis explores both facets to address the inefficiencies and limitations of existing work. The wait-hit protocol is a serializable concurrency control protocol that is designed to offer high performance across all scale points. This thesis also investigates mixed serialization graph testing by applying the recently revived graph-based approach to concurrency control, which minimizes unnecessary aborts, to cater for transactions running at weaker isolation levels, a phenomenon common in practice. Protocols are developed for ensuring reciprocal consistency and edge-order consistency in distributed graph databases, guarantees unique to graph databases which without sufficient concurrency control can be violated causing irreparable data corruption. Finally, we demonstrate that epoch-based distributed databases can amortize atomic commitment costs. In addition to developing an analytical model for choosing the optimal epoch size in such databases, this thesis presents epoch-based<BR>multi-commit which can reduce wasted work when database nodes fail.<BR>Each protocol has been subjected to extensive performance evaluation either through simulations or implementation. Our experiments show that each protocol offers a marked improvement over the current state-of-the-art.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Table of contents</P>
<P>List of figures&nbsp;xix<BR>List of tables&nbsp;xxiii<BR>Nomenclature&nbsp;xxvii<BR>&nbsp;&nbsp;&nbsp; 1 Introduction&nbsp;1<BR>1.1&nbsp;Research Challenges&nbsp;.&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp;4<BR>1.2&nbsp;Contributions&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; . .&nbsp; .&nbsp; . .&nbsp; .&nbsp; . .&nbsp; .&nbsp; . .&nbsp; . .&nbsp; .&nbsp; . .&nbsp; .&nbsp; . .&nbsp; .&nbsp;6<BR>1.3&nbsp;Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .&nbsp;8<BR>1.4&nbsp;Publications&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp;9<BR>&nbsp;&nbsp;&nbsp; 2 Background&nbsp;15<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Database Concurrency Control&nbsp;16<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.1 Serializability&nbsp;17<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.2 Weak Isolation&nbsp;18<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.3 Concurrency Control Approaches&nbsp;20<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.4 Many-Core Database Concurrency Control&nbsp;23<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.5 Distributed Database Concurrency Control&nbsp;25<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.6 2PC: Research Strikes Back&nbsp;28<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Graph Processing&nbsp;29<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1 Graph Data Consistency&nbsp;31<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.2 Distributed Graph Databases&nbsp;32<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.3 Data Corruption in Distributed Graph Databases&nbsp;33<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Performance Evaluation Techniques&nbsp;38<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.1 Evaluation Framework&nbsp;40<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.2 YCSB&nbsp;41<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.3 SmallBank&nbsp;42<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.4 TATP&nbsp;42<BR>&nbsp;&nbsp;&nbsp; 3 Wait-Hit Protocol&nbsp;43<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Introduction&nbsp;44<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Design Goals&nbsp;45<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Wait-Hit Approach&nbsp;46<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Basic Wait-Hit Protocol&nbsp;49<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.1 Protocol Description&nbsp;50<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.2 Implementation Details&nbsp;51<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.3 Evaluation&nbsp;54<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.4 Discussion&nbsp;54<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5 Many-Core Wait-Hit Protocol&nbsp;56<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.1 Protocol Description&nbsp;57<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.2 Optimizations&nbsp;58<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.3 Implementation Details&nbsp;61<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.4 Evaluation&nbsp;62<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.5 SmallBank&nbsp;63<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.6 YCSB&nbsp;63<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.7 TATP&nbsp;63<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.8 Discussion&nbsp;66</P>
<P><BR>3.6<BR>Distributed Wait-Hit Protocol . . . . . . . . . . . . . . . . . . . . . . . . .<BR>66</P>
<P>3.6.1&nbsp;Protocol Description . . . . . . . . . . . . . . . . . . . . . . . . .<BR>69</P>
<P>3.6.2&nbsp;Discussion&nbsp;. .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .<BR>71</P>
<P>3.6.3&nbsp;Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<BR>72<BR>3.7<BR>Conclusion&nbsp;. .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .&nbsp; .<BR>73</P>
<P>3.7.1&nbsp;Further Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<BR>74</P>
<P>&nbsp;&nbsp;&nbsp; 4 Mixed Serialization Graph Testing&nbsp;79<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Introduction&nbsp;80<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Serialization Graph Testing&nbsp;82<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2.1 Protocol Description&nbsp;82<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2.2 Many-Core Implementation&nbsp;85<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Mixing in the Wild&nbsp;87<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4 Mixing Theory&nbsp;87<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4.1 System Model&nbsp;89<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4.2 Weak Isolation Levels&nbsp;90<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4.3 Mixing of Isolation Levels&nbsp;92<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5 Mixed Serialization Graph Testing&nbsp;93<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.1 Protocol Description&nbsp;94<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.2 Optimizations&nbsp;94<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.3 Discussion&nbsp;97<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.4 Implementation Details&nbsp;97<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6 Evaluation&nbsp;98<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.1 Isolation&nbsp;99<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.2 Update Rate&nbsp;101<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.3 Contention&nbsp;103<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.4 Scalability&nbsp;103<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.5 SmallBank&nbsp;105<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.6 TATP&nbsp;105<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.7 Concurrency Control Overhead&nbsp;108<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.8 Optimizations&nbsp;110<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.7 Conclusion&nbsp;111<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.7.1 Further Work&nbsp;113<BR>&nbsp;&nbsp;&nbsp; 5 Edge Consistency in Distributed Graph Databases&nbsp;115<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Introduction&nbsp;116<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Delta Protocol&nbsp;118<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.1 Protocol Description&nbsp;119<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.2 Correctness Reasoning&nbsp;120<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.3 Performance Evaluation Strategies&nbsp;122<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.4 Evaluation&nbsp;128<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Deterministic Reciprocal Consistency Protocol&nbsp;129<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3.1 Protocol Description&nbsp;130<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4 Deterministic Edge Consistency Protocol&nbsp;131<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4.1 Protocol Description&nbsp;132<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4.2 Approximate Model&nbsp;134<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4.3 Evaluation&nbsp;141<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5 Conclusion&nbsp;145<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5.1 Further Work&nbsp;146<BR>&nbsp;&nbsp;&nbsp; 6 A Performance Study of Epoch-based Commit Protocols&nbsp;149<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Introduction&nbsp;150<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2 Analytical Models for Epoch-Based Commit&nbsp;154<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.1 Protocol Description&nbsp;154<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.2 Modeling Assumptions&nbsp;156<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.3 Maximum Throughput&nbsp;157<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.4 Average response time&nbsp;162<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.5 Upper bound, Wu&nbsp;162<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.6 Lower bound, Wd&nbsp;165<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3 Epoch-based Multi-commit&nbsp;167<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3.1 Rationale and Approach&nbsp;167<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3.2 Motivation: TPC-C Case Study&nbsp;169<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3.3 Multi-Commit Protocol Description&nbsp;171<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.4 Performance Evaluation Strategies&nbsp;172<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5 Evaluation&nbsp;174<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5.1 Maximum Throughput&nbsp;176<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5.2 Average Response Time&nbsp;177<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5.3 Paired Affinity&nbsp;178<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.6 Conclusion&nbsp;183<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.6.1 Further Work&nbsp;183<BR>&nbsp;&nbsp;&nbsp; 7 Conclusions&nbsp;187<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.1 Thesis Summary&nbsp;188<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.2 Limitations&nbsp;189<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.3 Future Research Directions&nbsp;190<BR>References&nbsp;193</P>
<P>&nbsp;</P>
<P><BR>List of figures</P>
<P>1.1&nbsp;Thesis Structure. Topics are given in boxes with square corners. Algorithmic contributions are illustrated with rounded boxes and shaded in orange; the performance evaluation technique used is shaded in blue. . . . . . . . . . .&nbsp;8<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Conflict graph representation of s&nbsp;18<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Logical and storage views of a reciprocally consistent edge ab&nbsp;32<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Local and distributed edges&nbsp;33<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Interleavings of concurrent writes to a distributed edge by transactions Tx<BR>and Ty&nbsp;35<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.5 Logical and storage views of a reciprocally inconsistent edge ab&nbsp;36<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.6 Edge-Order Consistency violation&nbsp;38</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Conflict graph representation of the access histories in Table 3.1&nbsp;51<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 SmallBank &#8211; 100 customers (high contention).&nbsp;55<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 SmallBank &#8211; 100 customers (high contention).&nbsp;64<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 YCSB &#8211; Performance measurements for the protocols as the core count is increased from 1 to 40 cores with medium contention, &#952; = 0.8, and a<BR>balanced update rate U = 0.5.&nbsp;65<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5 TATP &#8211; 100 entries.&nbsp;67<BR>xx</P>
<P>List of figures</P>
<P><BR>3.6</P>
<P>Distributed wait-hit protocol messages exchanged during happy path execu-</P>
<P><BR>tion. Orange circles link to the textual description of the protocol. Note, the</P>
<P><BR>wait phase is indicated by the orange hashed area in the validation phase. . .&nbsp;74</P>
<P>4.1<BR>Conflict graph representation of s&nbsp;83</P>
<P>4.2<BR>Direct serialization graph (DSG) representation of H&nbsp;90</P>
<P>4.3<BR>DSG(HG0) displays a Dirty Write (G0) anomaly&nbsp;91</P>
<P>4.4<BR>DSG(HG1c) displays a Circular Information Flow (G1c) anomaly&nbsp;92</P>
<P>4.5<BR>DSG(HG2) displays a G2 anomaly&nbsp;92</P>
<P>4.6<BR>Mixed serialization graph representation of H&nbsp;93</P>
<P>4.7<BR>MSGT and individual transaction&#8217;s relevant views.&nbsp;96</P>
<P>4.8<BR>Isolation &#8211; SGT and MSGT with 40 cores when varying the proportion of<BR>Serializable transactions from 0%&nbsp; to 100% with medium contention&nbsp; &#952; = 0.8</P>
<P><BR>and 50% update rate.&nbsp;100</P>
<P>4.9<BR>Update Rate &#8211; Performance measurements at 40 cores for the protocols as</P>
<P><BR>the proportion of update transaction (U ) is varied from 0% to 100% with</P>
<P><BR>medium contention, &#952; = 0.8, and a low proportion of Serializable transactions,<BR>&#969; = 0.2.&nbsp;102</P>
<P>4.10<BR>Contention &#8211; SGT and MSGT with 40 cores when varying the contention<BR>(skew factor) in the YCSB workload with &#969; = 0.2, and U = 0.5.&nbsp;104</P>
<P>4.11<BR>Scalability &#8211; Performance measurements for the protocols as the core count<BR>is increased from 1 to 40 cores with medium contention, &#952; = 0.8, low</P>
<P><BR>proportion of Serializable transactions, &#969; = 0.2, and a medium update rate</P>
<P><BR>U = 0.5.&nbsp;106</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.12 SmallBank &#8211; 100 customers (high contention) with all transactions executed<BR>at Serializable isolation.&nbsp;107<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.13 TATP &#8211; 100 entries all transactions executed at Read Committed isolation.&nbsp;109</P>
<P>List of figures&nbsp;xxi</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.14 MSGT with various cycle checking strategies when varying the proportion of<BR>Serializable transactions from 0% to 100% with medium contention &#952; = 0.8,<BR>50% update rate, and 40 cores.&nbsp;112<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Interleavings of concurrent writes to a distributed edge by transactions Tx<BR>and Ty&nbsp;120<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Edge transitions between clean, half-corrupted and semantically corrupt states.124<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Time until operational corruption (logU measured in days).&nbsp;130<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4 Fraction of aborts.&nbsp;130<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5 Edge-Order Consistency violation&nbsp;133<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.6 Abort rate as a function of N&nbsp;142<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.7 Abort rate as a function of &#955;&nbsp;143<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.8 Larger network delays&nbsp;144<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.9 Different distribution of updates&nbsp;144<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Work and 2PC intervals of a cycle in the epoch-based commit protocol.&nbsp;154<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2 Observation period with full cycles having N &#8722; 1 operative nodes, N &#8722; 1 and<BR>N operative nodes, and N operative nodes.&nbsp;158<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3 A cycle in epoch-based multi-commit. A node&#8217;s epoch-dependency list is indicated as dep&nbsp;167<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.4 Number of commit groups vs proportion of distributed transactions. The red line indicates the threshold after which single commit group is the only<BR>outcome.&nbsp;170<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5 Maximum throughput as work interval a varied from 40 to 1800 ms&nbsp;175<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.6 System throughput estimates vs. a. Green dotted lines indicate regions with different gradients.&nbsp;176<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.7 Average response time (ms) in epoch-based commit vs. a in ms&nbsp;178<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.8 Simulations under paired-affinity as work interval a varied from 10 to 100 ms.179<BR>xxii&nbsp;List of figures</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.9 Maximum throughput as work interval a varied from 10 to 100 ms&nbsp;181<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.10 Average response time (ms).&nbsp;182</P>
<P>&nbsp;</P>
<P>List of tables</P>
<P>2.1&nbsp;Profiles for the evaluation framework workloads.&nbsp;41</P>
<P>3.1&nbsp;Example table with access history {transaction id, operation type}.&nbsp;53</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Isolation Levels Supported by ACID and NewSQL Databases&nbsp;88<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 YCSB Workload Factors&nbsp;99<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Overhead of maintaining accesses, conflict detection, cycle tests, aborts, and<BR>live-lock handling. Reported metric is throughput at 40 cores.&nbsp;110</P>
<P>6.1&nbsp;Parameters of the analytical models and simulation.&nbsp;173</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>List of Algorithms</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 Conflict Pair Detection&nbsp;51<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 Basic Wait-Hit Protocol commit procedure.&nbsp;52<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3 Basic Wait-Hit Protocol abort procedure.&nbsp;53<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4 Many-Core Wait-Hit Protocol commit procedure.&nbsp;59<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5 Many-Core Wait-Hit Protocol abort procedure.&nbsp;59<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6 AIMD wait-phase&nbsp;61<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7 MSGT Edge Insertion&nbsp;95</P>
<P>&nbsp;</P>
<P><BR>Nomenclature</P>
<P>Acronyms / Abbreviations</P>
<P>ACID Atomicity, Consistency, Isolation, Durability AIMD Additive Increase/Multiplicative Decrease BASE Basically Available Soft State<BR>BCC&nbsp;Balanced Concurrency Control</P>
<P>CG&nbsp;Conflict Graph</P>
<P>DBMS Database Management System</P>
<P>DFS&nbsp;Depth First Search</P>
<P>DSG&nbsp;Direct Serialization Graph</P>
<P>FIFO First in first out</P>
<P>GDBMS Graph Database Management System</P>
<P>HAT&nbsp;Highly Available Transaction</P>
<P>LDBC Linked Data Benchmark Council</P>
<P>LSQB Labelled Subgraph Query Benchmark<BR>xxviii&nbsp;Nomenclature<BR>MC &#8722; WHP Many Core Wait Hit Protocol MOCC Mostly Optimistic Concurrency Control MPT Multi-Partition Transaction</P>
<P>MSGT Mixed Serialization Graph Testing</P>
<P>MTTR Mean time to repair</P>
<P>MVCC Multi-Version Concurrency Control</P>
<P>OCC&nbsp;Optimistic Concurrency Control</P>
<P>TPC &#8722; C Transaction Processing Performance Council Benchmark C<BR>OLTP Online Transaction Processing</P>
<P>PPS&nbsp;Product-Parts-Supplier</P>
<P>RDBMS Relational Database Management System</P>
<P>SGT Serialization Graph Testing SNB Social Network Benchmark BI Business Intelligance<BR>S2PL Strict Two-Phase Locking</P>
<P>TATP Telecommunication Application Transaction Processing Benchmark</P>
<P>TL&nbsp;Terminated List</P>
<P>TO&nbsp;Timestamp Ordering 2PC&nbsp;Two-Phase Commit<BR>Nomenclature&nbsp;xxix</P>
<P>2PL&nbsp;Two-Phase Locking</P>
<P>TPS&nbsp;Transactions per second</P>
<P>WAN Wide Area Networks</P>
<P>WHP Wait-Hit Protocol</P>
<P>YCSB Yahoo Cloud Serving Benchmark</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Chapter 1 Introduction</P>
<P>Modern data-orientated applications are built on top of online transaction processing (OLTP) database management systems (DBMSs). These DBMSs are either many-core, single-node systems with 10s or 100s of cores, or distributed, multi-node systems each storing a disjoint portion of the database. In both types of database, concurrency control is a performance critical component. Concurrency control approaches are typically clas- sified as pessimistic or optimistic [12]. The performance of each approach differs based on workload characteristics and database type. Many-core databases opt for optimistic approaches [58, 110, 122], whereas it has been demonstrated that for distributed databases pessimistic approaches generally perform better [54]. Thus, transitioning from a many-core to distributed deployment often requires re-architecting the concurrency control strategy to avoid poor performance, which is not compatible with the modern cloud environment and user expectations. Contemporary cloud providers offer users a wide range of hardware options, enabling them to scale easily from a single-core machine to a many-core machine, to deployments spread across data centers and the globe. Naturally, when users deploy DBMSs in the cloud they expect performance to scale seamlessly as hardware resources are scaled, without needing to consider the performance implications imposed by the underlying concurrency control strategy.<BR>In OLTP DBMSs, another aspect that significantly impacts performance is the isolation guarantee provided. The gold standard isolation level is Serializability, which guarantees that concurrent transactions appear to have been isolated from each other, sequentially executing over the database which, assuming transactions are individually correct, guarantees a correct DBMS state [12]. In practice, efficient implementation of serializable transaction processing is challenging and its performance often unpalatable for applications&#8217; requirements [85, 99]. Thus, DBMSs allow transactions to be executed at different, weaker isolation levels, e.g., Read Committed [1, 51]. Weaker isolation levels increase the number of permissible executions compared to Serializability, which allows for more concurrency, thus higher<BR>3</P>
<P>throughput and lower latency. But, this comes at the cost of potential non-serializable, anomalous behaviour. Despite the ubiquitous usage of weak isolation in practice, the majority of research has focused on making the processing of transactions with Serializable isolation performant [85], leaving the efficient support of processing transactions with weaker isolation guarantees underserved.<BR>The quest for high DBMS performance at scale, combined with the misguided notion that transactions are inherently non-scalable, fueled the development of numerous distributed NoSQL databases, e.g., DynamoDB [26]. These databases forgo cross-partition transactional semantics, at best offering transactions within a single partition. This is suitable for databases with a key-value or document data model, but problematic for one class of database: graph databases (GDBMS) [92]. Graph databases use the labelled property graph data model in which data is modeled as nodes and edges, which additionally can be labelled and have properties. Unfortunately, system designers have attempted to build graph databases by adding a graph layer on top of NoSQL databases, e.g., JanusGraph [61]. The lack of cross-partition concurrency control can violate graph integrity and cause irreparable data corruption [41].<BR>In distributed DBMSs that do provide cross-partition, distributed transactions, getting all partitions to agree on the outcome of a transaction requires an atomic commitment protocol (normally, two-phase commit (2PC) [12]), which is a well-established bottleneck that can significantly degrade performance. Numerous techniques have tried to avoid or minimize distributed transactions [23, 24], or avoid 2PC all together [71, 104]. One promising approach is epoch-based commit, which proposes that 2PC be executed only once for all transactions processed within a time interval called an epoch, rather than executed per-transaction [72]. However, determining the right epoch size for a given workload and cluster configuration remains a challenge and is key to achieving the desired throughput and latency. Additionally, a unfortunate drawback with epoch-based commit is if a database node fails then all transactions</P>
<P>within the group must be rolled back and re-executed, potentially resulting in a significant degree of wasted work.</P>
<P>1.1&nbsp;Research Challenges</P>
<P>In this section, given the outstanding issues introduced above we now describe in more depth the research challenges in OLTP database concurrency control and atomic commitment addressed in this thesis.</P>
<P>A Concurrency Control Protocol for Any Scale</P>
<P>Modern cloud providers enable users to scale easily from a many-core machine, to a de- ployment spread across data centers and the globe through the provision of a wide array of hardware options. Naturally, when users deploy DBMSs in the cloud they expect the performance of a DBMS to scale as hardware resources are scaled. A critical component in realizing high performance in a DBMS is concurrency control. Unfortunately, many concurrency control protocols are not designed to scale well; a protocol architected for one scale point inevitably needs to be re-adapted for another. This thesis addresses the challenge of designing a high-performance concurrency control protocol that can effectively scale from a single core, to multicore, to the globe whilst offering good performance at each scale point, without having to relax from the gold standard isolation level, Serializability.</P>
<P>High Performance Weak Isolation</P>
<P>Weak isolation is common in practice, with practical DBMSs in fact being mixed systems, supporting a range of weaker isolation levels. Yet research has primarily focused on improv- ing serialization transaction processing performance. A recent development in serializable transaction processing has been the revival of graph-based concurrency control, which was<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Research Challenges&nbsp;5</P>
<P>historically deemed nonviable due to concerns over the computational costs of maintaining an acyclic conflict graph. Graph-based concurrency control has the theoretically optimal property of accepting all and only valid schedules and has been demonstrated to offer com- parable, and often higher, performance when compared to traditional and contemporary concurrency control protocols in a many-core database. This thesis explores the question, how can graph-based concurrency control be extended to a mixed DBMS supporting transac- tions executed at weak isolation levels, whilst accepting all and only valid executions? Such an approach would permit higher concurrency and performance.</P>
<P>Preserving Edge Consistency</P>
<P>Several contemporary distributed graph databases (e.g., JanusGraph) use existing NoSQL databases (e.g., Apache Cassandra) as a storage backend, adapting them with an API in order to handle a graph data model. This approach is attractive as the underlying store offers high scalability. However, it only offers weak isolation guarantees across database nodes which has serious ramifications for the integrity of graph database systems and can lead to irreversible database corruption. This thesis will address the challenge of developing a suite of lightweight concurrency control protocols tailored for graph database workloads, which maintain graph integrity and offer suitable transactional throughput and latency.</P>
<P>Optimizing Epoch-Based Commit</P>
<P>Achieving good performance in an epoch-based distributed database requires the database operator to select the right epoch size. This thesis develops two analytical models to estimate throughput and average latency in terms of epoch size taking into account load and failure conditions. Additionally, this thesis addresses the challenge of developing an epoch-based commit protocol that avoids rolling back all transactions within an epoch in the event of a</P>
<P>single node failure. Reducing wasted work decreases system load resulting from retries, and improves performance and user experience.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Contributions</P>
<P>The work presented in this thesis addresses the research challenges in Section 1.1 and makes the following contributions:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (i) Development and evaluation of the wait-hit protocol (WHP), a general purpose con- currency control protocol for any scale. It can effectively scale vertically, as the core count is increased on a given machine and horizontally, as the number of machines is increased in a cluster, whilst providing Serializable isolation.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (ii) Design and evaluation of mixed serialization graph testing (MSGT), a concurrency control protocol for mixed DBMSs. MSGT extends graph-based concurrency control to mixed environments using Adya&#8217;s mixing-correct theorem [1]. It preserves the desirable property of minimizing the number of aborted schedules, whilst providing database operators with a selection of isolation levels.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (iii) Three concurrency control protocols that guarantee the preservation of graph integrity in a distributed graph database are introduced and evaluated. Specifically, these protocols ensure reciprocal consistency [41] and edge-order consistency, guarantees unique to graph databases, are maintained.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (iv) Development of two analytical models of epoch-based commit to aid database adminis- trators in selecting the epoch size that offers the desired trade-off between throughput and latency.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Contributions&nbsp;7</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (v) Proposal of epoch-based multi-commit, which aims to minimize number of aborted transactions when failures occur, but also performs identically to epoch-based commit when failures do not occur.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Thesis Structure</P>
<P>This section provides the thesis structure, with an abridged chapter-by-chapter summary. The thesis structure is summarized in Figure 1.1.</P>
<P>&nbsp;</P>
<P>Weak Isolation (Ch.4)</P>
<P>&nbsp;</P>
<P>MSGT Protocol<BR>Simulation<BR>Epoch-based Multi Commit<BR>Simulation<BR>Implementation</P>
<P>Implementation<BR>Delta Protocol</P>
<P>Simulation</P>
<P>Simulation</P>
<P>Fig. 1.1 Thesis Structure. Topics are given in boxes with square corners. Algorithmic contributions are illustrated with rounded boxes and shaded in orange; the performance evaluation technique used is shaded in blue.</P>
<P><BR>Chapter 1 presents the motivations behind the work carried out as part of this thesis, high- lights the main contributions of the research, and describes the related peer-reviewed publications produced across the course of the PhD.<BR>Chapter 2 describes the necessary technical background material closely linked with the work performed in the chapters of this thesis.<BR>Chapter 3 develops the wait-hit protocol, an optimistic multi-versioned concurrency control protocol that scales vertically, with the core count, and horizontally, with the cluster size. We compare the protocol&#8217;s performance against classical and state-of-the-art protocols on metrics such as throughput, latency, and proportion of aborted transactions.</P>
<P>Chapter 4 investigates the widespread availability of weak isolation in commercial and open source database systems, and proposes mixed serialization graph testing. A concurrency control protocol that gives high performance on servers with many- cores minimizes the number of unnecessary aborts, and crucially permits concurrent transactions to be executed at a range of isolation levels. We evaluate the protocol&#8217;s performance using several popular transaction processing benchmarks that have been augmented to generate workloads containing transactions declared with different isolation requirements.<BR>Chapter 5 outlines the design of two protocols that preserve reciprocal consistency and one that preserves edge-order consistency in a distributed graph database. Approximate models are developed for each protocol to allow for a comprehensive performance evaluation. To the best of our knowledge this is the first attempt to develop concurrency control protocols specifically catered for graph databases.<BR>Chapter 6 develops two analytical models of epoch-based commit. These allow for the choosing of an epoch size that maximizes throughput, minimizes average response time, or seeks a trade-off between them. Additionally, epoch-based multi-commit is presented which aims to minimize transaction aborts in the event of node failures. We execute a performance study of the protocols.<BR>Chapter 7 summarizes the conclusions of each chapter in this thesis and outlines the areas for future work.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.4 Publications</P>
<P>Across the course of my PhD I have contributed to nine peer-reviewed publications, five of which have directly contributed to this thesis. Contributing publications are listed below in</P>
<P>reverse chronological order, with the relevant thesis chapter indicated. A short description is provided for each publication.<BR>Waudby, J., Ezhilchelvan, P., Mitrani, I. and Webber, J., 2022. A Performance Study of Epoch-based Commit Protocols in Distributed OLTP Databases. To appear: In 41st International Symposium on Reliable Distributed Systems, SRDS. [Chapter 6]<BR>Description: distributed OLTP systems execute the high-overhead, 2PC protocol at the end of every distributed transaction. Epoch-based commit proposes that 2PC be executed only once for all transactions processed within a time interval called an epoch. Increasing epoch duration allows more transactions to be processed before the common 2PC. It thus reduces 2PC overhead per transaction, increases throughput but also increases average transaction latency. Therefore, the ability to choose the right epoch size that offers the desired trade-off between throughput and latency is required. To this end, we develop two analytical models to estimate throughput and average latency in terms of epoch size taking into account load and failure conditions. We then present epoch-based multi-commit which, unlike epoch-based commit, seeks to avoid all transactions being aborted when failures occur, and also performs identically when failures do not occur. Our performance study identifies workload factors that make it more effective in preventing transaction aborts and concludes that the analytical models can be equally useful in predicting its performance as well.<BR>Waudby, J., Ezhilchelvan, P., and Webber, J., 2022. Pick &amp; Mix Isolation Levels: Mixed Serialization Graph Testing. To appear: In Proceedings of the 14th TPC Technology Conference on Performance Evaluation &amp; Benchmarking. [Chapter 4]<BR>Description: Serialization graph testing (SGT) faithfully implements the conflict graph theorem by aborting only those transactions that would actually violate serializability (intro- duce a cycle), thus maintaining the required acyclic invariant. Historically, SGT was deemed unviable in practice due to the high computational costs of maintaining an acyclic graph.</P>
<P>Research has however overturned this historical view by utilizing the increased computational power available due to modern hardware. Furthermore, a survey of 24 databases suggests that not all transactions demand conflict serializability but different transactions can perfectly settle for different, weaker isolation levels which typically require relatively lower overheads. Thus, in such a mixed environment, providing only the isolation level required of each transaction should, in theory, increase throughput and reduce aborts. This paper extends SGT for mixed environments subject to Adya&#8217;s mixing-correct theorem and demonstrates the resulting performance improvement. We augment the Yahoo! Cloud Serving Benchmark (YCSB) benchmark to generate transactions with different isolation requirements. Mixed serialization graph testing can achieve up to a 28% increase in throughput and a 19% decrease in aborts over SGT.<BR>&nbsp;&nbsp;&nbsp; [115] &nbsp;Waudby, J., 2022. High Performance Mixed Graph-Based Concurrency Control. In Proceedings of the VLDB 2022 PhD Workshop co-located with the 48th International Conference on Very Large Databases. [Chapter 4]<BR>Description: this is an abridged version of &#8220;Pick &amp; Mix Isolation Levels: Mixed Serialization Graph Testing&#8221;.<BR>&nbsp;&nbsp;&nbsp; [116] &nbsp;Waudby, J., Ezhilchelvan, P., Webber, J. and Mitrani, I., 2020. Preserving reciprocal consistency in distributed graph databases. In Proceedings of the 7th Workshop on Principles and Practice of Consistency for Distributed Data (pp. 1-7). [Chapter 5]<BR>Description: reciprocal consistency is an important property that must be preserved in distributed graph databases, failure to do so seriously undermines the integrity of the database itself in the long term. Reciprocal consistency can be maintained as a part of enforcing any known isolation guarantee and such an enforcement is also known to lead to reduction in performance. Therefore, in practice, distributed graph databases are often built atop basically available soft state (BASE) databases with no isolation guarantees, benefiting from</P>
<P>good performance but leaving them susceptible to corruption due to violations of reciprocal consistency. This paper designs and presents a lightweight, locking-free protocol and then evaluates the protocol&#8217;s abilities to preserve reciprocal consistency and also offer good throughput. Our evaluations establish that the protocol can offer both integrity guarantees and sound performance when the value of its parameter is chosen appropriately.<BR>[40] Ezhilchelvan, P., Mitrani, I., Waudby, J. and Webber, J., 2019, November. Design and Evaluation of an Edge Concurrency Control Protocol for Distributed Graph Databases. In European Workshop on Performance Engineering (pp. 50-64). Springer, Cham. [Chapter 5]<BR>Description: a new concurrency control protocol for distributed graph databases is described. It avoids the introduction of certain types of inconsistencies by aborting vulnerable transac- tions. An approximate model that allows the computation of performance measures, including the fraction of aborted transactions, is developed. The accuracy of the approximations is assessed by comparing them with simulations, for a variety of parameter settings.</P>
<P>Other Publications</P>
<P>The remaining four peer-reviewed publications result from collaboration with the Linked Data Benchmark Council (LDBC) Social Network Benchmark (SNB) Benchmarking Task Force of which I am a member. These are not included in the thesis, but are demonstrations of my research skills.<BR>Szarnyas, G.,Waudby, J., Steer, B., Szakallas, D., Birler, A., Wu, M., Zhang, Y., and Boncz,<BR>P. 2023. The LDBC Social Network Benchmark: Business Intelligence workload. To appear: In Proceedings of the VLDB Endow. 2023.<BR>Description: the LDBC SNB&#8217;s Business Intelligence (BI) workload is a comprehensive graph OLAP benchmark targeting analytical data systems capable of supporting graph workloads.</P>
<P>This paper presents SNB BI experiments on both a relational and a native graph database system. SNB BI advances the state-of-the art in synthetic and scalable analytical database benchmarks in many aspects.</P>
<P>[118] Waudby, J., Steer, B., Prat-Perez, A., and Szarnyas, G., I., 2020. Supporting Dynamic Graphs and Temporal Entity Deletions in the LDBC Social Network Benchmark&#8217;s Data Generator. In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA) (pp. 1-8).</P>
<P>Description: this work extends the LDBC SNB data generator by introducing lifespan attributes for the creation and deletion dates of its graph entities to allow for the generation of a temporal dynamic graph. This allows for the definition of complex deletions in LDBC SNB benchmarks which further challenges the performance of graph processing systems.</P>
<P>[117] Waudby, J., Steer, B., Karimov, K., Marton, J., Boncz, B., and Szarnyas, G., I., 2020. Towards Testing ACID Compliance in the LDBC Social Network Benchmark. In Proceedings of the 12th TPC Technology Conference on Performance Evaluation &amp; Benchmarking.</P>
<P>Description: verifying ACID compliance is an essential part of database benchmarking, because the integrity of performance results can be undermined as the performance benefits of operating with weaker safety guarantees (at the potential cost of correctness) are well known. Traditionally, benchmarks have specified a number of tests to validate ACID compliance. However, these tests have been formulated in the context of relational database systems and SQL, whereas our context is systems for graph data, many of which are non-relational. This paper presents a set of data model-agnostic ACID compliance tests for the LDBC SNB suite&#8217;s Interactive workload, a transaction processing benchmark for graph databases. We test all</P>
<P>ACID properties with a particular emphasis on isolation, covering 10 transaction anomalies in total. We present results from implementing the test suite on five database systems.</P>
<P>[77] Mhedhbi, A., Lissandrini, M., Kuiper, L., Waudby, J., and Szarnyas, G., I., 2021. LSQB: A Large-Scale Subgraph Query Benchmark. In Proceedings of the 3rd Joint In- ternational Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA) (pp. 1-11).</P>
<P>Description: this paper introduces Labelled Subgraph Query Benchmark (LSQB), a new large-scale subgraph query benchmark. LSQB tests the performance of database management systems on an important class of subgraph queries overlooked by existing benchmarks. LSQB contains a total of nine queries and leverages the LDBC social network data generator for scalability. The benchmark gained both academic and industrial interest and is used internally by 5+ different vendors.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Chapter 2 Background</P>
<P>Summary<BR>This chapter provides core background material underpinning the work conducted in this thesis; later chapters introduce additional material as and when neccessary. Section 2.1 outlines database concurrency control, specifically, the concepts of Serializability and weak isolation, along with various concurrency control strategies and how these have been applied in many-core and distributed DBMSs. In Section 2.1, we also discuss recent advances in mitigating against the overhead of 2PC. Section 2.2 provides an overview of graph processing, focusing on graph databases. It introduces the notions of Reciprocal Consistency and Edge-Order Consistency outlining how they can be violated given insufficent concurrency control in a common distributed graph database architecture. Lastly, in Section 2.3 we describe the evaluation techniques used in the thesis and the evaluation frameworks used at various points throughout the thesis to analyze protocol performance. Where appropriate we state which chapter the background material is relevant to.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Database Concurrency Control</P>
<P>Definition 1 (OLTP DBMSs) Online transaction processing (OLTP) DBMSs respond to and concurrently process a large number of relatively simple database transactions in real-time.</P>
<P>Concurrency control is an integral component in an OLTP DBMS. Even if transactions are individually correct and there are no system failures, the operations of concurrently executing transactions can interleave in a manner such that the database state is left inconsistent. Concurrency control is responsible for ensuring that the effects of concurrently executing transactions are logically isolated from each other, providing each with the illusion of running alone in the DBMS.</P>
<P>Definition 2 (Concurrency Control) A concurrency control mechanism is used in order for OLTP DBMSs to maintain data consistency when executing multiple database transactions at the same time.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.1 Serializability&nbsp;(Relevant to Chapter 3)</P>
<P>Definition 3 (Database Isolation) Database isolation is a database&#8217;s ability to allow a transaction to execute as if it is alone in the system, even though there may be a large number of concurrently running transactions. The degree of isolation is determined by the concurrency control mechanism. The strongest isolation level is Serializability.<BR>The DBMS component responsible for providing concurrency control is the scheduler, which enforces a correctness criterion called Serializability. An execution of transactions is serializable if it produces a database state equivalent to some serial execution of the same set of transactions; assuming transactions are individually correct, a serial execution trivially ensures a consistent database state [12].<BR>In practice, schedulers enforce a stronger condition called Conflict Serializability, which is sufficient to ensure an execution is serializable. An execution of transactions in a DBMS can be represented by a schedule, a time ordered sequence of their operations. For example, consider transactions T1, T2, and T3 shown in schedule s below; a write on item x by transaction Ti is denoted by wi[x], a read by ri[x], and a commit operation by ci.<BR>s = w1[x] r2[x] r2[y] w1[y] w2[z] w3[z] r3[x] c1 c3 c2</P>
<P>As shown in Figure 2.1, the schedule s can be represented by a conflict graph CG(s). Each transaction is represented by a node in the graph. A conflict exists between two transactions if they both operate on the same data item and at least one operation is a write, thus, changing the order of these operations could alter the behaviour of at least one of the transactions.</P>
<P>In a conflict graph, conflicting operations ai of Ti and b j of Tj such that ai[x] &lt; b j[x],<BR>where Ti<BR>Tj, are represented by an edge Ti &#8594; Tj in the graph; possible conflict pairs are<BR>(a, b) &#8712; [(r, w), (w, r), (w, w)]. For example, in s, T2 reads x after T1 writes to x, thus there exists an edge from T1 to T2 in Figure 2.1. An execution of transactions and its corresponding<BR>schedule is conflict serializable if a serial ordering of transactions that satisfies all conflict edges can be found. Such a serial ordering exists iff the conflict graph is acyclic. This is<BR>known as the conflict graph theorem [12]. Note, s is not conflict serializable because CG(s)<BR>in Figure 2.1 contains a cycle.</P>
<P>Theorem 1 (Conflict Graph Theorem) A schedule s is conflict serializable iff its corre- sponding conflict graph CG(s) is acyclic.</P>
<P><BR>Fig. 2.1 Conflict graph representation of s.</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.2 Weak Isolation&nbsp;(Relevant to Chapter 4)</P>
<P>Definition 4 (Weak Isolation) Weak isolation refers to the set of databases isolation levels weaker than Serializability. Here weaker means the database increases the number of allowable executions.</P>
<P>Weak isolation refers to the set of isolation guarantees that increase the number of allowable executions compared to Serializability. Permitting more executions can increase performance, at the cost of possible non-serializable behaviour. Weak isolation levels were first introduced in [51], which describes four &#8220;degrees of consistency&#8221; (Degree 0-3) that provide transac- tions with increasing levels of protection from concurrent transactions. However, these</P>
<P>definitions are only applicable to single-versioned systems using lock-based concurrency control (see Subsection 2.1.3). The ANSI/ISO SQL-92 [76] specification aimed to define an implementation-independent standard for weak isolation, supporting lock, validation, and timestamp-based concurrency control mechanisms (see Subsection 2.1.3). The isolation levels proposed in the ANSI/ISO SQL-92 specification are written down informally in terms of anomalies1 they prevent: Read Uncommitted prevents no anomalies, Read Committed disallows Dirty Reads, Repeatable Read disallows both Dirty Reads and Fuzzy Reads, while Serializable additionally disallows Phantoms.<BR>Flaws with the ANSI/ISO SQL-92 specification were identified in [11]. Their informal definitions offer multiple interpretations, some of which result in inconsistencies. Further, they do not account for known anomalies such as Lost Updates and Dirty Writes, or concur- rency control mechanisms used in multi-versioned systems. Thus, Berenson et al. in [11] proposed a new set of correct, precise, isolation definitions, but acknowledged these defini- tions were disguised redefinitions of the earlier lock-based characterization in [51]. To rectify this Adya [1] introduced a framework extending the theory of multi-versioned serialization graphs to weak isolation; the differences between isolation levels being expressed in terms of specific cycles they prohibit in the serialization graph.<BR>Using Adya&#8217;s formalism the authors of [8] analyzed ACID2 isolation levels and replicated data consistency guarantees through the lens of high availability. They proposed several new isolation levels: Monotonic Atomic View, Predicate-Cut and Item-Cut and provided a new definition for Snapshot Isolation. Building on [8] a new isolation level called Read Atomic was proposed in [9]. Recently, a new client-centric state-based formalization for isolation levels was proposed by [22].<BR>Despite the aforementioned theoretical advancements, database concurrency control research has primarily focused on improving the performance of Serializable transaction<BR>1Anomalies are colored red.<BR>2Atomicity, consistency, isolation, durability.</P>
<P>processing [85]. Yet weak isolation is more commonly used in practice than Serializable isolation [85] and Read Committed is in fact the default isolation level for the majority of databases [8]. In Chapter 4, we attempt to address this disconnect by leveraging Adya&#8217;s formalization of weak isolation to develop a high performance concurrency control protocol for many-core databases that supports transactions executing at a range of isolation levels.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.3 Concurrency Control Approaches&nbsp;(Relevant to Chapter 3) The algorithms used by schedulers vary and have developed in response to changes in hardware (technological advancements and costs reductions) and workload profiles. Broadly speaking, there are four types of concurrency control algorithms: lock-, timestamp-, graph- and validation-based, with some systems using a hybrid approach [113]. Additionally, these algorithms are classified into pessimistic, which assumes non-serializable behaviour will happen and pro-actively prevents such behaviour by delaying transactions, and opti- mistic approaches, which assume non-serializable behaviour will not happen and only fixes things when such behaviour is detected, normally by aborting and restarting the offending<BR>transaction(s).<BR>We now provide a brief summary of each concurrency control approach.</P>
<P>Lock-based Lock-based concurrency control is a pessimistic approach in which transac- tions acquire locks to access data items. There is a variety of locking schemes, the simplest being shared/read and exclusive/write locks. Transactions hold all locks they have acquired (growing phase) until they have completed their operations, at which point they release locks (shrinking phase), this approach is known as two-phase locking (2PL); if all acquired locks are released together it becomes strict two-phase locking (S2PL) [39]. Locking can delay transactions but avoids rollbacks unless deadlocks occur. In 2PL deadlocks can occur when several transactions are waiting for a resource held by one of the others and none can progress. There are several deadlock management strategies: (i) maintain a waits-for</P>
<P>graph (deadlocks manifest as cycles), (ii) use timestamps and employ either wait-die or wound-wait to determine whether transactions should abort or wait for the lock, and (iii) prevent deadlocks happening; normally achieved by making transactions access data in some fixed order.</P>
<P>Timestamp-based Timestamp-based concurrency control is known as timestamp ordering (TO) and is an optimistic approach in which transactions are assigned unique (logical or real-time) timestamps, and additional meta-data is stored with each data item. Specifically, for a data item, the DBMS stores the timestamps of the latest reading transaction, and the latest writing transaction, along with a commit flag, indicating whether the latest writing transaction is active or not. The scheduler uses this information to determine whether a transaction&#8217;s operations are physically realizable, or the transaction must be aborted and restarted. As a general rule, TO is more suited to workloads consisting mostly of read-only transactions, but will suffer when conflicts are high and will result in frequent aborts and retries. Compared with 2PL, TO does not delay transactions but can cause rollbacks, which leads to a more serious delay and wasted resources. Note, if the DBMS stores multiple versions per data item then multi-version TO, often referred to as multi-version concurrency control (MVCC) can be used, which decreases the chance a read operation will cause a transaction to abort. The scheduler now creates a new version of a data item for each valid write, assigned with the writing transactions timestamp. When scheduling a read operation it locates the version that was written immediately before the transaction started.</P>
<P>Validation-based Validation-based concurrency is also an optimistic approach (often referred to as optimistic concurrency control (OCC)) that allows transactions to proceed without acquiring locks or checking timestamps, but at the appropriate time performs some validation before aborting or committing a transaction [66]. For a transaction, OCC requires the scheduler be informed of its write and read sets prior to execution. Then, a transaction</P>
<P>is executed in three phases: (i) read, the transaction executes all reads in its read set and computes all values arising from writes in its write set into a private workspace; (ii) validate, the scheduler validates the transaction by comparing its write and read sets with other transactions. If successful the transaction proceeds to the next phase, else it is aborted; (iii) write, each write in the transaction&#8217;s private workspace is written into the database. There are two validation strategies, (i) backward-oriented, validate the read set of the validating transaction with the write set of all transactions that were not committed before the start of the validating transaction, or (ii) forward-oriented, the write set of the validating transaction needs to be disjoint with all concurrent read-phase transactions.</P>
<P>Graph-based In graph-based concurrency control, often referred to as serialization graph testing (SGT), the scheduler maintains an acyclic serialization graph over the execution it controls. When a transaction desires to execute an operation, all resulting conflicts are calculated and corresponding edges inserted into the conflict graph. The operation proceeds provided the introduction of new edges did not create a cycle, in such cases the transaction is aborted. SGT has the theoretically optimal property of accepting all and only conflict serializable executions, offering a higher degree of concurrency and minimizing aborted transactions compared to alternative approaches. Several SGT variants exist [12] in addition to basic-SGT described above. Conservative-SGT never rejects any operations, but requires transactions to predeclare write and read sets so operations can be scheduled and executed with respect to some order that preserves the acyclicity of the conflict graph. In certifier-SGT, transactions execute operations optimistically, detecting conflicts as they progress, followed by a certification at commit time. There are several issues unique to SGT that must be addressed, specifically, (i) when nodes can be safely deleted from the conflict graph, (ii) how to detect conflicts, and (iii) how to check for cycles efficiently. SGT was, in fact, considered impractical due to (iii) the costs of cycle checking.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.4 Many-Core Database Concurrency Control (Relevant to Chapters 3 and 4)</P>
<P>Definition 5 (Many-Core Databases) A many-core database is a DBMS that is running on a single machine that has several CPU cores, often up to 100 cores.</P>
<P>The strategies described in Subsection 2.1.3 do not scale well with recent hardware trends, namely, the advent of machines with many-cores, often in the magnitude of 100 cores. Due to 2PL&#8217;s poor performance in such a setting, systems employ an optimistic strategy using timestamp allocation. These protocols have three key problems: (i) OCC requires an exclusive verification phase at commit time, which as the number of cores is increased, leads to high contention during this phase, resulting in extremely low throughput,<BR>(ii) such protocols require a global counter for timestamp allocation which can also become a bottleneck, and (iii) when there is a significant amount of conflict (high contention) between transactions then optimistic protocols typically exhibit a high number of aborts. These pitfalls have led to numerous protocols each offering new optimizations with performance varying dependent on workload characteristics, e.g., data skew, read ratio, payload size, table cardinality, and transaction size. Several notable examples are: Silo [110], MOCC [113], TicToc [122], BCC [123], and SGT [34] which are now described; for the interested reader see [25, 32, 42, 53, 58, 62, 63, 69, 88, 100, 114, 120, 121] for other contemporary protocols. To combat problems with timestamp allocation Silo introduces a novel way to calculate transaction ids. The bits of transaction ids are divided in two, with higher bits representing a system-wide global epoch counter of the transaction&#8217;s commit time, and lower bits being used to distinguish transactions within the same epoch. However, the lower bits do not capture the relative order within the same epoch, as a consequence only read-write dependencies can be captured, which restricts concurrency. To avoid assigning global timestamps to transactions TicToc lazily computes timestamps from a range of parameters such as read and write sets.<BR>TicToc then checks whether these timestamps are valid.</P>
<P>Mostly optimistic concurrency control (MOCC) attempts to mitigate OCC&#8217;s poor perfor- mance under high contention by combining it with 2PL. For highly contended tuples, MOCC uses 2PL and falls back to OCC for less contended ones. This is an appealing approach but introduces the challenge of correctly detecting hot tuples, this behaviour is often transient and can vary largely over time. Balanced Concurrency Control (BCC) attempts to reduce the number of unnecessary aborts (false positives) in backward-oriented OCC. It proposes an improved validation rule to determine non-serializable transaction schedules that exploits essential dependency patterns. This vastly reduces aborts in high contention workloads, however, this approach uses the same execution model as vanilla OCC and thus suffers from the drawbacks of an exclusive verification phase.<BR>Serialization Graph Testing takes a radically different approach in tackling the aforemen- tioned problems with optimistic protocols. It dusts down the longtime discarded graph-based concurrency control strategy. SGT possesses the theoretically optimal approach to concur- rency control, but has never been subjected to exhaustive research, nor has it been deployed in any commercial DBMS. An explanation for this is the general consensus that the approach is merely impractical, due to the costs associated with cycle checking. In [34] this claim is successfully refuted. Their key contribution was a highly concurrent graph data structure used to represent the CG. Access to their graph data structure is governed by a node-level locking structure. There are two types of node locks, shared locks, and exclusive locks. Shared locks allow concurrent edge insertion and cycle checks. Exclusive locks are taken only for commit critical checks. As stated in [12] a safe commit condition is to wait until a node in the CG has no incoming edges, thus, the exclusive lock can be taken and released quickly. If incoming edges exist, commitment is delayed. It has been demonstrated that this approach scales well on modern, many-core, hardware: (i) cycle checking proceeds in parallel, removing the bottleneck of OCC&#8217;s single-threaded validation phase, (ii) nodes in the graph double up as transactions ids removing any dependence on global timestamps, and (iii)</P>
<P>it accepts all conflict serializable schedules, removing all unnecessary aborts. SGT is used as the basis for the development of the Wait-Hit Protocol in Chapter 3 and Chapter 4 utilizes the concurrent graph data structure from [34] in the development of MSGT.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.5 Distributed Database Concurrency Control&nbsp;(Relevant to Chapter 3)</P>
<P>Definition 6 (Distributed Databases) A distributed database is a DBMS that is running across multiple machines. In this thesis, a distributed database is assumed to have a shared- nothing architecture, with each machine responsible for managing a disjoint portion of the total database.</P>
<P>A distributed, shared-nothing, DBMS is comprised of a number of partitions3, each holding a disjoint portion of the complete database. In such a system, transactions can access data at multiple partitions. These transactions are known as distributed or multi-partition transactions (MPTs). Distributed transactions typically have one partition responsible for coordinating their execution, which issues remote operations to the required remote partitions. The fundamental difference between distributed and centralized transaction processing is that the logically single commit/abort operation must now take place in multiple places. The challenge here is two-fold. Firstly, it is important that each partition involved in the processing of a transaction agree on its outcome, that is, if one partition desires to abort then all must. Secondly, the nature of failures takes on a different complexion in a distributed database.</P>
<P>Definition 7 (Atomic Commitment Protocols) An atomic commitment protocol, such as two-phase commit (2PC), is responsible for ensuring a collection of servers participating in a transaction, in the presence of partial failures, agree on the outcome of said transaction.<BR>3Also referred to as shards or sites.</P>
<P>In a centralized DBMS, failure is binary, either the system is up and transactions can be processed, or it has failed and no transactions can be processed at all. In a distributed system, however, there can be partial failures, some partitions may be functional while others have failed. Thus, the challenge can be reformulated as achieving consistent termination in the presence of partial failures. This challenge is delegated to an atomic commitment protocol, such as two-phase commit (2PC), which is used by the transaction&#8217;s coordinator at commit time to bring the set of servers participating in the transaction to agreement on the transaction&#8217;s termination status (commit or abort) in a manner which is resilient to failures. 2PC consists of two phases: prepare and commit. During the prepare phase, the coordi- nator requests a vote from the set of partitions involved in the transaction (participants) on whether they wish to commit or abort the transaction. Once the coordinator has collected the responses the commit phase begins. If any participant, the coordinator included, has voted to abort the transaction, the coordinator issues an abort message to all participants. Else, the coordinator broadcasts a commit message. When participants receive the commit or abort message, they perform any necessary clean up, and send an acknowledgment back to the co- ordinator. The coordinator also performs clean up during this phase but only responds to the client with the transaction outcome after it receives acknowledgments from all participants. A simplistic view is that 2PC can be merely glued onto any concurrency control protocol.<BR>This is true for 2PL, which differs in a distributed database only in that locks are held until participants have received the commit/abort message during 2PC. However, other concurrency control protocols, such as OCC, require slightly different integration. For example, in OCC, the validation phase is now performed when a participant receives the prepare message from the coordinator. The outcome of the validation phase informs the response to the prepare message.<BR>A comprehensive performance evaluation of distributed concurrency control was per- formed in [54], comparing four of the classic protocols described in Subsection 2.1.3: 2PL,</P>
<P>TO, MVCC, OCC. The study investigated: (i) the impact of workload factors: degree of contention, update rate, and the percentage of<BR>MPTs, (ii) scalability: increasing the number of partitions/servers, (iii) the performance over wide-area networks (WAN), and (iv) various different application scenarios, namely, TPC-C [109] and the Product-Parts-Supplier (PPS) workload. Overall, the outlook on distributed concurrency control performance was bleak. This is illustrated by the MPT experiment, when transactions involved 2-4 partitions, throughput dropped by 12-40% across all protocols. The study attributed the performance inhibition of MPTs to: (i) overhead of sending remote requests, stalling and resuming execution, and (ii) 2PC. On a brighter note, the scalability experiment did demonstrate that performance gains are possible, albeit limited. Only once scaled to 64 servers did protocols display a 1.7&#8211;3.8&#215; increase in throughput over single-server throughput. However, as workload contention increased this dropped to a 0.2&#8211;1.5&#215; gain.<BR>Interestingly, whilst research in many-core concurrency control has shunned lock-based approaches for optimistic ones, across the range of experiments performed it was 2PL with NO-WAIT deadlock detection that performed best. OCC&#8217;s poor performance was attributed to the overheads of validation and copying during transaction execution, whereas both MVCC and TO block newer transactions that conflict until the older ones commit, which when data items are highly contended can increase latency, even if the overall abort rate is low.<BR>The differing optimal approaches for many-core and distributed databases is not com- patible with cloud environments in which users expect matching performance increases as resources are scaled up and out. For example, users may initially scale up their system and thus opt for an optimistic approach, before then scaling out and then being left with a sub-optimal concurrency control protocol for their new scale point. This raises the following question: how to design a concurrency control protocol that performs equally well in both many-core and distributed databases. Designing such a protocol is tackled in Chapter 3.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1.6 2PC: Research Strikes Back&nbsp;(Relevant to Chapter 6) Given the somewhat bleak picture of distributed transaction processing emphasized by the study in [54], it has become a fertile area of research and numerous approaches to improve<BR>performance have been proposed.<BR>Several systems attempt to minimize or eliminate distributed transactions. Schism uses a workload-driven partitioning scheme to minimize distributed transactions [23], while G- Store [24] and LEAP [70] eliminate them through dynamic data repartitioning. Mandating determinism also avoids 2PC [90, 103, 104]. The idea behind deterministic databases is to order and replicate transactional inputs prior to execution using a centralized sequencing layer, thus eliminating coordination between servers [103]. There are two key advantages to this approach, atomic commitment can be avoided, and it greatly simplifies replication [91]. The study in [54] included one deterministic approach, Calvin [104]. In most experiments Calvin performed the best, but the dice were unfairly weighted as it avoids 2PC and other message passing. Calvin suffers when transactions involve conditional operations, may internally abort, or involve foreign key lookups. The study found when the workload included these most protocols&#8217; throughput was only affected by 2&#8211;10%, but Calvin experienced a 36% decrease &#8211; determinism is not a silver bullet. An important caveat with deterministic databases is that transactions&#8217; read and write sets be known prior to execution. If it cannot be met, a reconnaissance phase is executed to discover these sets which amounts to running a transaction twice. Aria [71] avoids this caveat by using a deterministic reordering mechanism, but its performance suffers under high contention workloads [72]. Prognosticator [60] circumvents this limitation by using symbolic execution to build key-level transaction profiles, which are used to execute transactions with a high degree of parallelism.<BR>Another category of systems weaken isolation and relax consistency guarantees to achieve better performance, offering Snapshot Isolation [33, 36, 96] or highly available transactions (HATs) [8]. Another group of systems opt for a blended approach by combining concurrency</P>
<P>control, replication, and commitment into a unified protocol to amortize the costs of 2PC. The principal aim here is to minimize the number of WAN round trips and hence latency is reduced (see MDCC [64], TAPIR [124], Ocean Vista [43], Janus [79], Helios [81], and G-PAC [74]). Another related technique is CockroachDB [98] which uses Parallel Commits to halve the latency of distributed transactions by concurrently performing consensus round trips required to commit a transaction.<BR>Several recent OLTP databases utilize epochs to exchange improved throughput for higher latency. Obladi tackles the problem of data access privacy by combining Oblivious RAM and epochs to hide access patterns [21] from cloud providers. STAR [73] runs distributed transactions and single-node transactions in different epochs. COCO [72] leverages epochs to mitigate against the costs of 2PC and synchronous replication. In Chapter 6 we evaluate the performance of epoch-based commit and develop epoch-based multi-commit, which to our knowledge, is the first protocol to combine epochs and data access patterns to minimize aborts in the presence of node failures.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Graph Processing&nbsp;(Relevant to Chapter 5) Graphs have long been a useful mathematical abstraction and commonplace data structure across many facets of computer science. Recent years have seen a proliferation of the development and use of dedicated graph processing technologies [14]. The emergence of these technologies appears to be motivated by organizations recognizing the potential value that can be extracted from rich, highly-connected data sets, and the perceived shortcomings of existing relational database management systems (RDBMSs) which are not suitable for serving graph workloads. Thus, organizations often complement their existing data<BR>processing pipelines with graph processing systems [93].<BR>It is well established there is no single &#8220;one-size-fits-all&#8221; data management technology that can serve all possible uses of data efficiently [97] which holds true within the realm of</P>
<P>graph data management, owing largely to the variety exhibited across graph workloads. As such graph processing systems broadly fall into one of three categories:<BR>&nbsp;&nbsp;&nbsp; &#8226; Graph databases focus on transactional online graph persistence and are accessed in real time from some application. Queries are either: (i) local, involving a single vertex or edge, (ii) neighborhood, retrieve all edges attached to a given vertex, or (iii) traversals, exploring a part of the graph beyond a single neighborhood. In each case only a subset of the graph is required to satisfy a query. Examples include Neo4j [92], JanusGraph [61], TigerGraph [106], and Dgraph [29].<BR>&nbsp;&nbsp;&nbsp; &#8226; Graph Analytical Frameworks focus on offline analytics, supporting global graph algorithms such as clustering and community detection. Most computations involve the whole graph. Examples include GraphX [50], Giraph [49], and GraphChi [67].<BR>&nbsp;&nbsp;&nbsp; &#8226; Graph Streaming Engines focus on supporting analytics over a continuously changing graph. Typically these systems offer a simple graph-based data model and lack transactional support [13]. Examples include Aspen [30], Kineograph [19], and GraphOne [65].<BR>Given the expressiveness of graphs and the variety of system types, application areas are wide reaching from healthcare, to social networks and fraud detection [35]. The work in this thesis (Chapter 5) is primarily concerned with graph databases. In such systems data, the most common data model is the labeled property graph [92]. In the property graph model each vertex has a unique ID, a label indicating the type of vertex, a set of incoming and a set of outgoing edges and a collection of key-value properties. Each edge has a unique ID, its start and end vertices, a relationship type label and a collection of properties.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1 Graph Data Consistency</P>
<P>Definition 8 (Reciprocal Consistency) Reciprocal consistency is an invariant that ensures the two physical edge pointers that comprise a logical edge in a graph database point to one another.</P>
<P>Regardless of application-level semantics, the property graph data model imposes a fundamental consistency guarantee that must always remain valid: Reciprocal Consistency. In the property graph model, edges have direction and each edge runs from a source vertex to a destination vertex. In the storage layer, however, edge directionality does not exist; both the source and the destination vertices store information about each other. This allows edge traversal to be bidirectional and speeds up query performance.<BR>Consider, for example, the statement: &#8220;Tolkien wrote The Hobbit&#8221;. It is expressed using vertex a for &#8220;Tolkien&#8221; and vertex b for &#8220;The Hobbit&#8221;, and an edge wrote running from a (source) to b (destination). Corresponding openCypher [83] code 4 is given below and Figure 2.2(a) shows the model level view of edge ab.<BR>MATCH (a:Person), (b:Book)<BR>WHERE a.name = &#8216;Tolkien&#8217; AND b.title = &#8216;The Hobbit&#8217; CREATE (a)-[w:WROTE]-&gt;(b)</P>
<P>Figure 2.2(b) depicts the internal representation of a graph arising from JanusGraph [61] and TitanDB [108]. A vertex, such as a, is represented by a record that contains one or more properties of that vertex, followed by a sequence of edge pointers pointing to all those vertices to which this vertex is related either as a source or a destination. The sequence of edge pointers is also called the adjacency list. It can be seen in Figure 2.2(b) that a&#8217;s adjacency<BR>4There are many graph query languages, with each vendor often developing their own proprietary lan-<BR>guage. Many offer a declarative language, e.g., Neo4j&#8217;s Cypher [45], TigerGraph&#8217;s GSQL [106] and Oracle&#8217;s PGQL [84], with others offering the Gremlin API [107]. There have been a few concerted efforts to stan- dardized graph query languages. Efforts include G-Core [5], OpenCypher [83], and more recently, GQL and SQL/PGQ [27].</P>
<P>list has an edge pointer entry that stores &#8220;a wrote b&#8221; while b&#8217;s list has a corresponding entry storing the reciprocal (or inverse) information &#8220;b written by a&#8221;. When the adjacency list entries for a given edge refer to each other in a complementary manner like this, that edge is said to exhibit Reciprocal Consistency.<BR>Consider a query: &#8220;list all titles by the author who wrote The Hobbit&#8221;. This query needs to start from b which represents the only entity specified explicitly in it. Thanks to the reciprocal information in b, it can reach a from b, even though edge ab is &#8220;directed&#8221; from a to b, and then compile the necessary list from a. Note that Reciprocal Consistency is assumed to prevail when a query reads only the source or destination vertex of an edge.</P>
<P>name:Tolkien&nbsp;title:The Hobbit<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Logical view.</P>
<P>a:Person</P>
<P>name:Tolkien</P>
<P>&#8594; wrote b</P>
<P>edge</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>b:Book</P>
<P>property</P>
<P>title:The Hobbit<BR>&#8592; written by a</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>vertex id</P>
<P>property</P>
<P>property</P>
<P>edge</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>.&nbsp;.&nbsp;.&nbsp;.&nbsp;.</P>
<P>vertex id</P>
<P>property</P>
<P>edge</P>
<P>edge</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Storage view.<BR>Fig. 2.2 Logical and storage views of a reciprocally consistent edge ab.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.2 Distributed Graph Databases</P>
<P>Definition 9 (Distributed Graph Databases) A distributed graph database is a distributed DBMS that is designed for managing graph data spread across several machines.<BR>In practice, graphs can be extremely large, sometimes in the magnitude of 100 billion edges [93], exceeding the storage capacity of a single-node graph database and motivating the need for distributed graph databases. A distributed graph database employs a shared-nothing</P>
<P>architecture, partitioning a graph among loosely cooperating servers. Graph partitioning is non-trivial and a common approach is to use a k-balanced edge cut [56]. The objective of such an approach is to minimize the proportion of edges that span partitions and also to balance the distribution of vertices to partitions.&nbsp; Figure 2.3 depicts a graph database partitioned<BR>across three servers, Si, i = 1, 2 and 3. Typically, each partition would be replicated for fault<BR>tolerance and availability. Intra-partition and inter-partition edges are respectively referred to as local edges and distributed edges (shown using dashed lines in Figure 2.3). The proportion of distributed edges is not negligible and can range from 25-75% [56].<BR>S2<BR>Fig. 2.3 Local and distributed edges</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.3 Data Corruption in Distributed Graph Databases</P>
<P>Recent work [41, 119] highlighted that a common distributed graph database architecture permits violations of Reciprocal Consistency in distributed edges, introducing corruption into the database. Such an architecture uses a NoSQL database [89], e.g., Apache Cassandra [6], for storage and is then adapted with a query language expressed in terms of edges and vertices along with some glue-code to bind that interface to the underlying database. Opting for this design appears to be a good choice: it offers the application programmer the modeling convenience of graphs together with the operational characteristics of the underlying highly scalable NoSQL database. However, a major problem with this design option is, in a desire to offer higher performance, a lack of transactional semantics from the underlying database</P>
<P>which seldom provide guarantees for multi-operation, multi-object transactions that span partitions. Without concurrency control across partitions, concurrent updates of distributed edges can interleave and violate Reciprocal Consistency.5 Moreover, due to the scale-free property exhibited by many real world graphs, this corruption can propagate through the database at alarming rates, rendering it irreversibly corrupt. We now describe in detail the mechanism by which this corruption can occur.<BR>Suppose that the edge (a)-[:WROTE]-&gt;(b) is a distributed edge, with vertices a and b</P>
<P>in servers Si and S j, j&nbsp;i, respectively. When a transaction writes this edge ab: (i) two writes</P>
<P>are performed: reciprocal entries in the adjacency lists of nodes a and b are updated, and (ii) write order is unconstrained: a transaction is equally likely to write a then b as it is to write b then a. Concurrent transactions Tx and Ty can interleave in the following three ways and each one is depicted in Figure 2.4:<BR>&nbsp;&nbsp;&nbsp; (a) Tx starts before Ty; it writes a at Si first and then proceeds to S j across the network; Ty operates the other way round, beginning with S j and proceeding to Si (see Figure 2.4(a)). The net effect is: Tx &#8594; Ty at Si and at Ty &#8594; Tx at S j, where T &#8594; T &#8242; at S denotes that T precedes T &#8242; at server S.</P>
<P>&nbsp;&nbsp;&nbsp; (b) Same as the previous case, except that Ty starts earlier than Tx (see Figure 2.4(b)).</P>
<P>&nbsp;&nbsp;&nbsp; (c) Same net effect as in previous two cases, except that both Tx and Ty start their first writes at Si, Tx &#8594; Ty, but Ty overtakes Tx in reaching S j where Ty &#8594; Tx (see Figure 2.4(c)).<BR>We could envisage three more corresponding cases (a&#8217;) - (c&#8217;) where the roles of Tx and Ty in cases (a) - (c) are simply interchanged; e.g., in case (a&#8217;), Ty starts before Tx, writes a at Si first and then proceeds to S j across the network; Tx operates the other way round, beginning with S j and proceeding to Si. Thus, there are only six ways concurrent Tx and Ty can interleave. The arguments we make based on cases (a) - (c) of Figure 2.4 equally apply,<BR>5The concurrency control primitives provided by NoSQL databases are typically sufficient to ensure<BR>Reciprocal Consistency for local edges.</P>
<P>Si&nbsp;S j<BR>Tx<BR>Ty<BR>Si&nbsp;S j</P>
<P>Tx<BR>Si&nbsp;S j<BR>Tx Ty</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>(a)</P>
<P>(b)</P>
<P>(c)</P>
<P>Fig. 2.4 Interleavings of concurrent writes to a distributed edge by transactions Tx and Ty.<BR>by symmetry, to cases (a&#8217;) - (c&#8217;) and so, for brevity, we will not consider the latter. At the end of each case in Figure 2.4, the last update on a is by Ty and that on b is by Tx. Unless updates of Tx and Ty are commutative, ab cannot be reciprocally consistent. Note that the case of Figure 2.4(c) can be avoided easily by using sequence numbers and by exploiting the fact that both Tx and Ty modify the edge starting from the same end Si. Each end of a distributed edge maintains start sequence number (ssn) to indicate the number of transactions that modified the edge starting from its end. It also maintains a finish sequence number ( f sn) to indicate the number of transactions that modified the edge starting from the other end. A transaction (such as Ty) should not modify the edge at the remote end (at S j) until its ssn is equal to one less than the f sn found at the remote server. After modification(s), the transaction sets f sn = ssn. Thus, only cases (a) and (b) of Figure 2.4 are of concern. As an example, suppose that Tx deletes the wrote edge while Ty concurrently appends a property year:</P>
<P>// Tx<BR>MATCH (a:Person)-[w:WROTE]-&gt;(b:Book)<BR>WHERE a.name = &#8216;Tolkien&#8217; AND b.title = &#8216;The Hobbit&#8217; DELETE w</P>
<P>// Ty<BR>MATCH (a:Person)-[w:WROTE]-&gt;(b:Book)</P>
<P>WHERE a.name = &#8216;Tolkien&#8217; AND b.title = &#8216;The Hobbit&#8217; SET w.year = 1937</P>
<P><BR>:WROTE<BR>year:1937</P>
<P>name:Tolkien&nbsp;title:The Hobbit<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Logical view.</P>
<P>a:Person</P>
<P>name:Tolkien</P>
<P>&#8594; wrote b</P>
<P>edge</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>b:Book</P>
<P>year:1937</P>
<P>title:The Hobbit</P>
<P>&nbsp;</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>vertex id</P>
<P>property</P>
<P>property</P>
<P>edge</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>.&nbsp;.&nbsp;.&nbsp;.&nbsp;.</P>
<P>vertex id</P>
<P>property</P>
<P>edge</P>
<P>edge</P>
<P>edge</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Storage view.<BR>Fig. 2.5 Logical and storage views of a reciprocally inconsistent edge ab.</P>
<P>The interleaving patterns depicted in Figure 2.4 leave ab in violation of Reciprocal Consistency, as shown in Figure 2.5. When Tx and Ty do not interleave, either Ty &#8594; Tx or Tx &#8594; Ty holds at both servers Si and S j; in that case, the last update on both a and b would be by either Tx or Ty, respectively. That is, when transactions update at both ends of ab in some arbitrarily chosen but identical order, ab is left reciprocally consistent after each transaction&#8217;s update. When interleaving updates leave ab reciprocally inconsistent, ab can be said to have become half-corrupted because if Ty &#8594; Tx is the chosen order between Ty and Tx, then the edge pointer in b of ab is in error; otherwise, the edge pointer in a is erroneous. Thus, a reciprocally inconsistent edge ab certainly has a corrupt half but the question of exactly which half is corrupt is decided by what happens subsequently. Suppose that a future transaction Tw first reads the edge pointer of reciprocally inconsistent ab, say, at vertex a. (Note that when Tw reads an edge, it does not check for reciprocal consistency). At that moment, Tw (implicitly) chooses the order Tx &#8594; Ty and thereby invalidates the other order</P>
<P>Ty &#8594; Tx that prevails at vertex b. Thus, from that moment onward, the b end of edge ab<BR>becomes the corrupt end and, conversely, the a end becomes the correct end.</P>
<P>// Tz<BR>MATCH (b:Book),(u:Person)<BR>WHERE NOT (:Person)-[:WROTE]-&gt;(b:Book)<BR>AND u.name = &#8216;unknown&#8217; CREATE (u)-[:WROTE]-&gt;(b)<BR>If no transaction ever reads the edge pointer at vertex b, then the order Tx &#8594; Ty effectively prevails and the half-corruption of ab remains invisible to the rest of the database. However, if Tz is to subsequently read the edge pointer at b and write another edge based on what it read, i.e., The Hobbit has unknown author, then it is introducing updates not consistent with what Tw read earlier; it thus introduces semantic corruption into the database. Further writes based on reading semantically corrupt data also spread corruption. A database is said to be operationally corrupt when a significant proportion of its data records are in a semantically corrupt state [41].6<BR>To illustrate the severity of this problem, the authors of [41, 119] constructed an analytical model to analyze the time until operational corruption. They found for fair assumptions regards database size, proportion of distributed edges, and network latencies, even with modest transaction arrival rates the database was often found to be operationally corrupt in under 50 hours!</P>
<P>Definition 10 (Edge-Order Consistency) Edge-order consistency is the guarantee that up- dates by transactions to multiple edges happen in the same order across all edges.<BR>6Two relevant remarks on past works: a half-corrupted edge is due to a dirty write (ANSI P0 [11], Adya G0<BR>[2]) in the context of distributed graph databases. If the database provides the ANSI isolation level Read Uncommitted across all objects, it will identically order the writes of concurrent transactions and this would prevent all interleaving patterns shown in Figure 2.4 and thus avert half-corruption altogether.</P>
<P>Following on from Reciprocal Consistency, we also observe a different type of possible<BR>conflict that can arises when transactions update more than one edge during their lifetime. Suppose that transactions TA and TB both update edges e and e&#8242;, and do without interference either among themselves or with other transactions. It may happen that e is updated by TA before TB, while e&#8242; is updated by B before A, as illustrated in Figure 2.6 (here time flows from left to right and the conflict-free reciprocally consistent updates are collapsed to single instants). Such an occurrence, if allowed, would violate the property of Edge-Order Consistency between transactions.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>t1&nbsp;t2&nbsp;t3&nbsp;t4&nbsp;time Fig. 2.6 Edge-Order Consistency violation</P>
<P><BR>These issues will be taken up in Chapter 5 where we develop lightweight concurrency control protocols for maintaining Reciprocal Consistency and Edge-Order Consistency, thus avoiding data corruption.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Performance Evaluation Techniques</P>
<P>The performance of the protocols presented in this thesis are evaluated and benchmarked using a range of techniques: analytic models, simulations, and direct implementations allowing for experimentation with real hardware.<BR>Analytic models allow for the representation of a protocol and its interactions within a system as mathematical expressions. Once derived, such models have a number of ad- vantages [75]. They can be used to quickly garner insights into protocol behaviour under</P>
<P>a variety of different conditions allowing for a quicker feedback loop compared to direct implementations as the critical factors that influence protocol can be easier distilled. Ad- ditionally, the process itself of deriving analytic models can be helpful in improving the theoretical understanding of protocols. Lastly, analytic models act as a useful litmus test that can inform the decision of moving to a more labour-intensive implementation within a real system. Analytic models are not without their limitations. They are dependent on their simplifying assumptions, which make them tractable, but can result in a mismatch with reality, as George Box said, "All models are wrong, some are useful". Therefore, validation against real-world data is imperative for ensuring the applicability of analytic models in accurately predicting protocol performance.<BR>Simulations are step up from analytic model in terms of how closely they approximate reality, attempting to mimic an often complex protocol through a virtual model [78]. Sim- ulations share many advantages and limitations with analytic models. They allow for the systematic exploration of a wide range of scenarios through the varying of simulation param- eters. Some of these scenarios may be difficult to reproduce in reality, either due to time or financial constraints, hence simulations are often much more cost-effective than direct imple- mentations. However, simulations are also sensitive to the validity of the assumptions made within them. This can have a large impact on the accuracy of the simulation&#8217;s predictions. Again, as with analytic model it must be emphasized that validation with real-world data is vital for extracting reliable insights from simulations and caution must always be exercised when interpreting simulated results.<BR>Direct implementations are the closest approximation to the reality protocols will ex- perience in practice, but they are by far the most costly in terms of implementation effort and hardware resources. Being an accurate representation of the complexity faced in the real world means that direct implementation can undercover issues not considered during the modeling phases, which can in turn facilitate the development of improved models.</P>
<P>Therefore, direct implementation is important for the validation of analytic models and simulations. With direct implementations there is the challenge of selecting or developing an appropriate benchmark [59]. In Subsection 2.3.1, the evaluation framework used for direct implementations is presented along with the benchmarks used and why they were selected.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.1 Evaluation Framework&nbsp;(Relevant to Chapters 3 and 4) At various points in this thesis protocols are implemented in our evaluation framework7. The framework contains a prototype in-memory many-core database, which has a single versioned storage layer and a modular transaction scheduler, and is extendible for multiple workloads; requiring the implementation of a parameter generator, loader, and stored proce- dures. Within the framework each core acts as an independent client generating transactions,<BR>thus the protocols experience a truly concurrent workload.<BR>Unless stated otherwise, experiments were performed using an Azure Standard D48v3 instance with 48 virtualized CPU cores and 192GB of memory. Prior to each experiment, tables are loaded, followed by a warm-up period, before a measurement period; both are of configurable length, we use 60 seconds and 5 minutes respectively in this thesis. We measure the following metrics:<BR>&nbsp;&nbsp;&nbsp; &#8226; Throughput: number of transactions committed per second.</P>
<P>&nbsp;&nbsp;&nbsp; &#8226; Abort rate: rate at which transactions are being aborted.</P>
<P>&nbsp;&nbsp;&nbsp; &#8226; Average latency: the latency time of committed transactions (in ms) averaged across the measurement period.<BR>We now describe the workloads implemented in the evaluation framework: YCSB, SmallBank, and TATP. There are several OLTP workloads that could have been selected for implementation; OLTP-bench [31] lists 15. The workloads described below were chosen as<BR>7https://github.com/jackwaudby/spaghetti</P>
<P>they offer sufficient coverage to empirically exercise protocols under a breath of scenarios. The transactions in the TATP workload generate few conflict cycles, so there should be few aborts. Whereas, SmallBank was specifically designed to generate non-serializable executions, hence there should be a higher abort rate. It would be expected that optimistic protocols would perform better on TATP, and pessimistic protocols on SmallBank. Lastly, YCSB offers a high degree of configuration, which allows us to explore performance when other workload dimensions change, e.g., proportion of update transactions. A notable exception from the evaluation framework is TPC-C [109], which was excluded a some of its transactions require functionality, such as scans, that are not currently supported in the framework. The workloads are summarized in Table 2.1<BR>Workload<BR>Tables<BR>Columns<BR>Transactions<BR>Read-Only Transactions<BR>TATP<BR>4<BR>51<BR>7<BR>40%<BR>YCSB<BR>1<BR>11<BR>6<BR>50%<BR>SmallBank<BR>3<BR>6<BR>6<BR>15%<BR>Table 2.1 Profiles for the evaluation framework workloads.</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.2 YCSB<BR>The YCSB [20] was originally designed to evaluate large-scale Internet applications, it is re-purposed in this thesis as an OLTP microbenchmark. It has one table with a primary key and 10 additional columns each with 100B of random characters. For all our experiments in this thesis, we use a YCSB table of 100K rows. There are two types of transaction: read or update, each contains 10 independent operations accessing 10 distinct items. Update transactions consist of 5 reads and 5 writes that occur in random order. Read transactions consist solely of read operations. The proportion of update transactions is controlled by the parameter, U . Data contention, when multiple transactions try to read or write the same database items, follows a Zipfian distribution, where the frequency of access to sets of hot records is tuned using a skew parameter, &#952; . When &#952; = 0, data is accessed with uniform</P>
<P>frequency, and when &#952; = 0.9 it is extremely skewed (high contention). For Chapter 4, in<BR>order to measure the impact of transactions running at weaker isolation we introduce an additional parameter, &#969;, which controls the proportion of transactions running at Serializable isolation. The remaining transactions are split between Read Committed (90%) and Read Uncommitted (10%).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.3 SmallBank</P>
<P>SmallBank mimics a basic banking application and comprises of six transactions that per- form simple operations on customers&#8217; accounts. It was designed to generate non-serializable schedules when executed at weak isolation levels (in contrast to TPC-C) [3]. The workload configuration used in this thesis is derived from OLTP-bench [31], a standardized bench- marking tool, thus 25% of all operations are executed on a hotspot area of 100 accounts. The contention levels are varied by adjusting the number of accounts: high contention (100 accounts), mid contention (1000 accounts), and low contention (10000 accounts).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.4 TATP</P>
<P>The Telecommunication Application Transaction Processing (TATP) benchmark models a telecommunications application [102]. It consists of three read-only transactions and two update transactions. As regards contention, the primary key distribution uses a non- uniform row access pattern to increases tuple contention. The interesting property of this workload is that TATP transactions can generate few conflict cycles. In fact, only if two UpdateSubscriberData transactions interleave by accessing the same keys can a cycle be created in the conflict graph. As a consequence, schedulers that accept all valid executions should seldom abort.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Chapter 3</P>
<P>Wait-Hit Protocol</P>
<P>Summary<BR>This chapter presents the design of the Wait-Hit Protocol, a general purpose concurrency control protocol for any scale. It can effectively scale vertically, as the core count is increased on a given machine and horizontally, as the number of machines is increased in a cluster, without having to relax the gold standard isolation level (Serializability). The protocol takes an optimistic approach, letting transactions collect dependencies efficiently as they execute, and before performing a commit time validation. Based on the type of dependencies collected (write-write, write-read, read-write) a validating transaction will: (i) abort (hit) in-flight transactions that if allowed to commit could result in non-serializable behaviour, and/or (ii) delay itself (wait) for an in-flight transaction to complete so that dirty reads are avoided.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Introduction</P>
<P>Cloud providers enable seamless scaling from a few-core machine, to a many-core machine, to deployments spread across data centers and the globe. Users deploying databases in the cloud expect OLTP DBMS performance to scale with hardware resources. Unfortunately, as seen in Subsections 2.1.4 and 2.1.5, many concurrency control protocols do not scale well [54]; a protocol architected for one scale point inevitably needs to be re-adapted for another, or a different strategy altogether used. This raises the challenge of designing high-performance concurrency control protocols that can effectively scale from few-core, to many-core, to the globe whilst offering good performance at each scale point, without having to relax from Serializable isolation. This chapter describes the design and evaluation of the Wait-Hit Protocol which has been developed specifically for this task. To be precise, the Wait-Hit Protocol is a family of optimistic concurrency control protocols each targeting a different deployment.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Design Goals&nbsp;45</P>
<P>The rest of this chapter is structured as follows. We begin in Section 3.2 by discussing the design goals of the Wait-Hit Protocol. Section 3.3 presents the principles behind the wait-hit approach. Section 3.4 gives a description of the Basic Wait-Hit Protocol, implementation details, and an evaluation demonstrating the need for the many-core adjustments. Section 3.5 then presents the Many-Core Wait-Hit Protocol, discusses implementation details and op- timizations, before evaluating the protocol&#8217;s performance. Next, Section 3.6 describes the Distributed Wait-Hit Protocol. Finally, Section 3.7 draws conclusions and discusses future work.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Design Goals</P>
<P>We focus on two database architectures, many-core and distributed. Recall from Subsec- tion 2.1.4, optimistic approaches typically perform better than pessimistic approaches in many-core databases. Moreover, high performance can be achieved by: (i) avoiding global timestamp allocation, (ii) avoiding an exclusive validation phase, and (iii) minimizing unnec- essary aborts. As described in Subsection 2.1.5, for distributed databases the converse is true: pessimistic approaches generally perform better than optimistic approaches. For optimistic approaches performance is constrained by the validation phase. For example, in OCC more than 50% of time can be spent in the validation phase, or in a queue waiting to enter the validation phase [54]. An optimistic approach that reduces validation overheads could offer performance comparable to lock-based approaches in distributed databases. Note, it must be stated that 2PC is the dominating performance factor in a distributed database, but the chosen concurrency control protocol still influences performance [54].<BR>A concurrency control protocol that scales well with cores and database nodes should satisfy all the above criteria. Of recently proposed many-core concurrency control protocols, SGT achieves all many-core design goals. However, unfortunately SGT once again appears impractical in a distributed database. Consider the distributed SGT sketch in [34], each</P>
<P>partition stores a local graph representing conflicts between its local transactions, with a special node type indicating a remote partition; this must store information needed to correctly resolve all edges in the global conflict graph. The commit procedure remains unchanged in that transactions delay until they have no incoming edges, but now they must execute 2PC after this. The problem is that SGT effectively validates each operation before execution via a cycle check, which in a distributed database incurs significant remote access overhead. Cycle checking would require traversing a graph distributed across several machines, incurring network hops each in the magnitude 1-10ms. Additionally, due to non-locality of access during traversals, servers not directly involved in the processing of the validating transaction may be contacted. These two factors conspire to increase the time a transaction spends in the database, in turn increasing contention. Contention implies more conflicts and hence increases size of the conflict graph to be traversed, thus more time spent cycle checking and large transaction lifetimes &#8211; a vicious circle.<BR>SGT&#8217;s validation strategy minimizes unnecessary aborts, but maintaining this property appears too costly in a distributed environment. The wait-hit approach borrows aspects from SGT, but crucially allows some unnecessary aborts in order to gain a highly parallelizable lightweight validation strategy which performs well in both a many-core and distributed database.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Wait-Hit Approach</P>
<P>The wait-hit approach aims to get transactions in and out of the database as fast as possible through optimistic execution and a low-overhead validation phase, minimizing the window in which transactions can contend. Similarly to SGT, it is assumed transactions can cheaply detect conflicts via meta-data existing on data items. An important point is that the wait-hit approach detects all direct and transitive conflicts collectively referred to as a transaction&#8217;s predecessors. To illustrate this consider Ti reads x, Tj then writes x, then Tk writes x. Tk<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Wait-Hit Approach&nbsp;47</P>
<P>directly conflicts with Tj and transitively conflicts with Ti, thus Tk&#8217;s predecessors are Tj and Ti. For example, in Table 3.1, for the data item "Jack", T2 would detect T1 and T0 as predecessors.<BR>During execution, rather than converting conflicts into edges, inserting them into a conflict graph, and performing a cycle check as in SGT, predecessors are grouped based on whether the conflict was detected by a write operation (ww and rw conflicts), referred to as predecessors-upon-write, or by a read operation (wr conflicts), referred to as predecessors- upon-read.<BR>At commit time the compressed representation of a transaction&#8217;s conflict information is used to perform validation. A transaction&#8217;s predecessors reflect its local view of the expected serialization order. However, local serialization orders across transactions may differ, i.e., there exists a cycle in the (global) conflict graph. Thus, validation must ensure that, using per-transaction predecessor-upon-write and predecessor-upon-read information, the acyclic invariant is preserved. The wait-hit approach achieves this through a combination of forcing a transaction&#8217;s predecessors to abort, making transactions check if they themselves have been forced to abort, and by waiting for transactions&#8217; predecessors to terminate.<BR>To implement these rules, in addition to transactions tracking their predecessors, the wait-hit approach relies on a hit list containing the predecessors of previously committed transactions. If these predecessors are permitted to commit it could result in non-serializable behaviour (details to follow). When validating, transactions check if they exist in the hit list, aborting if so, this rule is referred to as the hit rule and is a key aspect of the wait- hit approach. Additionally, the wait-hit approach requires a terminated list containing the outcome (commit/abort) of terminated transactions. Note, all transactions enter their outcome in the terminated list. The validation is divided into two stages: hit phase and wait phase.<BR>The first step in the hit phase for transaction T is checking whether another transaction has forced it to abort by entering it in the hit list. After this, T considers each of its</P>
<P>predecessors-upon-write and attempts to forcibly abort (hit) them. There are four cases to consider for each predecessor P in the set of predecessors-upon-write for T : (i) P has committed, but did not have T as a predecessor, (ii) P has committed and detected T as one of its predecessors-upon-write, (iii) P has aborted, and (iv) P is still executing, i.e., it is in-flight.<BR>In case (i), P cannot be involved in a cycle with T , as if it had directly or transitively conflicted with T , then T would have been present among P&#8217;s predecessors-upon-write and hence entered T into the hit list, thus P can be ignored. In case (ii), T must abort as P and T are involved in a cycle. By traversing conflicts, T is reachable from P and P from T , thus there exists a cycle in the conflict graph. Note by the hit rule, P will have entered T into the hit list, thus the hit check ensures T aborts. For case (iii), an aborted P cannot introduce a cycle with T so can be ignored. Lastly, for (iv) if T completes validation it will then enter P into the hit-list, essentially &#8220;hitting&#8221; them ensuring that P aborts when it validates by the same logic as case (ii). This avoids non-serializable behaviour occurring, but is pessimistic as it is possible that P does not have T as a predecessor and unnecessarily aborts.<BR>The second step considers predecessors-upon-read. As in SGT, the wait-hit approach allows transactions to optimistically read dirty records. Thus, for each predecessor P in the set of predecessors-upon-read for T there are three possibilities to consider: (i) P has aborted,<BR>(ii) P has committed, or (iii) P is in-flight.<BR>For case (i), T must also abort. This is required to ensure T has not acted upon aborted data. For case (ii), T has read from a committed transaction which is permissible and no further action is needed. As regards case (iii), the fate of P is unknown, it may commit or abort. If T is permitted to wait until P has terminated, then correct action could be taken (case (i) or (ii)), unfortunately this could lead to a deadlock. Consider if T had P as a predecessor-upon-read, and vice-a-versa, then both would indefinitely wait for the other to terminate! Note that if T and/or P are read-only transactions, then there is no possibility</P>
<P>of a deadlock &#8211; a feature we will exploit later in Subsection 3.5.2. For now, deadlocks are resolved by employing a zero-wait strategy, i.e., in the event of case (iii), T would simply abort itself. This is, of course, a heavy-handed, potentially abort-heavy strategy that may result in unnecessary aborts. However, given that the wait-hit approach essentially compresses conflict information it is necessary, unlike SGT which, by storing a conflict graph, can execute a cycle check to arrive at the correct decision. We present a smarter wait strategy in Subsection 3.5.2.<BR>To summarize, the wait-hit approach validates a transaction by: (i) aborting predecessors- upon-write, that is, hitting them and checking if itself has been hit, and (ii) delaying until its predecessors-upon-read completes, that is waiting. This is the core essence of the wait-hit approach which simplifies validation greatly. We position this approach as a middle ground between a graph-based approach, e.g., SGT [34], and a single-threaded execution model, e.g., H-Store [97].</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Basic Wait-Hit Protocol</P>
<P>In this section, we describe the Basic Wait-Hit Protocol which implements the wait-hit approach in Section 3.3 using exclusive data structures to represent: (i) hit list, (ii) termi- nated list; an entry with -1 denotes the corresponding transaction aborted and 1 denotes it committed, and (iii) transaction id generator. To ensure exclusive access to data structures the hit and terminated lists are wrapped with a Mutex lock and the transaction id generator uses an atomic counter. Each transaction stores two per-transaction data structures to track a transaction&#8217;s predecessors-upon-read and predecessors-upon-write. In discussions here we assume the database has a set of threads called workers that execute transactions.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.1 Protocol Description</P>
<P>Before execution of transactions the database initializes the hit list, terminated list, and transaction id generator. When a worker receives a transaction T it gets an id from the id generator and initializes the predecessors-upon-write and predecessors-upon-read containers for that transaction.<BR>The database optimistically executes T collecting predecessors encountered using the per-item access history (see Table 3.1) and records them as either predecessors-upon-read or predecessors-upon-write depending on the operation performed (see Algorithm 1). When T performs a read operation it reads the latest version of a given item and includes the ids of all transactions that previously wrote to that item amongst its predecessors-upon-read. Note, the predecessor may be still in-flight (at the time of reading). When T performs a write operation on an item it stores the id of all transactions that wrote and read the item before among its predecessors-upon-write. After executing an operation a transaction appends its id and operation type to the access history for the item it accessed.<BR>At commit time, T enters the validation procedure given in Algorithm 2 consisting of two phases, a wait phase and a hit phase. First, T enters the hit phase, acquires the exclusive lock and checks whether it exists in the hit list (Lines 1-2, Algorithm 2). If it does, then another already terminated transaction has directly or transitively conflicted with it and there is a possibility of non-serializable behaviour, so validation fails, the lock is released, and the abort procedure given in Algorithm 3 is executed: T removes itself from the hit list, then acquires the terminated list lock and appends itself with -1. Else, it continues to the next step: T adds its predecessors-upon-write into the hit list after filtering out those which have already terminated for garbage collection reasons explained in Subsection 3.4.2 (Lines 4-7, Algorithm 2). The wait phase (Lines 10-15, Algorithm 2) deals with predecessors detected upon a read. For each predecessor, if it has committed no further action is needed, else (it is</P>
<P>in-flight or aborted) T is aborted. Upon completion of the hit and wait phases, T acquires the lock on the terminated list and appends itself with 1 (Line 18, Algorithm 2).<BR>Algorithm 1: Conflict Pair Detection<BR>Data: Transaction thisTx, Operation op, List accessHistory<BR>Result: Boolean opSuccess<BR>1 foreach elem&nbsp;accessHistory do<BR>2&nbsp;if op.type == read then<BR>3&nbsp;if elem.type == write then<BR>4&nbsp;thisTx.readPredecessors(elem.tid)<BR>5&nbsp;if op.type == write then<BR>6&nbsp;thisTx.writePredecessors(elem.tid)<BR>7 return true</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.2 Implementation Details</P>
<P>We implemented the Basic Wait-Hit Protocol in our prototype in-memory database introduced in Subsection 2.3.1. We explain here how conflicts are detected and how the size of the hit and terminated lists are managed.</P>
<P>Conflict Detection In order to efficiently detect conflicts each row in the database stores with it a sequential history of accesses. Each access stores the associated type, read or write, and the associated transaction id. An example of the required meta-data is given in Table 3.1. This information is sufficient for transactions to correctly identify conflicts as they execute, as from it all types of conflict: wr, rw, and ww can be deduced. The corresponding conflict graph is given in Figure 3.1.<BR>Fig. 3.1 Conflict graph representation of the access histories in Table 3.1.</P>
<P>&nbsp;</P>
<P>Algorithm 2: Basic Wait-Hit Protocol commit procedure.<BR>Data: Transaction thisTx, List terminated, List hit<BR>Result: Boolean successfullyCommitted<BR>/* hit phase&nbsp;*/<BR>1 hit.lock()<BR>2 if thisTx&nbsp;hit then<BR>3&nbsp;hit.unlock()<BR>4&nbsp;return !abort()&nbsp;// hit by another txn<BR>5 else<BR>6&nbsp;foreach pred&nbsp;thisTx.writePredecessors do<BR>7&nbsp;if pred / terminated then<BR>8&nbsp;hit.add(pred)&nbsp;// hit predecessors<BR>9&nbsp;hit.unlock()<BR>/* wait phase&nbsp;*/<BR>10 while thisTx.readPredecessors = 0/ do<BR>11&nbsp;foreach pred&nbsp;thisTx.readPredecessors do<BR>12&nbsp;if pred&nbsp;terminated then<BR>13&nbsp;if terminated[pred].state == 1 then<BR>14&nbsp;thisTx.readPredecessors.remove(pred) // read from committed txn<BR>15&nbsp;else<BR>16&nbsp;return !abort()&nbsp;// read from aborted txn<BR>17&nbsp;else<BR>18&nbsp;return !abort()&nbsp;// zero-wait policy<BR>19 terminated.lock()<BR>20&nbsp; terminated.add(thisTx,1)<BR>21 terminated.unlock()<BR>22 return true</P>
<P>Epoch-based Garbage Collection The protocol presented in Subsection 3.4.1 requires two data structures that without space management mechanisms would grow monotonically over time. Firstly, two elements of the algorithm in Subsection 3.4.1 ensure the space requirements of the hit list are bounded. When a transaction is adding its predecessors into the hit list (after successfully validating) it uses the terminated list (TL) to merge only its predecessors that are in-flight. The merged transactions are now effectively doomed to eventually abort and when a transaction aborts, it removes itself from the hit list. Thus, the size of the hit list is bounded by the maximum number of in-flight transactions in the database. However, the same is not true of the terminated list which requires a different garbage collection mechanism.</P>
<P>&nbsp;</P>
<P>Algorithm 3: Basic Wait-Hit Protocol abort procedure.<BR>Data: Transaction thisTx, List terminated, List hit<BR>Result: Boolean successfullyAborted<BR>/* remove from hit list&nbsp;*/<BR>1 hit.lock()<BR>2 if thisTx&nbsp;hit then<BR>3&nbsp;remove thisTx from hit<BR>4 hit.unlock()<BR>/* add to terminated list&nbsp;*/<BR>5 terminated.lock()<BR>6 terminated.add(thisTx, 1)<BR>7 terminated.unlock()<BR>8 return true</P>
<P>Name<BR>Age<BR>Access History<BR>Jack<BR>27<BR>{0, w} &#8594; {1, w} &#8594; {2, r}<BR>Stuart<BR>61<BR>{1, w} &#8594; {4, r}<BR>Holly<BR>21<BR>{0, w} &#8594; {3, r} &#8594; {4, r}<BR>Heather<BR>56<BR>{2, w}<BR>Table 3.1 Example table with access history {transaction id, operation type}.</P>
<P>A transaction Ti can safely be removed from the terminated list when no in-flight transac- tion can identify Ti as a predecessor. This is true when Ti has terminated and has removed history of its access from records involved in Ti. This is ensured using an epoch-based garbage collector. Time is divided into epochs, e = 1, 2, ..., and a global counter E displays the current epoch. Associated with each epoch is a counter denoting the number of in-flight transactions in that epoch, A(e), that is, transactions that started in that epoch but have not yet terminated.<BR>When a transaction Ti begins it records the current value of E as its start epoch ei,s and increments A(ei,s) by 1. When a transaction Ti terminates it records the current value of E as its finish epoch ei, f (ei,s &#8804; ei, f ) and decrements A(ei,s) by 1. Thus, the lifetime of a transaction can be expressed as an epoch tuple (ei,s, ei, f ). Intuitively, once all transactions started during Ti&#8217;s lifetime (ei,s, &#8804; e &#8804; ei, f ) have terminated Ti will never be identified as a predecessor and can be removed from the terminated list.</P>
<P>Let &#945; be the smallest epoch number with at least 1 in-flight transaction, hence, all<BR>transactions that started with es &lt; &#945; have terminated: (A(&#945;) &gt; 0) &#8743; (&#8704;e &lt; &#945; : A(e) = 0). Note, as all transactions eventually terminate (commit or abort) A(e) will eventually become 0 for all epochs. As, Ti &#8712; TL will have no in-flight transaction referring it as a predecessor if ei, f &lt; &#945;. Then, all T &#8712; TL with e f &lt; &#945; can be removed from TL. In practice, periodically, the garbage collector increments the epoch, computes &#945;, and removes all transactions with e f &lt; &#945; from the terminated list.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.3 Evaluation</P>
<P>To measure the protocol&#8217;s scalability the SmallBank workload with high contention described in Subsection 2.3.3 was used. Figure 3.2 displays throughput, abort rate, and average latency. It is evident from Figure 3.2 that the Basic Wait-Hit Protocol does not scale as the core count increases. In Figure 3.2(a) the throughput drops from over 1.25M transactions/s with a single-core to below 100K with 40 cores. This is reflected in the abort rate in Figure 3.2(b), across 1-40 cores it increases linearly, peaking at over 40% when the database has 40 cores. The increase in average latency in Figure 3.2(c) across the range is also near-linear.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4.4 Discussion</P>
<P>It is evident from Subsection 3.4.3 that the Basic Wait-Hit Protocol does not scale. This can be attributed to its single-threaded validation phase, that is, the hit and terminated lists require exclusive accesses; additionally it uses a centralized transaction id generator. These two elements are known causes of bottlenecks and thus performance degradation in many- core environments (see Section 3.2). However, this does not mean the underlying wait-hit approach is not compatible with scalability as we now demonstrate in Section 3.5.</P>
<P>&nbsp;</P>
<P>WHP</P>
<P><BR>1.00</P>
<P><BR>0.75</P>
<P><BR>0.50</P>
<P><BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Throughput vs. cores.</P>
<P>WHP</P>
<P>40</P>
<P>30</P>
<P>20</P>
<P>10</P>
<P>0<BR>0&nbsp;10</P>
<P>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Abort rate vs. cores</P>
<P>WHP</P>
<P>0.100</P>
<P>&nbsp;</P>
<P>0.010</P>
<P>&nbsp;</P>
<P>0.001<BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) Average latency vs. cores.</P>
<P>Fig. 3.2 SmallBank &#8211; 100 customers (high contention).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5 Many-Core Wait-Hit Protocol<BR>In short, the Many-Core Wait-Hit Protocol avoids global timestamp allocation and parallelizes the validation phase. We assume there are n worker threads to process transactions, &#964; = 1, 2, ..., n. Each worker thread receives a transaction and executes it to completion before receiving additional work, thus can have at most one in-flight transaction at any moment. Each worker thread stores a local termination list, a list of entries for each transaction the worker thread has executed. Each entry denotes the transaction&#8217;s state, in-flight: -1, aborted: 0, or committed: 1. Also, as in Section 3.4, each worker thread maintains two per- transaction data structures to track the predecessors-upon-read and predecessors-upon-write of the transaction it is currently managing.<BR>With this configuration a centralized hit list is no longer required, as assuming that transactions can access other transactions&#8217; entries in termination lists stored on other workers, then a transaction Ti can "hit" Tj by setting Tj&#8217;s state to aborted: -1. Additionally, a central- ized termination list is not required as a validating transaction is concerned only with the termination statuses of its predecessors, which again, assuming transactions can access other transactions&#8217; entries in termination lists stored on other workers, it can directly look up as and when needed.<BR>The key observation in scaling the wait-hit approach here is that the hit list and terminated list can be combined into a single data structure, called the termination list, and distributed across worker threads, with each storing only information on the transactions they are responsible for executing.<BR>In summary, each worker thread &#964; needs to maintain two thread-local data structures:</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9642; Transaction id generator (ID&#964; ). A thread-local counter, or sequence number, used in combination with the thread id to assign transactions with unique ids (thread-id, seq-num).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; Termination list (TL&#964; ). A sequence of entries for each transaction the thread has dealt with, indexed by transaction id. Each entry denotes the corresponding transaction&#8217;s state (in-flight, aborted, or committed), e.g., Transaction Ti on thread &#964; has the entry TL&#964; (i) &#8712; {0, &#8722;1, 1}.<BR>Also, each worker thread maintains two data structures for each transaction to track the predecessors-upon-read and predecessors-upon-write of the transaction is it currently manag-<BR>ing. We denote the predecessors-upon-read for transaction Ti on worker thread &#964; as PuR&#964; (i)<BR>and the predecessors-upon-write as PuW&#964; (i).<BR>The Many-Core Wait-Hit Protocol has two benefits over the Basic Wait-Hit Protocol. Firstly, the transaction id generation process is distributed across workers mitigating this bottleneck. Secondly, during validation, transactions only need to access the termination lists of the workers who manage its predecessors, avoiding the exclusive validation phase of the Basic Wait-Hit Protocol. Additionally, workers&#8217; termination lists do not require exclusive access through a global lock, instead individual entries can have their own &#167;locks, which again increases parallelism.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.1 Protocol Description<BR>Prior to execution of transactions the database initializes the termination list on each worker thread &#964;, TL&#964; = 0/ and the transaction id generator, ID&#964; = 0. When worker thread &#964; receives a transaction, T , it gets a new id i from ID&#964; and initializes the predecessor lists for that transaction Ti, PuR&#964; (i) = 0/ and PuW&#964; (i) = 0/ .<BR>Worker thread &#964; optimistically executes Ti, collecting predecessor transactions encoun- tered, and recording them in PuR&#964; (i) or PuW&#964; (i) as appropriate. Conflict detection is the same as in the Basic Wait-Hit Protocol (see Algorithm 1). For each update of either PuR&#964; (i) or PuW&#964; (i) by thread &#964;, the worker can check the status of TL&#964; (i), aborting Ti if TL&#964; (i) = &#8722;1. This reduces wasted work when transactions are doomed to abort.</P>
<P>Thread &#964; completes the optimistic execution of transaction Ti and invokes the commit procedure in Algorithm 4. The validation phase again consists of a wait and a hit phase. In the hit phase (Lines 3-9, Algorithm 4), for each predecessor the transaction has detected when performing write operations during its lifetime, it accesses the transaction&#8217;s state on the corresponding worker&#8217;s thread and acquires an exclusive lock on the entry. It sets the state to<BR>&#8722;1 if the predecessor is in-flight (hit). In the wait phase, for each predecessor, it accesses the<BR>transaction&#8217;s state on its worker&#8217;s thread and aborts itself if the predecessor state is either<BR>&#8722;1 or 0; assuming a zero-wait policy (Lines 12-19, Algorithm 4). Note, in between each phase the transaction checks if it has been hit. Upon completion of the hit and wait phases the transaction sets its state to 1 in its thread-local termination list (Line 23, Algorithm 4).</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.2 Optimizations</P>
<P>Read-only Transactions The zero-wait policy in the Many-Core Wait-Hit Protocol (Lines 16-19, Algorithm 4) can be troublesome for transactions consisting solely of read operations. It is, however, harmless for read-only transactions to wait until all predecessors-upon-read have terminated. This adjustment is permissible as a read-only transaction cannot be a constituent in a wait deadlock.<BR>Deadlocks arise in the wait phase when transactions mutually read what each other has written, in other words, a cycle forms consisting of wr edges. Consider a read-only<BR>transaction Ti. A predecessor that exists in Ti&#8217;s predecessor-upon-read list (Tj &#8712; PuR(i)) must<BR>be an update transaction, Tj cannot be a read-only transaction, it must have written at least one record in order for Ti to read it. Consider the case in which Ti waits for Tj which is an update transaction. Tj or no other update transaction can be waiting for Ti to commit because Ti cannot be PuR&#964; of Tj, as it has not written anything. Thus, Tj cannot be involved in a wait cycle with Ti. Therefore, no update transaction a read-only transaction may wait for during</P>
<P>&nbsp;</P>
<P>Algorithm 4: Many-Core Wait-Hit Protocol commit procedure.<BR>Data: Transaction thisTx, Thread thisThread, Set&lt;Thread&gt; otherThreads<BR>Result: Boolean successfullyCommitted<BR>/* hit check&nbsp;*/<BR>1 if thisThread.terminated[thisTx.id] == -1 then<BR>2&nbsp;return false&nbsp;// hit by another tx<BR>/* hit phase&nbsp;*/<BR>3 while thisTx.writePredecessors = 0/ do<BR>4&nbsp;foreach pred&nbsp;thisTx.writePredecessors do<BR>5&nbsp;otherThread = otherThreads.get(pred.thread)<BR>6&nbsp;pred.state = otherThread.terminated[pred.id]<BR>7&nbsp;if pred.state == 0 then<BR>8&nbsp;set pred.state = -1&nbsp;// hit predecessor<BR>9&nbsp;thisTx.writePredecessors.remove(pred)<BR>/* hit check&nbsp;*/<BR>10 if thisThread.terminated[thisTx.id] == - 1 then<BR>11&nbsp;return false<BR>/* wait phase&nbsp;*/<BR>12 while thisTx.readPredecessors = 0/ do<BR>13&nbsp;foreach pred&nbsp;thisTx.readPredecessors do<BR>14&nbsp;otherThread = otherThreads.get(pred.thread)<BR>15&nbsp;pred.state = otherThread.terminated[pred.id]<BR>16&nbsp;if pred.state == 1 then<BR>17&nbsp;thisTx.readPredecessors.remove(pred)&nbsp;// read from committed tx<BR>18&nbsp;else<BR>19&nbsp;return !abort()&nbsp;// zero-wait policy/read from aborted tx<BR>/* hit check&nbsp;*/<BR>20 if thisThread.terminated[thisTx.id] == - 1 then<BR>21&nbsp;return false&nbsp;// hit by another tx<BR>22 else</P>
<P><BR>Algorithm 5: Many-Core Wait-Hit Protocol abort procedure.</P>
<P>Data: Transaction thisTx, Thread thisThread<BR>Result: Boolean successfullyAborted<BR>1 set thisThread.terminated[thisTx.id] = -1<BR>2 return true</P>
<P>the wait phase can give rise to a deadlock, read-only transactions may safely wait until all predecessors have terminated.</P>
<P>AIMD Wait Policy This section describes an optimization to the Many-Core Wait-Hit Protocol that improves the wait-policy in the wait-phase; note, it can also be applied to both Basic Wait-Hit Protocol and Distributed Wait-Hit Protocol. Thus far the wait phase has adopted a zero-wait policy, that is, if a predecessor arising from a read operation is in-flight when a transaction is validating then the validation fails. Such a strategy achieves:<BR>(i) correctness, alleviates the possibility of reading from a transaction that subsequently aborts, and (ii) avoids deadlocks in which transactions are waiting for each other to terminate. However, this approach could lead to spurious aborts that could be avoided by a limited degree of waiting. We now describe a wait policy based on the additive increase/multiplicative decrease (AIMD) algorithm, of which the zero-wait policy is a special case. It is fashioned after a similar use in the TCP/IP congestion model. The principles of the AIMD algorithm<BR>are waiting continues for &#948; time (one wait cycle). At the end of a wait cycle, if a positive outcome is seen then the wait cycle duration is increased &#948; = &#948; + a; otherwise waiting is<BR>decreased &#948; = &#948; &#215; b. After some upper wait limit, waiting is terminated, else another wait<BR>cycle is executed.<BR>The AIMD wait policy is given in Algorithm 6 and has four parameters: &#948; is the initial wait time which reflects the time period needed to pass before most predecessors have terminated, W denotes a high watermark which imposes a bound on the maximum time spent waiting, a is the additive increase which is applied each time the set of predecessors does not decrease, and b (0 &lt; b &lt; 1) is the multiplicative decrease which is applied when the set of predecessors decreases in size. If there is a deadlock, the size of the set of read predecessors will not decrease after a point and &#948; is guaranteed to fall below the low watermark, and the transaction will abort. Note, in the zero-wait policy: &#948; = 0 and a = 0.</P>
<P>&nbsp;</P>
<P>Algorithm 6: AIMD wait-phase</P>
<P>Data: Transaction thisTx, Thread thisThread, Set&lt;Thread&gt; otherThreads, W<BR>watermark, &#948; delta, a additive increase, b multiplicative decrease<BR>Result: Boolean successfulWaitPhase<BR>1&nbsp;&nbsp; while (&#948;&nbsp;W )&nbsp;( thisTx.readPredecessors = 0/ ) do<BR>2&nbsp;S = thisTx.readPredecessors.size()<BR>3&nbsp;wait &#948;<BR>4&nbsp;foreach pred&nbsp;thisTx.readPredecessors do<BR>5&nbsp;otherThread = otherThreads.get(pred.thread)<BR>6&nbsp;pred.state = otherThread.terminated[pred.id]<BR>7&nbsp;if pred.state == 1 then<BR>8&nbsp;thisTx.readPredecessors.remove(pred)&nbsp;// read from committed tx<BR>9&nbsp;else if pred.state == -1 then<BR>10&nbsp;return !abort()&nbsp;// read from aborted tx<BR>11&nbsp;else<BR>12&nbsp;continue<BR>13&nbsp;if thisTx.readPredecessors.size() &lt; S then<BR>14&nbsp;&#948; = &#948; + a<BR>15&nbsp;else<BR>16&nbsp;&#948; = &#948;&nbsp;b<BR>17&nbsp; if (thisThread.terminated[thisTx.id] ==&nbsp;1)&nbsp;(&#948; &lt; W ) then<BR>18&nbsp;return false<BR>19 else</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.3 Implementation Details</P>
<P>The Many-Core Wait-Hit Protocol requires the database to use a thread-per-core model, with each worker thread pinned to its respective core. As each thread can access the terminated list on other threads it is necessary such accesses are thread-safe, for example, each entry in the list is guarded by a Mutex. For space reasons, the terminated list must be pruned, this next section describes the epoch-based garbage collection mechanism used.<BR>Epoch-based Garbage Collection The Many-Core Wait-Hit Protocol adapts the epoch- based garbage collector mechanism in Subsection 3.4.2. Again, time is divided into epochs numbered, e = 1, 2,&nbsp; Let E denote the current epoch number, which the garbage collector<BR>periodically increments and asynchronously informs the threads of the new E. Thus, each</P>
<P>thread &#964; maintains its own local view of E, which is denoted as E&#964; . E&#964; &#8804; E at any time. In addition, the garbage collector maintains a list A of the number of active transactions that started in epoch e: A(e) indicates the number of transactions that started in epoch e and are<BR>still known to be active. When &#964; starts a new transaction Ti, it records E&#964; as start time ei,s,<BR>for Ti : ei,s = E&#964; and increments A(E&#964; ) by 1. When Ti terminates, &#964; records the current value of E&#964; as its finish epoch ei, f &#8594; (ei,s &#8804; ei, f ) and decrements A(ei,s) by 1.<BR>Let &#945; be the smallest epoch number across all threads with at least 1 in-flight transac- tion, hence, all transactions Ti that started with ei, f &lt; &#945; have terminated. Therefore, such transactions will never be predecessor and can be garbage collected. The garbage collection thread periodically views the local views of &#945;&#964; and computes the global view of &#945; by taking the minimum value of &#945;&#964; across all threads. &#945; is then broadcast back to threads, who use it to safely remove all entries in TL&#964; where e&#964;, f &lt; &#945;&#964; . This approach is appealing as it does not require threads&#8217; views of E and &#945; to be synchronously updated.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.4 Evaluation</P>
<P>In this section, we experimentally compare the Many-Core Wait-Hit Protocol, referred to as MC-WHP, with the many-core SGT implementation described in Subsection 4.2.2 and with classical strict 2PL (Subsection 2.1.3) in which locks are held until commit point; the evaluation framework does not support predicate locks, thus only read/write locks are used. To be precise, we focus on ascertaining the performance implications and scalability of the Many-Core Wait-Hit Protocol&#8217;s lightweight validation strategy and determining how the wait-hit approach compares to a graph-based approach. We include 2PL as it is still used in several commercial systems, e.g., [87, 92].</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.5 SmallBank</P>
<P>Firstly, we test the protocols using the SmallBank workload described in Subsection 2.3.3. We consider the case of high contention by setting the database to contain only 100 customers; transaction requests are uniformly distributed spread across the 100 customers. As can be seen from Figure 3.3(a), as the core count is increased MC-WHP offers comparable throughput to SGT (within 2% difference) and exceeds that of 2PL by over 4x. However, in Figure 3.3(b), MC-WHP does have around a 2x increase in aborts over SGT, but half as many as 2PL. This is anticipated as the Many-Core Wait-Hit Protocol&#8217;s validation strategy is designed to sacrifice some executions in order to be lightweight and highly parallelizable. For average latency displayed in Figure 3.3(c), SGT and MC-WHP&#8217;s performance is indistinguishable (within a 1% difference).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.6 YCSB<BR>Then, we further investigate protocol performance using YCSB with the following workload factors: we opt for a medium contention level, &#952; = 0.8, and a balanced read/update transaction mix, U = 0.5. The observed pattern is similar to Subsection 3.5.5. However, interestingly in Figure 3.4(a) the Many-Core Wait-Hit Protocol is able to outperform SGT at 30 cores, showing a 1.3% increase in throughput and 36% increase at 40 cores. Again, the abort rate is around 3x higher in MC-WHP compared to SGT in Figure 3.4(b), even though after 30 cores the gap shrinks to 2x. Lastly, in Figure 3.4(c), SGT has marginally lower latency up to 30 cores after which MC-WHP has lower average latency. Across all metrics, SGT and the Many-Core Wait-Hit Protocol outperform 2PL.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.7 TATP</P>
<P>Finally, we examine the scalability of the protocols using the TATP workload. TATP is interesting as its transactions generate few conflict cycles, thus the abort rate is naturally</P>
<P>&nbsp;</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>10.0</P>
<P>7.5</P>
<P>5.0</P>
<P>2.5</P>
<P><BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. cores.</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>20</P>
<P>15</P>
<P>10</P>
<P>5</P>
<P>0<BR>0&nbsp;10</P>
<P>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. cores</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>&nbsp;</P>
<P>0.010</P>
<P><BR>0.003</P>
<P><BR>0.001</P>
<P>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. cores.</P>
<P>Fig. 3.3 SmallBank &#8211; 100 customers (high contention).</P>
<P>&nbsp;</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P><BR>3</P>
<P><BR>2</P>
<P><BR>1</P>
<P><BR>0<BR>0&nbsp;10</P>
<P><BR>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Throughput vs. cores.</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>20</P>
<P>15</P>
<P>10</P>
<P>5</P>
<P>0<BR>0&nbsp;10</P>
<P>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Abort rate vs. cores</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>0.100</P>
<P><BR>0.030</P>
<P>0.010</P>
<P><BR>0.003<BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) Average latency vs. cores.</P>
<P>Fig. 3.4 YCSB &#8211; Performance measurements for the protocols as the core count is increased from 1 to 40 cores with medium contention, &#952; = 0.8, and a balanced update rate U = 0.5.</P>
<P>lower. We anticipate that due to this the performance difference between MC-WHP and SGT will be narrow. This is what we observe in Figure 3.5, whilst SGT does outperform the Many-Core Wait-Hit Protocol across all metrics, the magnitude of difference is lower than in previous experiments.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5.8 Discussion</P>
<P>All three workloads considered so far have featured a non-negligible proportion of aborts, however, in reality abortion may rarely take place in some workloads. Under these conditions the Many-Core Wait-Hit Protocol protocol with a zero-wait policy again has an advantage over its competitors.<BR>Firstly, consider the case when infrequent aborts arise from low contention in the work- load. A lack of contention implies transactions are not accessing the same data concurrently, which for the Many-Core Wait-Hit Protocol translates to few predecessors being collected and therefore a quicker validation phase. Secondly, there is the case when there is high contention in the workload, but this does not translate to cycles and thus the abort rate still remains low. In this case, as stated in Subsection 3.5.2, the zero-wait policy could lead to spurious aborts, hence the AIMD wait policy could be used. This however is not a panacea, as in the case when a transaction has incorrectly waited and aborted, we have in effect increased the transaction&#8217;s time spent in the system. An increased transaction lifetime means an increased contention window and therefore the overall abort rate in the system may rise as a sequence of transactions that are &#8216;doomed&#8217; to abort needlessly wait for one another.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.6 Distributed Wait-Hit Protocol</P>
<P>In this section, we show how the wait-hit approach can be adapted to a distributed database. The Distributed Wait-Hit Protocol assumes a shared-nothing database consisting of several</P>
<P>&nbsp;</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>&nbsp;</P>
<P>12</P>
<P>8</P>
<P>4</P>
<P>0<BR>0&nbsp;10</P>
<P>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. cores.</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P>&nbsp;</P>
<P>2</P>
<P><BR>1</P>
<P><BR>0<BR>0&nbsp;10</P>
<P>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. cores</P>
<P>2PL&nbsp;MC&#8722;WHP&nbsp;SGT</P>
<P><BR>0.010</P>
<P><BR>0.003</P>
<P><BR>0.001</P>
<P>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. cores.</P>
<P>Fig. 3.5 TATP &#8211; 100 entries.</P>
<P>machines, or shards. Each shard is responsible for managing a disjoint partition of the database. Each multi-partition transaction, which involves data stored in multiple partitions has a home shard, which is the shard responsible for coordinating the transaction on behalf of the client and a set of remote shards. From a shard&#8217;s perspective, transactions for which it is the coordinator are referred to as local transactions, whereas, transactions that are managed by another shard, but execute on the shard, are referred to as foreign transactions.<BR>A multi-partition transaction begins at its home shard and is assigned to a coordinator thread. The multi-partition transaction then visits at least one remote shard, where it is managed by a surrogate thread on the remote shard. In summary, within each shard there is a set of threads designated for transaction management, which are further divided into two sets:<BR>&nbsp;&nbsp;&nbsp; (i) coordinator threads: responsible for handling all local transactions and coordinating with remote shards if transactions are multi-partition. (ii) surrogate threads: responsible for handling foreign transactions. Each thread, &#964;, has a number of thread-local data structures:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; Transaction ID generator: used to assign a transaction with a database-wide unique id. It is a combination of the shard id, thread id, and a thread-local sequence number. (Coordinator thread only).<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; Termination list (TL&#964; ). A list of entries for each transaction the thread has dealt with, indexed by transaction id. Each entry denotes the corresponding transaction&#8217;s state (in-flight: 0, aborted: -1, or committed:1). Note, if the thread is the coordinator thread then all entries will be local transactions, else it is a surrogate thread and all transactions will be foreign transactions.<BR>Additionally, as in Sections 3.4 and 3.5 each worker maintains two per-transaction data struc- tures to track the predecessors of the transaction (local or foreign) it is currently managing: predecessors-upon-read and predecessors-upon-write.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.6.1 Protocol Description</P>
<P>The Distributed Wait-Hit Protocol consists of three stages: initialization, execution, and commitment. The commitment stage is broken into two phases: wait and agreement. This section describes each stage from the perspective of the coordinator thread and the surrogate threads. The normal execution flow of the protocol is provided in Subsection 3.6.3, orange circles link to key points in the execution flow. Messages are introduced as and when needed; a summary of messages sent in the protocol is given in Subsection 3.6.3.</P>
<P>Coordinator Thread</P>
<P>Initialization A shard in the database receives a transaction request from a client. The shard then allocates the transaction to a thread. This thread is the coordinator thread for the transaction and it performs three actions: (i) assigns the transaction with an id using its ID generator, (ii) appends the transaction to its termination list, and (iii) creates transaction-local<BR>predecessor upon read/writes lists,&nbsp;.</P>
<P><BR>Execution The coordinator thread optimistically executes the operations within the transac- tion. If read/write predecessors are encountered while accessing local data, they are recorded into the appropriate predecessor list. An important point to note is that if the operation is a write operation and predecessors are detected then they are proactively hit and their state set to aborted once the operation has completed. Whilst this may induce more aborts it simplifies atomic commitment (details to follow in Subsection 3.6.2). If the transaction requires remote data on another shard, the coordinator forwards the operation to the relevant shard using a<BR>REMOTE-OP message,&nbsp;. Note, the remote shard will return the result of the REMOTE-<BR>OP request; if the remote operation is a write operation then any predecessors-upon-write detected on the remote shard will also be proactively hit on the remote shard. This process repeats until the completion of all operations within a transaction and it is ready to commit at</P>
<P>which point the coordinator sends a VALIDATE message to participant shards (including itself) contacted during the transaction&#8217;s execution in order to begin the commit procedure,<BR>.</P>
<P>Commit Due to proactive hitting when the commit procedure is called all of a transaction&#8217;s predecessors-upon-write will have been aborted globally. Thus, the commit procedure con- tains a single validation phase: wait. In the wait phase the coordinator iterates through its predecessor-upon-read list, if the predecessor is committed it is removed from the list, else the predecessor has aborted or is in-flight, thus the coordinator must abort the transaction. Additionally, concurrent with this procedure the coordinator is listening for FAILED mes- sages from remote shards; such a message is sent by surrogate threads on remote shards to indicate the transaction has failed validation locally. If the coordinator fails the wait phase locally or receives such a message then it terminates the transaction, informs the client, and sends ABORT to all surrogate threads. Else, the wait phase is completed and the coordinator delays until it has received a VERIFIED message from all remote shards at which point all surrogates have completed the wait phase and the transaction has globally been validated. The coordinator now enters the agreement phase and sets the transaction state to committed in its termination list. This is the point of no return, once here the transaction must and will always commit.<BR>In the agreement phase the coordinator simply sends COMMIT,&nbsp;, to all remote shards<BR>and waits until it has received COMMITTED from all participants before responding to the</P>
<P>client,&nbsp;.</P>
<P><BR>Surrogate Thread</P>
<P>Initialization When a shard receives a REMOTE-OP it allocates the request to a surrogate thread. The request will include the transaction id assigned by the coordinator thread. It adds the REMOTE-OP&#8217;s corresponding transaction to its termination list; indexed by the</P>
<P>corresponding transaction ID (assigned by the coordinator thread). The surrogate thread then</P>
<P>creates transaction-local predecessor upon read/writes lists,&nbsp;. Note, these lists only store</P>
<P>predecessors detected from operations executed for this transaction on this remote shard.</P>
<P>Execution The surrogate thread optimistically executes the operation and any subsequent operations for this transaction, detecting predecessors and installing them into the appropriate predecessor list. If the remote operation is a write operation then any predecessors-upon-write detected on the remote shard are proactively hit. The surrogate thread then delays until it<BR>receives VALIDATE from the coordinator thread and then enters the commit procedure,&nbsp;.<BR>Note, at any time during execution if an ABORT message is received the surrogate thread aborts the transaction.</P>
<P>Commit In the commit procedure, the surrogate performs the wait procedure, iterating through its predecessor-upon-read list: if the predecessor is committed it is removed from the list, else the predecessor has aborted or is in-flight, thus the surrogate thread must abort the transaction and sends a FAILED message to the home shard. If the wait phase is successful then a VERIFIED message is sent to the coordinator and the surrogate waits for a COMMIT message before proceeding to the agreement phase. In the agreement phase, the surrogate simply sets the transaction state to committed and replies with COMMITTED to<BR>the coordinator,&nbsp;.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.6.2 Discussion</P>
<P>To summarize, the Distributed Wait-Hit Protocol follows a similar pattern to other optimistic concurrency control protocols in that it integrates the validation phase with the first round of 2PC: the validation phase maps to the prepare phase of 2PC. In the Distributed Wait-Hit Protocol this is enabled by proactively hitting any predecessors-upon-write detected at the end of each write operation performed. Without proactive hitting an additional phase (round</P>
<P>trip) in the commit procedure would be needed to ensure that all predecessors-upon-write are globally hit across all participant shards. The additional latency incurred per-transaction for another round trip would potentially be unpalatable for most OLTP applications.<BR>The benefit of Distributed Wait-Hit Protocol is that validation at each shard (remote and home) can proceed independently in parallel, a clear advantage over distributed SGT which requires distributed cycle check and thus a high-level of inter-shard communication. The validation phase also possesses the same benefits as that in the Many-Core Wait-Hit Protocol in that it is extremely lightweight, compared to OCC, the Distributed Wait-Hit Protocol does not have the overheads of copy items during transaction execution. Compared to MVCC and TO which block newer transactions that conflict until the older ones commit, the Distributed Wait-Hit Protocol effectively allows transactions to race, thus should increase throughput in situations when data items are highly contended, but the overall abort rate is low.<BR>In Subsection 3.5.8 the performance of the Many-Core Wait-Hit Protocol under a work- load with infrequent aborts was discussed. We now discuss the possible implications of such a workload for the Distributed Wait-Hit Protocol. When the abort rate is low due to low contention the performance of the protocol is bound by the network speed for performing validation. In the case there is contention (thus predecessors lists are non-empty) and a non-zero wait policy is employed, the drawbacks of incorrectly waiting are more pronounced than in the Many-Core Wait-Hit Protocol as the lifetime of a transaction is already higher due to the network calls required for validation, so the contention window of transactions would increase. This could lead to cascading aborts, degrading system performance.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.6.3 Messages</P>
<P>This section summarizes the messages exchanged between servers in the execution of a distributed transaction.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; REMOTE-OP: issued by a coordinator thread to a remote shard in order to access data on the remote server. The acknowledgement to a REMOTE-OP can include the results to read operations if required.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; VALIDATE: issued by a coordinator thread to indicate its transaction has completed execution and wishes to be committed. Sent to all surrogate threads to initiate their validation phase.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; VERIFIED: sent by surrogate threads to the coordinator thread to indicate they have completed the wait phase and locally successfully validated.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; FAILED: sent by surrogate threads to the coordinator thread to indicate it has failed validation locally.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; COMMIT: issued by the coordinator thread to surrogate threads if they have all responded with VERIFIED (all completed validation).<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; ABORT: issued by a coordinator thread to surrogate threads if at least one surrogate thread responded with FAILED.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#8226; COMMITTED: sent by surrogate threads to the coordinator thread to indicate it has completed the agreement phase and successfully committed.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.7 Conclusion</P>
<P>In this chapter we presented Wait-Hit Protocol, an optimistic concurrency control protocol that is designed to scale vertically, as the core count is increased, and horizontally, as database nodes are added. This is achieved through a lightweight validation phase that is highly parallelizable. We have experimentally demonstrated that the wait-hit approach can offer comparable performance with contemporary high performance protocols in a many-core</P>
<P>&nbsp;</P>
<P>S1(x)<BR>coordinat</P>
<P><BR>S2(y)<BR>surrogate</P>
<P>S3(z)<BR>surrogate</P>
<P>Fig. 3.6 Distributed wait-hit protocol messages exchanged during happy path execution. Orange circles link to the textual description of the protocol. Note, the wait phase is indicated by the orange hashed area in the validation phase.</P>
<P>environment at the cost of some additional aborts. The validation phase is designed in a manner that addresses previous shortcomings of optimistic protocols in distributed databases.</P>
<P>3.7.1&nbsp;Further Work</P>
<P>Additional Experiments As part of future work we wish to extend our evaluation frame- work to allow for the implementation of several classical and state-of-the-art distributed concurrency control protocols. Doing so will allow us empirically to compare the per- formance of Distributed Wait-Hit Protocol with other protocols such as distributed OCC, MVCC, and SGT, enabling us to validate our design and gain empirical support for the claims made in Subsection 3.6.2.<BR>Prior work on evaluating distributed concurrency control protocols focused on perfor- mance in a single data center [54]. Future work could expand to large-scale distributed database systems that span multiple data centers and geographic regions. In such a setting the communication overhead imposed by optimistic methods (such as the Distributed Wait-Hit Protocol) for validation may increase latency and reduce system throughput to unpalatable levels. This challenge presents an opportunity to explore incorporating several mitigation</P>
<P>strategies into the protocol. One such strategy would be aggregating multiple transactions into batches to amortize the network overheads over many transactions, this could increase the latency of individual, but maintain a stable average latency making for an improved user experience. Another approach would be to exploit advances in modern hardware and use technologies such as programmable switches [16] to design optimistic protocols that interact better with the underlying hardware to maximize performance.</P>
<P>Integration with Machine Learning Another fertile area for exploration is the impact machine learning (ML) can have on improving the performance of concurrency control protocols in database systems. Some initial work has focused on making improved scheduling decisions in many-core main memory database systems [95], it would be interesting to investigate whether similar techniques could be applied in a distributed context in combination with the wait-hit approach. Additionally, other recent work [112] has leveraged ML to develop a learning-based framework that adapts the concurrency control protocol to the workload, future work could determine if a similar approach could be used to optimize the variants of the Wait-Hit Protocol to the workload and the scale point it is targeting.</P>
<P>Fault Tolerance and Availability For fault tolerance and availability, shards within a distributed database are normally replicated. Balancing fault tolerance and availability with transactional performance and system consistency is a delicate act. One approach future work could explore is how the wait-hit approach can be integrated with strong replication strategies such as Raft [82] and Paxos [68]. However, using these protocols can lead to increased latency due to the extra communication between replicas to guarantee consensus, also reducing throughput. On the other hand, weaker replication strategies offer improved performance at the cost of weak consistency guarantees in the face of component failures and network partitions, which may not match up with user expectations.</P>
<P>Switching&nbsp; The motivation for developing the family of optimistic concurrency controls in this Chapter was to have a single protocol that can scale from a many-core environment to a large-scale deployment within a data center or across multiple data centers. Protocols have been presented for each of these scale points, but missing from discussions was how to facilitate the transition between them. Therefore, an open and interesting avenue for future work is to investigate how to dynamically switch from the Many-Core Wait-Hit Protocol to the Distributed Wait-Hit Protocol as resources are added and removed. This will help satisfy users&#8217; expectations of seamless elastic database scaling as hardware resources are altered.<BR>The goal to strive for when switching from the Many-Core Wait-Hit Protocol and the Distributed Wait-Hit Protocol from the client perspective is for there to be no performance drop, for there to be no read or write unavailability to the system, and for the transition to be tolerant to faults. One key issue to be addressed is whether the single-node running the Many-Core Wait-Hit Protocol would become part of the distributed system running the Distributed Wait-Hit Protocol or not. Such an in-place transition would be resource efficient, but the bootstrap time to form the distributed cluster may impact performance. This could be particularly pronounced if the cluster is running at near peak load as the underlying thread pool would be reduced due to division between coordinator threads and surrogate threads for the Distributed Wait-Hit Protocol.<BR>An alternative approach would be to start a shadow cluster, avoiding the need for a boot- strap phase, and use a database proxy to mirror traffic to the system running the Distributed Wait-Hit Protocol- keeping them in sync. This approach would be less resource efficient compared to an in-place transition, but this may be tolerable in a cloud environment as the server running the Many-Core Wait-Hit Protocol can be decommissioned as soon as the switch has been made. Running a hot shadow cluster would also possibly provide more flexibility around data sharding. Before the transition the shadow cluster can move data between shards without the concern of impacting throughput or latency, whereas, with the</P>
<P>in-place switch the initial cluster running the Many-Core Wait-Hit Protocol would have a imbalanced data partitioning which could yield unstable performance.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Chapter 4</P>
<P>Mixed Serialization Graph Testing</P>
<P>Summary<BR>Serialization Graph Testing faithfully implements the conflict graph theorem described in Subsection 2.1.1 by aborting only those transactions that would actually violate serializability (introduce a cycle), thus maintaining the required acyclic invariant. All other known approaches, such as 2PL and the Wait-Hit Protocol, disallow certain valid schedules to increase throughput. Thereby, Serialization Graph Testing has the theoreti- cally optimal property of accepting all and only conflict serializable schedules. However, as discussed in Subsection 2.1.2, not all applications require conflict serializability, but can perfectly settle for various, known weaker isolation levels which typically require relatively lower overheads; we corroborate this with a survey of the isolation levels supported by 24 databases in Section 4.3. In such a mixed environment, providing only the isolation level required of each transaction should, in theory, increase throughput and reduce aborts. This chapter extends Serialization Graph Testing for mixed environments subject to Adya&#8217;s mixing-correct theorem and confirms improvement in performance. Mixed Serialization Graph Testing can achieve up to a 28% increase in throughput and a 19% decrease in aborts over Serialization Graph Testing.</P>
<P>4.1&nbsp;Introduction</P>
<P>Compared to the other approaches described in Subsection 2.1.3, the graph-based approach1 to serializable transaction processing possesses the desirable theoretical property of accepting all conflict serializable schedules. Other approaches approximate the complete space of conflict serializable schedules, but in doing so sacrifice a degree of concurrency in an attempt to achieve higher throughput.<BR>This can be illustrated by the following example. Consider two transactions, T1 and<BR>T2, execute in a database running 2PL. Both transactions concurrently attempt to access</P>
<P>1Also referred to as Serialization Graph Testing (SGT)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Introduction&nbsp;81</P>
<P>a single data item, x; assume T1 and T2 access no other items and that there are no other transactions executing in the system. T1 wishes to write x and T2 wishes to read x. Let us say T1 accesses x first and thus acquires a write lock on x. When T2 attempts to acquire a read lock on x, depending on the deadlock detection strategy used, either T1 or T2 will be aborted.<BR>This discounts a valid conflict serializable schedule, as no interleaving of T1 and T2 could introduce a cycle in the conflict graph (either T1 &#8594; T2, or T2 &#8594; T1).<BR>Despite Serialization Graph Testing&#8217;s theoretical upper-hand, it was historically dis-<BR>counted as a viable strategy because the associated computational costs of maintaining an acyclic graph were deemed impractical. However, recent research has refuted this perceived wisdom. In [34] it was demonstrated how SGT can be implemented efficiently in a many-core database, offering comparable, and often higher, performance when compared to traditional and contemporary concurrency control protocols.<BR>In spite of recent advances, serializable transaction processing performance often remains unsatisfactory for application demands. As introduced in Subsection 2.1.2, another tool at databases&#8217; disposal to increase performance is to execute transactions at weak isolation levels, e.g., Read Committed. Here, the number of permissible schedules is increased at the expense of potentially allowing non-serializable behaviour, e.g., Fuzzy Reads. Weak isolation is pervasive in real world systems, with most systems offering a range of isolation levels; a comprehensive survey of the isolation levels supported by commercial and open source databases is given in Table 4.1. A database that allows concurrent transactions to be executed at different isolation levels is said to be mixed [1]. For example, transaction TRC can run at Read Committed and transaction TS at Serializable.<BR>The classical method to implement a mixed DBMS is to opt for a 2PL-variant in which transactions vary the duration they hold locks [51]. For example, TRC would release read locks on data items immediately after performing the read operation, whereas TS holds all locks until commit time. This mechanism and others used in mixed DBMSs suffer from</P>
<P>the same problem as their serializable equivalents: some valid executions are prevented leading to unnecessary aborts. This begs the question: how can SGT be extended to allow some transactions to be executed at weak isolation levels, whilst accepting all and only valid executions? Such an approach would permit higher concurrency and performance.<BR>This chapter presents Mixed Serialization Graph Testing (MSGT), which accepts all valid schedules under Adya&#8217;s mixing-correct theorem [1], thus maintaining SGT&#8217;s property of minimizing aborts. We evaluate MSGT&#8217;s performance using a range of popular OLTP benchmarks.<BR>The remainder of this chapter is structured as follows: Section 4.2 provides a detailed overview of SGT and the many-core implementation presented in [34]. Section 4.3 provides a survey of the isolation levels provided by ACID and NewSQL [86] databases highlight- ing the prevalence of mixed DBMSs and demonstrating the utility of MSGT. Section 4.4 describes Adya&#8217;s formalism of weak and mixed isolation levels. Section 4.5 presents Mixed Serialization Graph Testing. Section 4.6 gives results, before Section 4.7 concludes.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Serialization Graph Testing</P>
<P>This section presents Serialization Graph Testing. Subsection 4.2.1 provides a description of the protocol. Subsection 4.2.2 describes the many-core optimized concurrent graph data structure used by the SGT implementation in [34]. Where appropriate we draw comparisons with the Wait-Hit Protocol in Chapter 3.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2.1 Protocol Description</P>
<P>In this chapter we focus on basic-SGT [12], which operates as follows:</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. When the database scheduler receives transaction Ti it creates a node in its conflict graph for Ti.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Then, for each operation on item x, opi(x), within Ti,</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Conflicts are determined. opi(x) conflicts with opj(x) of Tj if opj(x) &lt; opi(x), and if opi(x) is a read and opj(x) is a write (wr conflict), or if opi(x) is a write and opj(x) is a read or write (rw/ww conflicts).<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) For each conflict, an edge is inserted into the graph from the conflicting transac- tion, Tj &#8594; Ti; provided an edge Tj &#8594; Ti does not already exist. Note, transactions only insert incoming edges to themselves.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) Then, a cycle check is performed before executing the operation. If executing the operation would introduce a cycle Ti is aborted and its outgoing edges removed.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. At commit time, a Ti delays until it has no incoming edges and then removes its outgoing edges. The removal of outgoing edges ensures conflicting transactions will commit. Note, if Ti has no incoming edges then it cannot be involved in a cycle.<BR>Delaying the commitment of transactions until they have no incoming edges achieves:<BR>(i) safe node deletion, (ii) recoverability, and (iii) order preservation. To illustrate this consider schedule s, which contains transactions T1, T2, T3, and T4. Recall, a write on item x by transaction Ti is denoted by wi[x], a read by ri[x], and a commit operation by ci. The corresponding conflict graph CG(s) is shown in Figure 4.1.</P>
<P>s = w1[x] r2[x] r2[y] w1[y] w2[z] w3[z] r3[x] r3[a] w4[a] c1 c3 c2 c4</P>
<P>&nbsp;</P>
<P><BR>Fig. 4.1 Conflict graph representation of s.</P>
<P>Due to space constraints nodes must be pruned from the conflict graph. Allowing transactions to commit with incoming edges can lead to subtle serialization violations.<BR>For example, assume in Figure 4.1, T3 has committed and T2 is still executing. If T3 is removed upon commitment, T2 may subsequently perform an operation that introduces a cycle with T3, but which goes undetected due to T3&#8217;s removal, introducing a serialization error. For a node to be involved in a cycle it must have at least one incoming and one outgoing edge. As transactions only insert edges incoming to themselves during execution, after issuing a commit request, no more incoming edges are added. Thus, once all incoming edges are removed from the committing node, via the parent node terminating, it will not be in a cycle. Recoverability ensures failures do not leave the database in an inconsistent state. In s, T1 writes x, then T2 reads x. If T2 commits before T1, T1 may subsequently abort, at which point T2 has read from a value that never existed. Delaying T2&#8217;s commit until it has no incoming<BR>edges prevents this issue; if T1 aborts then T2 is also aborted.<BR>Order preservation ensures the real-time commit order matches the serialization order. In s, T4 overwrites the value a read by T3, thus in the serial order: T3 &#8594; T4. If T4 commits before T3 no recoverability issues are introduced, but the real-time commit order does not match the serialization order. Delaying T4&#8217;s commit until it has no incoming edges avoids this, providing users with an improved, more intuitive, experience.<BR>Note, the Wait-Hit Protocol in Chapter 3 bears resemblance to SGT in that for each operation within a transaction conflicts are detected, but rather than converting this informa- tion into edges in a conflict graph, the Wait-Hit Protocol compresses conflict information into predecessor-upon-read and predecessor-upon-write sets. Thus, exact cycle detection in Wait-Hit Protocol is not possible and some schedules are unnecessarily aborted. However, the Wait-Hit Protocol&#8217;s compressed conflict information allows the protocol&#8217;s validation to scale horizontally in a partitioned database, whereas in SGT&#8217;s cycle checking across</P>
<P>partitions would incur significant network overheads for each conflict detected. In summary,<BR>Wait-Hit Protocol is trading off some unnecessary aborts for scalability.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2.2 Many-Core Implementation</P>
<P>SGT was efficiently implemented in [34] using a graph data structure designed to facilitate concurrent cycle checking and avoid the pitfalls of other many-core concurrency control protocols.<BR>As stated in Subsection 2.1.4, many-core DBMSs use optimistic approaches as their pessimistic counterparts suffer from poor performance as the core count increases. A common bottleneck in optimistic approaches is an exclusive single-threaded verification phase. Regarding SGT, in [34], to avoid a global lock for graph operations, the graph data structure utilizes a node-local locking protocol. Nodes in the graph each store a transaction status (committed, active, aborted) and two sets of pointers representing incoming and outgoing edges. Nodes can then be locked in two modes:<BR>&nbsp;&nbsp;&nbsp; &#8226; Shared mode: transactions can concurrently access the node for edge insertions, edge deletions, and cycle checking. Edge sets guarantee thread-safe concurrent access for scans, insertions, and deletes under the shared lock.<BR>&nbsp;&nbsp;&nbsp; &#8226; Exclusive mode: used for the commit-critical check for incoming edges.</P>
<P>The node-level locking protocol works as follows: when a transaction Tx identifies a conflict with transaction Ty it first checks if an edge already exists from Ty to Tx, if so, no additional work is needed. Else, Tx acquires a shared lock on Ty&#8217;s node. If Ty is active, then an edge pointer to Tx is inserted into Ty&#8217;s outgoing edge set and an edge pointer from Ty is inserted into Tx&#8217;s incoming edge set. Tx then checks for a cycle using a reduced depth-first search (DFS) algorithm. Reduced DFS begins at the validating node (Tx) and traverses only the portion of the graph that is needed; each step holds nodes in shared lock mode. At commit</P>
<P>time, Tx acquires an exclusive lock on its node and checks for incoming edges. If there are none, Tx commits and shared locks are acquired on each node in Tx&#8217;s outgoing edge set and the edge from Tx removed. Else, there exists at least one incoming edge and the exclusive lock is released and the check is repeated until either the transaction commits or is aborted. Note, the exclusive lock is needed to prevent other transactions traversing Tx during their cycle checking or from adding in outgoing edges from Tx.<BR>Another common bottleneck is the reliance on a global timestamp allocator, this is avoided in [34] by letting conflict graph nodes double up as transaction ids.<BR>SGT requires a mechanism to derive conflicts. In [34] database rows store a sequential history of accesses. Each access stores the operation type, read or write, and the transaction id. Decoupling the access information from the graph data structure decreases contention. Note, sequentially ordered access is ensured by per-row spin-locks which are released immediately after the operation completes, i.e., the lock is not held until commit time, an improvement over lock-based approaches.<BR>In summary, the SGT implementation in [34] decouples conflict detection from the management of the conflict graph, and employs graph structure that allows concurrent cycle checking. In particular, the commit critical check only shortly blocks other threads from accessing a node and only the part of the serialization graph needed for validation is traversed. Lastly, nodes in the graph double up as transaction ids, avoiding a well established bottleneck in other protocols. It achieves this whilst also minimizing the number of unnecessary aborts, accepting all conflict serializable schedules and provides an ideal baseline for the development of MSGT.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Mixing in the Wild&nbsp;87</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Mixing in the Wild</P>
<P>This section motivates the development of a mixed graph-based scheduler that minimizes unnecessary aborts by surveying the isolation levels supported by commercial and open source DBMSs.<BR>It is rare that practical DBMSs offer applications only a singular isolation, instead permitting transactions to be run at different isolation levels. In order to assess this claim we surveyed the isolation levels offered by 24 ACID and NewSQL database systems in Table 4.1. Classification was performed based on each database&#8217;s public documentation. We found seven isolation levels represented: Read Uncommitted (RU), Read Committed (RC), Cursor Stability (CS), Snapshot Isolation (SI), Consistent Read (CR), Repeatable Read (RR), and Serializable (S). Note, the exact behaviour of each isolation level is highly system-dependent. Interestingly, we found 18 databases supported multiple isolation levels. Of systems offering a single isolation level Serializable was the most common; these systems were typically NewSQL [86] systems, e.g., CockroachDB [98]. This may suggest a trend away from mixed databases, however, TiDB [55, 105] recently added support for Consistent Read isolation indicating the utility of weaker isolation remains.<BR>Our survey&#8217;s findings are corroborated by a 2017 survey of database administrators on how applications use databases [85], the survey found the majority of transactions execute at Read Committed. In short, this evidence illustrates the ubiquity of mixed databases and motivates the development of MSGT.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4 Mixing Theory</P>
<P>This section presents the correctness criteria utilized by MSGT. Subsection 4.4.1 reproduces the system model from [1], which is used to define weak isolation levels in Subsection 4.4.2, before the mixing-correct theorem is defined.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Table 4.1 Isolation Levels Supported by ACID and NewSQL Databases</P>
<P><BR>Database<BR>System<BR>Isolation Level</P>
<P>RU<BR>RC<BR>CS<BR>SI<BR>CR<BR>RR<BR>S<BR>Actian Ingres 11.0<BR>Clustrix 5.2<BR>CockroachDB 20.1.5 Google Spanner Greenplum 6.8<BR>Dgraph 20.07<BR>FaunaDB 2.12 Hyper<BR>IBM Db2 for z/OS 12.0 MySQL 8.0<BR>MemGraph 1.0<BR>MemSQL 7.1<BR>MS SQL Server 2019 Neo4j 4.1<BR>NuoDB 4.1<BR>Oracle 11g 11.2 Oracle BerkeleyDB Oracle BerkeleyDB JE Postgres 12.4<BR>SAP HANA SQLite 3.33<BR>TiDB 4.0<BR>VoltDB 10.0<BR>YugaByteDB 2.2.2<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;b<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;<BR>&#10003;b<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;e<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;a<BR>&#10003;<BR>&#10007;<BR>&nbsp;&nbsp;&nbsp; &#8226; &#8727;e<BR>&#10003;&#8727;<BR>&#10003;&#8727;<BR>&#10003;<BR>&#10003;&#8727;<BR>&#10003;<BR>&#10003;<BR>&#10003;&#8727;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&nbsp;&nbsp;&nbsp; &#8226; &#8727;c<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10003;c<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10003;<BR>&#10003;&#8727;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10003;<BR>&#10007;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;<BR>&#10007;<BR>&#10007;<BR>&#10003;<BR>&#10003;<BR>&#10003;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10007;<BR>&#10003;&#8727;<BR>&#10003;<BR>&#8727; Indicates the default setting.<BR>a Referred to as Read Stability.<BR>b Behaves like Read Committed due to MVCC implementation.<BR>c Implemented as Snapshot Isolation. d Requires manual lock management. e Behaves like Consistent Read.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4.1 System Model<BR>In Adya&#8217;s system model, transactions consist of an ordered sequence of read and write operations to an arbitrary set of data items, book-ended by a BEGIN operation and a COMMIT or an ABORT operation. The set of items a transaction reads from and writes to is termed its item read set and item write set. Each write creates a version of an item, which is assigned a unique timestamp taken from a totally ordered set (e.g., natural numbers) version i of item x is denoted xi; hence, a multiversioned system is assumed. All data items have an initial unborn version &#8869; produced by an initial transaction T&#8869;. The unborn version is located at the start of each item&#8217;s version order. An execution of transactions on a database is represented by a history, H. This consists of a partial order of events, which reflects (i) each transaction&#8217;s read and write operations, (ii) data item versions read and written and (iii) commit or abort operations, and a version order, which imposes a total order on committed data item versions. There are three types of dependencies between transactions, which capture the ways in which transactions can directly conflict. Read dependencies capture the scenario where a transaction reads another transaction&#8217;s write. Antidependencies capture the scenario where a transaction overwrites the version another transaction reads, they can be seen as the opposite of read dependencies. Write dependencies capture the scenario where a transaction overwrites<BR>the version another transaction writes. Their definitions are as follows:</P>
<P>Read-Depends Transaction Tj directly read-depends (wr) on Ti if Ti writes some version xk<BR>and Tj reads xk.</P>
<P>Anti-Depends Transaction Tj directly anti-depends (rw) on Ti if Ti reads some version xk<BR>and Tj writes x&#8217;s next version after xk in the version order.</P>
<P>Write-Depends Transaction Tj directly write-depends (ww) on Ti if Ti writes some version<BR>xk and Tj writes x&#8217;s next version after xk in the version order.</P>
<P>Using these definitions, from a history H a direct serialization graph DSG(H) is con-<BR>structed. Each node in the DSG corresponds to a committed transaction and edges correspond to the types of direct conflicts between transactions. Anomalies can then be defined by stat- ing properties about the DSG. To illustrate the difference between an Adya history and a classic schedule, s is given with versions accessed by each operation (version order: [x0 &#8810; x1, y0 &#8810; y1, z2 &#8810; z3]). This is visualized in Figure 4.2.<BR>H = w1[x1] r2[x1] r2[y0] w1[y1] w2[z2] w3[z3] r3[x1] c1 c3 c2</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>wr<BR>Fig. 4.2 Direct serialization graph (DSG) representation of H.</P>
<P>The above item-based model can be extended to handle predicate-based operations [1]. Database operations are frequently performed on sets of items provided a certain condition called the predicate, P, holds. When a transaction executes a read or write based on a predicate P, the database selects a version for each item to which P applies, this is called the version set of the predicate-based denoted as Vset(P). A transaction Tj changes the matches of a predicate-based read ri(Pi) if Ti overwrites a version in Vset(Pi).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4.2 Weak Isolation Levels</P>
<P>Using the system model in Subsection 4.4.1, definitions of isolation levels are given via a combination of constraints and the prevention of types of cycles in the DSG. In total 11 isolation levels are presented in [1]. In this chapter we consider a subset: Read Uncommitted, Read Committed, Repeatable Read, and Serializable.</P>
<P>Read Uncommitted Read Uncommitted isolation prevents anomaly Dirty Write (G0), that is, the DSG cannot contain cycles consisting entirely of write-depends edges. HG0 below contains a Dirty Write (G0) anomaly and the corresponding DSG is shown in Figure 4.3. Informally, this means writes to different data items must be ordered consistently across transactions.</P>
<P>HG0 = w1[x1] w2[y2] w1[y1] w1[x1] c1 c2</P>
<P>ww</P>
<P><BR>ww<BR>Fig. 4.3 DSG(HG0) displays a Dirty Write (G0) anomaly.</P>
<P><BR>Read Committed Read Committed isolation prevents G0 and anomalies, (i) Aborted Read (G1a), transactions cannot read data item versions created by aborted transactions (HG1a), (ii) Intermediate Reads (G1b), transactions cannot read intermediate data item versions (HG1b), and (iii) Circular Information Flow (G1c), the DSG cannot contain cycles consisting of write-depends and read-depends edges (HG1c, DSG in Figure 4.4). Informally, this means transactions only read committed information.</P>
<P>HG1a = w1[x1] r2[x1] a1 c2</P>
<P><BR>HG1b = w1[x1] w1[x2] r2[x1] c1 c2</P>
<P><BR>HG1c = w1[x1] r2[x1] w2[y2] r1[y2] c1 c2</P>
<P>wr</P>
<P><BR>wr<BR>Fig. 4.4 DSG(HG1c) displays a Circular Information Flow (G1c) anomaly.</P>
<P>Serializable Serializable isolation prevents anomalies G0, G1, and G2, the DSG cannot contain cycles containing one or more anti-depends edges. In short, the DSG cannot con- tain any cycles. HG2 below contains a G2 anomaly and the corresponding DSG is shown in Figure 4.5.</P>
<P>HG2 = r1[x0] w2[x1] r2[y0] w1[y1] c1 c2</P>
<P>rw</P>
<P><BR>rw<BR>Fig. 4.5 DSG(HG2) displays a G2 anomaly.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4.3 Mixing of Isolation Levels<BR>To define a correctness criteria for a mixed DBMS, a DSG variant is used to represent a mixed history, referred to as a mixed serialization graph, MSG(H). A MSG only includes relevant and obligatory conflicts. A relevant conflict is a conflict that is pertinent to a given isolation level, e.g., read-depends edges are relevant to Read Committed transactions but not Read Uncommitted transactions. An obligatory conflict is a conflict that is relevant to one transaction but not the other, e.g., an item-anti-depends edge between a Read Committed transaction and a Serializable transaction is relevant to the Serializable transaction and not the Read Committed transaction but still must be included in the MSG. Adya defines the edge inclusion rules for an MSG as follows:</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Write-depends edges are relevant to all transactions regardless of isolation level thus always included.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Read-depends edges are relevant for edges incoming to Read Committed, Repeatable Read, or Serializable transactions.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. Item-anti-depends edges are included for outgoing edges from Repeatable Read and<BR>Serializable transactions.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Predicate-anti-depends edges are included for outgoing edges from Serializable trans- actions.<BR>Now in a mixed DBMS, a history is correct if each transaction is provided the isolation guarantees that pertain to its level leading to the mixing-correct theorem [1]. Figure 4.6 illustrates the differences between DSG (Figure 4.2) and MSG representations of a history with the non-relevant and non-obligatory edges removed.<BR>Theorem 2 (Mixing-Correct Theorem) A history H is mixing-correct if MSG(H) is acyclic and phenomena G1a and G1b do not occur for Read Committed, Repeatable Read, and Serializable transactions.</P>
<P>Read Committed</P>
<P>Serializable&nbsp;Read Uncommitted<BR>Fig. 4.6 Mixed serialization graph representation of H.</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5 Mixed Serialization Graph Testing</P>
<P>In this section, we describe mixed serialization graph testing, focusing on the adjustments to the SGT algorithm (sketched in Subsection 4.2.1) and the concurrent graph data structure</P>
<P>described in Subsection 4.2.2. Subsection 4.5.2 discusses several potential performance optimizations.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.1 Protocol Description</P>
<P>For MSGT we use the graph data structure and node-level locking protocol in Subsection 4.2.2 to represent an MSG, which requires one alteration: nodes include transactions&#8217; required isolation levels. MSGT proceeds in the same manner as SGT, with one key exception: for each operation, edge insertion of a detected conflict is subject to MSG&#8217;s edge inclusion rules enumerated in Subsection 4.4.3. MSGT&#8217;s edge insertion algorithm is given in Algorithm 7. First, if an edge already exists for this conflict type no further action is needed, else a cascading abort check is performed. If the parent node has aborted and the inserting node is Serializable or Read Committed it must also abort to avoid G1a anomalies. Then, the edge is inserted iff it satisfies MSG&#8217;s inclusion rules, before a cycle check is executed. If a cycle is found the transaction must abort. At commit time the transaction delays until it has no incoming edges.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.2 Optimizations<BR>Restricted DFS Our first optimization arises from the following observation: the reduced DFS used in the SGT implementation described in Subsection 4.2.2 can result in a transaction aborting due to detection of a cycle it is not involved in. For example, assume Ti has detected a conflict with Tj, thus inserted Tj &#8594; Ti. Now, assume Tj and Tk are involved in a cycle, Tj &#8594; Tk &#8594; Tj. It is possible for the cycle check initiated from Ti to detect this cycle and unnecessarily abort. This can be solved by restricting the DFS to only detect cycles involving the edge which has just been inserted. Informally, this is correct as if adding an edge introduces a cycle into the graph, then it and the two nodes it connects will be members of the cycle. Restricted DFS can benefit both SGT and MSGT.</P>
<P>&nbsp;</P>
<P>Algorithm 7: MSGT Edge Insertion</P>
<P>1 Input: Node&amp; this, Node&amp; from, Conflict cType<BR>2 if (from.id,cType) / this.inSet then<BR>3&nbsp;if cType != RW&nbsp;this.iso == (RC&nbsp;S)&nbsp;from.state == aborted then<BR>4&nbsp;return false // cascading abort<BR>5&nbsp;if cType == ww then<BR>6&nbsp;this.inSet.add(from.id)<BR>7&nbsp;from.outSet.add(this.id)<BR>8&nbsp;else if cType == wr&nbsp;this.iso != RU then<BR>9&nbsp;this.inSet.add(from.id)<BR>10&nbsp;from.outSet.add(this.id)<BR>11&nbsp;else if cType == rw&nbsp;from.iso == S then<BR>12&nbsp;this.inSet.add(from.id)<BR>13&nbsp;from.outSet.add(this.id)<BR>14&nbsp;else<BR>15&nbsp;return true // not relevant/obligatory<BR>16&nbsp;cycle = cycleCheck(thisNode)<BR>17&nbsp;return !cycle<BR>18 else</P>
<P><BR>Relevant DFS The second optimization can be attributed to the observation that reduced DFS is blind to the type of edge being traversed and thus the type of cycle found, which can also contribute to unnecessary aborts. In other words, transactions can unnecessarily abort due to a detecting cycle that is not applicable to its isolation level. For example, the cycle involving T1 and T2 in Figure 4.7(a) is a G2 cycle, which is relevant to T2 the Serializable transaction (Figure 4.7(c)), but not to T1 the Read Committed transaction (Figure 4.7(b)). Without explicit consideration, T1 could detect the cycle, which results in T2 also (cascadingly) aborting due to the wr edge between T1 and T2. To address this we propose the following heuristic: assuming restricted DFS, if Ti has inserted a rw edge between itself and Tj, if Ti is a Read Committed or Read Uncommitted transaction, then if a G2 cycle is detected then Tj is aborted, else Ti aborts. We term this relevant DFS. Informally, this works as if a cycle is detected then it can be broken by aborting either transaction connected by the edge. In</P>
<P>rw&nbsp;rw</P>
<P><BR>PL-2<BR>wr&nbsp;PL-3<BR>PL-2<BR>wr&nbsp;PL-3<BR>PL-2<BR>wr&nbsp;PL-3<BR>&nbsp;&nbsp;&nbsp; (a) MSGT<BR>&nbsp;&nbsp;&nbsp; (b) T1&#8217;s view<BR>&nbsp;&nbsp;&nbsp; (c) T2&#8217;s view</P>
<P>Fig. 4.7 MSGT and individual transaction&#8217;s relevant views.</P>
<P>the case of a G2 cycle, we leverage this to selectively abort the transaction with a higher isolation level where possible to minimize the impact of cascading aborts.<BR>Early Commit Rule The third optimization comes from the observation that a non-relevant incoming edge can delay a transaction from committing. For example, consider Ti (Read Committed) is attempting to commit, but has a single incoming anti-depends edge from<BR>Tj (Serializable), T<BR>rw<BR>j &#8722;&#8594;<BR>Ti. Recall from Section 4.2, delaying commitment until a node<BR>has no incoming edges simplifies node deletion, ensures recoverability, and provides order- preservation. However, in the example, Ti can never lie on a cycle relevant to its isolation level (G0 or G1c cycle.), thus under the early commit rule a transaction can commit without delay with incoming edges, provided these are not relevant to the transaction&#8217;s isolation level, i.e., Ti could commit immediately. Such a scheme should provide lower latency for transactions that require weaker isolation levels via this more flexible commit rule. In the example, Ti would not unnecessarily waste CPU cycles checking its incoming edge set.<BR>Care, however, must be taken regarding node deletion if the early commit rule is used.<BR>To illustrate this, Ti may still be a member in a cycle for a transaction committing at a<BR>higher isolation level, e.g., T<BR>may insert the edge T rw T . Similarly to that described<BR>j&nbsp;i&nbsp; &#8722;&#8594;&nbsp;j<BR>in Subsection 4.2.1, removing the node upon commit would allow the possibility of missed cycles and hence executions not permitted under the mixing correct theorem. Thus, regardless of isolation level a node can only be scheduled for removal once all incoming edges have been removed. A possible implementation is to offload the management of early committed nodes to a background thread that periodically checks if all edges have been removed, at</P>
<P>which point the nodes can be deleted and recycled; this would allow nodes to be accessed, if needed, during cycle checking initiated by other nodes.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.3 Discussion</P>
<P>MSGT has several benefits. Firstly, most DBMSs offer weak isolation levels and thus a considerable portion of real world applications are built atop such guarantees. Normally, weak isolation is an afterthought as DBMSs chase better performance, with MSGT weak isolation catered for as a first-class citizen. Such an approach provides a higher degree of concurrency and hence performance, whilst also providing the optimal property of no unnecessary aborts. The relevant DFS optimization further minimizes aborts and the early commit rule promises lower latency for transactions that require weaker isolation levels.<BR>There are multiple drawbacks that must also be considered. It is worth noting the utility of weak isolation is limited to applications that can tolerate potentially non-serializable behaviour. Additionally, if a workload exhibits low contention or is designed in a manner such that anomalies provably do not occur [44], then the additional overhead of managing the MSG has little benefit. Unfortunately, if the early commit rule is deployed, the real-time commit order may no longer match the serialization order, i.e., order-preservation is violated. However, in the search for high performance this may be a trade-off the database operator is willing to make.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5.4 Implementation Details</P>
<P>MSGT was implemented in our prototype in-memory database (Subsection 2.3.1). The DBMS has a pool of worker threads and each transaction is pinned to a specific worker thread for its duration. Each worker thread has an independent workload generator. Thus when a transaction is committing we repeatedly execute the commit routine (check for incoming edges).</P>
<P>To ensure high operation throughput under a concurrent workload, the necessary data structures use atomic operations where appropriate. The use of totally ordered transaction ids was avoided using the address of the transaction&#8217;s node in the graph as the transaction id to avoid contention updating a global atomic counter. For safe memory reclamation in a concurrent environment epoch-based garbage collection is used [46] and nodes&#8217; edge sets are recycled after a transaction is committed and deleted.<BR>Adya&#8217;s system model is defined in terms of a multiversion model, but as the MSGT scheduler allows transactions to optimistically read dirty records, the possibility of cascading aborts is introduced. Unwinding writes due to cascading aborts lead to unnecessary system load and are not useful for either the user of the system. Therefore, only one uncommitted transaction is allowed to modify a data item. Also, the prototype DBMS does not currently support predicate-based operations, thus a simple read/write model is assumed. Hence, some isolation levels, e.g., Repeatable Read, can not be captured.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6 Evaluation</P>
<P>In this section, we experimentally compare MSGT with SGT focusing on two implementa- tions: a graph-based scheduler (SGT) and a mixed graph-based scheduler (MSGT). To be precise, we focus on answering the following questions:<BR>&nbsp;&nbsp;&nbsp; &#8226; What are the performance benefits of MSGT vs. SGT under a workload with transac- tions executed at different isolation levels?<BR>&nbsp;&nbsp;&nbsp; &#8226; Under what workload conditions does MSGT exhibit better performance than SGT?</P>
<P>&nbsp;&nbsp;&nbsp; &#8226; What is the overhead of MSGT vs. SGT when all transactions are executed at Serializ- able isolation?<BR>Our first set of experiments in Subsections 4.6.1 to 4.6.4 use YCSB (described in Sub- section 2.3.2) as a microbenchmark. This benchmark allows various aspects of an OLTP</P>
<P>workload to be altered to measure how the protocols perform under a variety of workload conditions. Specifically, we tweak the proportion of serializable transactions, vary the pro- portion of update transactions, differ the contention level, and increase the core count to measure scalability. YCSB&#8217;s workload factors are summarized in Table 4.2. The next set of experiments in Subsections 4.6.5 and 4.6.6 model different application scenarios, namely, SmallBank and TATP described in Subsection 2.3.3 and Subsection 2.3.2 respectively. Small- Bank is helpful in addressing our research question of ascertaining the overhead of MSGT compared to SGT, as all SmallBank transactions are executed at Serializable isolation. TATP also provides an ideal candidate for measuring the overhead of MSGT in comparison with SGT. However, TATP permits all transactions to be executed at Read Committed isolation. Thus, it is useful to help determine whether MSGT can still provide performance gains in workloads with a low abort rate.<BR>Table 4.2 YCSB Workload Factors</P>
<P>Symbol<BR>Meaning<BR>Values<BR>&#969;<BR>Contention Level<BR>0.6-0.9<BR>U<BR>Proportion of update transactions<BR>0.0-1.0<BR>&#952;<BR>Proportion of Serializable transactions<BR>0.0-1.0<BR>cores<BR>DBMS core count<BR>1-40</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.1 Isolation<BR>We begin with measuring the impact of increasing the proportion of transactions executing at Serializable isolation (&#969;) from 0% to 100%. This aims to test MSGT&#8217;s ability to leverage its theoretical properties to offer increased performance when transactions are run at weaker isolation levels. For this experiment, we opt for a medium contention level by setting &#952; = 0.8, and the framework is configured to run with 40 cores.<BR>In Figure 4.8(a), SGT&#8217;s throughput is invariant to the proportion of Serializable transac- tions, this is anticipated as it is unable to take advantage of transactions&#8217; declared isolation</P>
<P><BR>MSGT&nbsp;SGT</P>
<P><BR>3.0</P>
<P>2.8</P>
<P>2.6</P>
<P>2.4</P>
<P>2.2</P>
<P>0.00</P>
<P>0.25</P>
<P>0.50</P>
<P>0.75</P>
<P>1.00<BR>% of Serializable Transactions (&#61559;)<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. &#969;.</P>
<P>MSGT&nbsp;SGT<BR>3.5</P>
<P>3.0</P>
<P>2.5</P>
<P>2.0</P>
<P>1.5</P>
<P>1.0</P>
<P>0.00</P>
<P>0.25</P>
<P>0.50</P>
<P>0.75</P>
<P>1.00<BR>% of Serializable Transactions (&#61559;)<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. &#969;</P>
<P>MSGT&nbsp;SGT</P>
<P>0.014</P>
<P><BR>0.012</P>
<P><BR>0.010</P>
<P><BR>0.008</P>
<P>0.00</P>
<P>0.25</P>
<P>0.50</P>
<P>0.75</P>
<P>1.00<BR>% of Serializable Transactions (&#61559;)<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. &#969;.</P>
<P>Fig. 4.8 Isolation &#8211; SGT and MSGT with 40 cores when varying the proportion of Serializable<BR>transactions from 0% to 100% with medium contention &#952; = 0.8 and 50% update rate.</P>
<P>levels, in effect, executing all transactions at Serializable. Meanwhile, the throughput of<BR>MSGT decreases as &#969; is increased, converging towards SGT&#8217;s throughput. When there are no Serializable transactions (&#969; = 0.0), MSGT achieves a 39% increase in throughput. At &#969; = 0.4, this drops to a 21% increase and at &#969; = 0.8 a 4% gain is exhibited. When &#969; = 1.0, SGT marginally outperforms MSGT, this can be attributed to the additional overhead of managing the MSG. This relationship is reflected in the abort rate displayed in Figure 4.8(b), across the range of &#969;, SGT&#8217;s abort rate varies from a 3x increase over MSGT&#8217;s abort rate to an equivalent abort rate when all transactions are executed at Serializable. A higher abort rate degrades the user experience, reduces throughput and, as can be seen in Figure 4.8(c), harms latency.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.2 Update Rate<BR>For the next experiment, we explore the effect of varying the proportion of update transactions (U ). For this experiment, we opt for a medium contention level, &#952; = 0.8, set the proportion of Serializable transactions to &#969; = 0.2, with the framework configured to run with 40 cores. When U = 0.0, the workload is read-only thus no (ww, wr, rw) conflicts are generated.<BR>Thus, no edges are inserted and neither SGT or MSGT are required to perform any work, which explains why performance is indistinguishable across all metrics in Figure 4.9. As U is increased more updates are executed and more conflicts occur, thus both schedulers have work (edge insertion/cycle checks) to perform, which explains the downward trend in throughput in Figure 4.9(a). MSGT&#8217;s throughput decreases at a slower rate as it is able to leverage its selective conflict detection rules, achieving between 11% and 27% higher throughput compared to SGT.</P>
<P><BR>MSGT&nbsp;SGT<BR>5</P>
<P>4</P>
<P>3</P>
<P>2</P>
<P>0.00&nbsp;0.25&nbsp;0.50&nbsp;0.75&nbsp;1.00<BR>% of Update Transactions (U)<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. U .</P>
<P>MSGT&nbsp;SGT</P>
<P>6</P>
<P><BR>4</P>
<P><BR>2</P>
<P><BR>0<BR>0.00</P>
<P>0.25</P>
<P>0.50</P>
<P>0.75</P>
<P>1.00<BR>% of Update Transactions (U)<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. U</P>
<P>MSGT&nbsp;SGT</P>
<P>0.020</P>
<P>&nbsp;</P>
<P>0.010</P>
<P>0.007</P>
<P>0.00&nbsp;0.25&nbsp;0.50&nbsp;0.75&nbsp;1.00<BR>% of Update Transactions (U)<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. U .</P>
<P>Fig. 4.9 Update Rate &#8211; Performance measurements at 40 cores for the protocols as the proportion of update transaction (U ) is varied from 0% to 100% with medium contention, &#952; = 0.8, and a low proportion of Serializable transactions, &#969; = 0.2.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.3 Contention<BR>In the next experiment we measure the effect of increasing contention in the system by varying &#952; from 0.6 to 0.9. In theory, contention increases the chance of conflicts between transactions. This should translate into an increase in the number of edges inserted into the conflict graph. Under SGT all edges are inserted, whereas MSGT utilizes isolation levels to be more selective over edge insertions (only adding relevant or obligatory edges) hence it inserts fewer edges into the conflict graph, and should find fewer cycles (aborts) compared to SGT. We set the proportion of Serializable transactions to &#969; = 0.2. Again the experiment was run with 40 cores.<BR>Figure 4.10(a) displays the throughput of SGT and MSGT as the contention is increased. As &#952; increases the throughput decreases for both protocols. For low levels of contention SGT performs marginally better than MSGT (&lt;1% difference), but under high contention this reverses and MSGT offers a 24% increase in throughput. Figure 4.10(b) shows that after &#952; = 0.7, the abort rate begins increasing for both protocols. At the highest level of contention (&#952; = 0.9), 0.1% of the data is accessed by 35% of the queries, and SGT aborts 17% more transactions than MSGT. Lastly, in Figure 4.10(c), above &#952; = 0.7, MSGT achieves between a 9% and 28% reduction in the average latency.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.4 Scalability</P>
<P>The previous three experiments have investigated the impact of several workload factors in a database with 40 cores. In this experiment we fix the workload factors and vary the core count (1 to 40) to evaluate MSGT&#8217;s scalability compared to SGT. We anticipate that MSGT scales better than SGT as its scheduler generally performs less work (edge insertions and cycle checking). From Figure 4.11(a) it can be seen that until 20 cores the throughput of both protocols is indistinguishable; in fact, up to 10 cores SGT exhibits between a 1.2% and 3.1% increase over MSGT. After this point, a gap appears, at 30 cores MSGT has 13.1% higher</P>
<P><BR>MSGT&nbsp;SGT</P>
<P>&nbsp;</P>
<P>3</P>
<P><BR>2</P>
<P><BR>1<BR>0.6</P>
<P>0.7</P>
<P>0.8</P>
<P>0.9<BR>Skew Factor (&#61553;)<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. &#952; .</P>
<P>MSGT&nbsp;SGT</P>
<P>8</P>
<P>6</P>
<P>4</P>
<P>2</P>
<P>0<BR>0.6</P>
<P>0.7</P>
<P>0.8</P>
<P>0.9<BR>Skew Factor (&#61553;)<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. &#952;</P>
<P>MSGT&nbsp;SGT</P>
<P>0.03</P>
<P><BR>0.02</P>
<P><BR>0.01</P>
<P>0.6&nbsp;0.7&nbsp;0.8&nbsp;0.9<BR>Skew Factor (&#61553;)<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. &#952; .</P>
<P>Fig. 4.10 Contention &#8211; SGT and MSGT with 40 cores when varying the contention (skew factor) in the YCSB workload with &#969; = 0.2, and U = 0.5.</P>
<P>throughput and at 40 cores this difference increases to 27.9%. In Figure 4.11(b) it can be seen the abort rate of the protocols starts to diverge after 10 cores: SGT has an abort rate of 1.65% and 3.39% at 30 and 40 cores respectively, whereas MSGT&#8217;s is 0.50% and 1.54%.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.5 SmallBank</P>
<P>Next we test the protocols using the SmallBank workload. In this workload all transactions are executed at Serializable isolation, we consider the case of unusual high contention by setting the database to contain only 100 customers; transaction requests are uniformly distributed across the 100 customers. This creates a worst case scenario for MSGT, the high contention results in a significant number of edge insertions, and it cannot leverage its selective edge insertion rules due to all transactions requiring the highest isolation level. Therefore, this allows us to quantify the overheads of MSGT compared to SGT.<BR>It is clear from Figure 4.12 that MSGT has a non-negligible overhead, with SGT display- ing improved performance across all metrics. Thus, if a workload requires all transactions to execute at Serializable isolation then SGT is the better choice. In Figure 4.12(a), at 30 cores, SGT has a 6% increase in throughput over MSGT. From Figure 4.12(b), it can be seen that SGT surprisingly has a lower abort rate than MSGT, this can be explained by Figure 4.12(c), which shows the average latency is higher under MSGT. The longer transactions are in the database, the more their chances for conflict increases, which results in more aborted transactions.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.6 TATP</P>
<P>Lastly, we examine the scalability of the protocols using the TATP workload. TATP is inter- esting as its transactions generate few conflict cycles. Thus, aborts are naturally minimized especially for a protocol that accepts all conflict serializable schedules (SGT). However, it also only requires transactions to be executed with Read Committed isolation. This provides</P>
<P><BR>MSGT&nbsp;SGT</P>
<P>&nbsp;</P>
<P>2</P>
<P><BR>1</P>
<P>&nbsp;</P>
<P>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. U .</P>
<P>MSGT&nbsp;SGT</P>
<P><BR>3</P>
<P>2</P>
<P>1</P>
<P>0<BR>0&nbsp;10</P>
<P>20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. U</P>
<P>MSGT&nbsp;SGT</P>
<P><BR>0.010</P>
<P>&nbsp;</P>
<P>0.005</P>
<P><BR>0.003<BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. U .</P>
<P>Fig. 4.11 Scalability &#8211; Performance measurements for the protocols as the core count is increased from 1 to 40 cores with medium contention, &#952; = 0.8, low proportion of Serializable transactions, &#969; = 0.2, and a medium update rate U = 0.5.</P>
<P><BR>MSGT&nbsp;SGT</P>
<P>&nbsp;</P>
<P>6</P>
<P><BR>4</P>
<P><BR>2</P>
<P>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Throughput vs. &#969;.</P>
<P>MSGT&nbsp;SGT</P>
<P>2.0</P>
<P>1.5</P>
<P>1.0</P>
<P>0.5</P>
<P>0.0<BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Abort rate vs. &#969;</P>
<P>MSGT&nbsp;SGT</P>
<P>0.005</P>
<P>0.004</P>
<P>0.003</P>
<P>0.002</P>
<P>0.001<BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) Average latency vs. &#969;.</P>
<P>Fig. 4.12 SmallBank &#8211; 100 customers (high contention) with all transactions executed at<BR>Serializable isolation.</P>
<P>a situation to evaluate whether MSGT can still provide a performance gain in workload with low abort rates, i.e., when cycles are not common.<BR>Figure 4.13 demonstrates in realistic workloads with a low conflict rate, MSGT can outperform SGT across all metrics, provided transactions are run at weaker isolation levels. In Figure 4.13(a), at 40 cores MSGT achieves at 10% increase in throughput, this is matched with a significant decrease in the abort rate and latency (Figures 4.13(b) and 4.13(c)).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.7 Concurrency Control Overhead<BR>To separate the concurrency control overhead from the actual transaction workload, we turned off the tuple access history, conflict detection, cycle checking, and restarted handling for both protocols. We re-ran the TATP and SmallBank experiments, and ran YCSB with high contention (&#952; = 0.9), balanced read/update transaction mix (U = 0.5), and half of the transactions running at Serializable isolation (&#969; = 0.5). All workloads were run with 40 cores. Note, the only concurrency control requirement for &#8220;No CC&#8221; is that tuple accesses are serialized.<BR>Table 4.3 displays the results. In all experiments, both protocols spend the majority of time for concurrency control: tuple access history, conflict detection, cycle checking, and restart handling. SGT has marginally lower overhead in the SmallBank experiment, which contains all Serializable thus MSGT wastes cycles unnecessarily checking if it can avoid inserting an edge. In TATP and YCSB, MSGT exhibits a 1.3% and 4.9% less overhead compared to SGT. In these workloads, MSGT is able to insert fewer edges, which results in few cycle checking invocations and a smaller graph to explore when cycle checking is required.</P>
<P>&nbsp;</P>
<P>MSGT&nbsp;SGT<BR>16</P>
<P>12</P>
<P><BR>8</P>
<P><BR>4</P>
<P><BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. &#969;.</P>
<P>MSGT&nbsp;SGT</P>
<P>1.2</P>
<P><BR>0.8</P>
<P><BR>0.4</P>
<P><BR>0.0<BR>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. &#969;</P>
<P>MSGT&nbsp;SGT<BR>0.0030</P>
<P>0.0025</P>
<P>0.0020</P>
<P>0.0015</P>
<P>0.0010</P>
<P>0&nbsp;10&nbsp;20&nbsp;30&nbsp;40<BR>cores<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. &#969;.</P>
<P>Fig. 4.13 TATP &#8211; 100 entries all transactions executed at Read Committed isolation.</P>
<P>Table 4.3 Overhead of maintaining accesses, conflict detection, cycle tests, aborts, and live-lock handling. Reported metric is throughput at 40 cores.</P>
<P><BR>Benchmark<BR>SmallBank<BR>TATP<BR>YCSB<BR>No CC<BR>26.7M<BR>36.9M<BR>3.88M<BR>SGT<BR>7.57M<BR>13.4M<BR>0.67M<BR>MSGT<BR>7.41M<BR>15.2M<BR>0.72M<BR>SGT Overhead<BR>71.7%<BR>63.7%<BR>82.7%<BR>MSGT Overhead<BR>72.3%<BR>58.8%<BR>81.4%</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6.8 Optimizations</P>
<P>In this section, we explore the performance implications of the optimizations described in Subsection 4.5.2.<BR>Cycle Checking Strategies In this experiment, we compare the performance of reduced, restricted, and relevant DFS cycle checking strategies for MSGT. We repeat the isolation experiment from Subsection 4.6.1, varying the proportion of transactions executing at Se- rializable isolation from 0% to 100%; medium contention, &#952; = 0.8, balanced read/update transaction ratio, U = 0.5, and 40 cores.<BR>Interestingly, as seen in Figure 4.14(b) reduced DFS has a lower abort rate than restricted DFS. One explanation is under restricted DFS cycle checking takes longer, as any discovered cycles must include the inserted edge, increasing the transaction lifetime and hence resulting in more conflicts and cycles. This hypothesis is supported by Figure 4.14(c) in which MSGT with restricted DFS has the highest average latency. In Figure 4.14(b), relevant DFS displays the lowest abort rate, between a 6% and 16% decrease compared to reduced DFS. Surprisingly, this does not translate into higher throughput or lower latency in Figures 4.14(a) and 4.14(c). One explanation for this it that transactions with lower isolation are still delayed from committing as they have incoming edges from higher isolation transactions that will</P>
<P>subsequently abort. Potentially, this can be solved with the early commit rule, which is now given preliminary investigation.<BR>Early Commit Rule To investigate the potential usefulness of the early commit rule, the above experiment was repeated, however now when a Read Uncommitted or Read Committed transaction was committing we checked whether it could have committed early, computing the proportion of transactions that would have benefitted from the rule, ce. When &#969; = 0.0, we found ce = 0%, as there were no Serializable transactions to introduce rw edges into the MSG. At &#969; = 1.0, we also found ce = 0%, as there are no weaker isolation transactions to leverage the rule. In between the extremes, we found that up to 2.5% of weaker isolation transactions were prevented from committing early due to non-relevant edges, when &#969; = 0.8. Such a non-negligible proportion surely would impact performance, thus these provisional findings motivate a full implementation and evaluation of the early commit rule.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.7 Conclusion</P>
<P>In this chapter we presented mixed serialization graph testing, a graph-based scheduler that leverages Adya&#8217;s mixing-correct theorem to permit transactions to execute at different isolation levels. When workloads contain transactions running at weaker isolation levels, MSGT is able to outperform serializable graph-based concurrency control by up to 28%. Additionally, MSGT scales as the number of cores is increased, an important property given modern hardware. Like SGT, MSGT minimizes the number of aborted transactions, accepting all useful schedules under the mixing-correct theorem, which greatly improves user experience. Additionally, we presented several optimizations: restricted DFS, relevant DFS, and early commit that aim to further reduce aborts and decreases latency respectively. In summary, this work in this chapter strengthens recent work refuting the assumption that graph-based concurrency control is impractical.</P>
<P><BR>reduced&nbsp;relevant&nbsp;restricted<BR>3.2</P>
<P>3.0</P>
<P>2.8</P>
<P>2.6</P>
<P>2.4</P>
<P>2.2</P>
<P>0.00</P>
<P>0.25</P>
<P>0.50</P>
<P>0.75</P>
<P>1.00<BR>% of Serializable Transactions (&#61559;)<BR>&nbsp;&nbsp;&nbsp; (a) Throughput vs. &#969;.</P>
<P>reduced&nbsp;relevant&nbsp;restricted</P>
<P>&nbsp;</P>
<P>3</P>
<P><BR>2</P>
<P><BR>1<BR>0.00</P>
<P>0.25</P>
<P>0.50</P>
<P>0.75</P>
<P>1.00<BR>% of Serializable Transactions (&#61559;)<BR>&nbsp;&nbsp;&nbsp; (b) Abort rate vs. &#969;</P>
<P>reduced&nbsp;relevant&nbsp;restricted</P>
<P>0.014</P>
<P><BR>0.012</P>
<P><BR>0.010</P>
<P><BR>0.00&nbsp;0.25&nbsp;0.50&nbsp;0.75&nbsp;1.00<BR>% of Serializable Transactions (&#61559;)<BR>&nbsp;&nbsp;&nbsp; (c) Average latency vs. &#969;.</P>
<P>Fig. 4.14 MSGT with various cycle checking strategies when varying the proportion of Serializable transactions from 0% to 100% with medium contention &#952; = 0.8, 50% update rate, and 40 cores.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.7.1 Further Work</P>
<P>Additional Isolation Levels As demonstrated in Section 4.3, databases offer a wide range of isolation levels and Adya provides a definition of several (11 in total [1]) which could possibly be expressed by the MSGT protocol. Of notable interest would be the popular Snapshot Isolation [11].<BR>The simplest extension would be to incorporate Adya SI [1] . To model this isolation<BR>level, Adya extends the system model to include a transaction&#8217;s logical start and commit timestamps. This graph is referred to as a start-ordered serialization graph, SSG(H), and adds start-dependency edges between the nodes, i.e., two transactions Ti and Tj are start- ordered if the commit timestamp of one precedes the start timestamp of the other. The simplest implementation would use a centralized timestamp allocator to add the start and end timestamp nodes. This however reintroduces a global bottleneck which could negatively impact performance.</P>
<P>Additional Experiments As part of future work we wish to extend our performance evaluation to include industry standard benchmarks, e.g., TPC-C [109], and analyze their isolation requirements to help understand where various transaction types occur in practice. Another avenue for further work would be to evaluate MSGT&#8217;s performance against other mixed concurrency control protocols such as mixed 2PL. This will allow for validation of the claim that MSGT accepts all valid schedules under the mixing-correct theorem and hence reduces aborts.</P>
<P>Distributed MSGT Another avenue for future work is exploring how MSGT can be integrated into a distributed shared-nothing database. Specifically, how isolation levels, e.g., Read Committed, can be highly available [8] in the presence of concurrent transactions executing at isolation levels that are provably unavailable, e.g., Serializable.</P>
<P>Mixed Wait-Hit Protocol Also as part of future work we will explore how the Wait-Hit Protocol can be extended to mixed environments. As in MSGT the inclusion of a given edge is determined when the associated conflict is detected this should also be feasible in the Wait-Hit Protocol with certain edges now not included in either the predecessor-upon-read or predecessor-upon-write sets.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Chapter 5</P>
<P>Edge Consistency in Distributed Graph Databases</P>
<P>Summary<BR>This chapter discusses the design and evaluation of protocols for ensuring the preserva- tion of edge consistency in distributed graph databases. Three protocols are presented: Delta protocol, Deterministic Reciprocal Consistency Protocol, and Deterministic Edge Consistency Protocol. The first two focus on preserving Reciprocal Consistency and leverage the fact that the updating of end pointers of a distributed edge must immediately follow each other and the small interval between them is the sole raison d&#8217;&#234;tre for Reciprocal Consistency violations.<BR>The Delta protocol is a lightweight, locking-free protocol, which depending on the choice of the protocol&#8217;s parameter, &#8710;, still permits inconsistencies. Evaluations establish that the protocol can offer both integrity guarantees and sound performance when the value of its parameter is chosen appropriately.<BR>The Deterministic Reciprocal Consistency Protocol ensures no possibility of Reciprocal Consistency violations. The protocol associates metadata with each update of an edge pointer, which is tracked and used to identify violations, aborting the relevant transaction if so. Corruption is thus prevented, at the expense of some aborted transactions.<BR>The Deterministic Edge Consistency Protocol extends the Deterministic Reciprocal Consistency Protocol to provide an additional guarantee: Edge-Order Consistency. This protocol is a simplified version of the Wait-Hit Protocol. Performance evaluations investigate the magnitude of aborts under various system configurations.</P>
<P>5.1&nbsp;Introduction</P>
<P>Section 2.2 introduces graph databases, specifically two important edge consistency guaran- tees in these systems: Reciprocal Consistency and Edge-Order Consistency. Further in Sub- section 2.2.3, we illustrated how Reciprocal Consistency can be violated in a distributed graph database built atop a NoSQL database. Importantly, the resulting data corruption can swiftly<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Introduction&nbsp;117</P>
<P>propagate throughout the database rendering it in a state with little operational value. This motivates the development of protocols that prevent said violations, and as a consequence, slow or avert this corruption from spreading. To this end, two protocols have been developed. The Delta protocol is presented in Section 5.2 and is exclusively designed for one purpose only: Reciprocal Consistency in distributed edges. The protocol assumes that the interval that elapses between a transaction updating both pointers of a distributed edge can be estimated. Thus, by selecting the parameter &#8710; to be larger than this value, it can be assured when a transaction is updating an edge pointer, that both updates of previous updates to that distributed edge have completed. If a preceding update within &#8710; is encountered the updating transaction aborts (although in some cases this may be unnecessary). Therefore, none of the conflicts arising from concurrent writes outlined in Figure 2.4 will occur and semantic corruption will not spread; a correctness reasoning is provided in Subsection 5.2.2. However, there is a chance that the selected value for &#8710; is an underestimate and violations of Reciprocal Consistency still occur. To measure the efficacy of the Delta protocol the model developed in [41, 119] for measuring the degradation of distributed graph databases built atop NoSQL databases is augmented. This requires an adjustment of the conflict probability, qi, which is derived in Subsection 5.2.3. The evaluation uses two metrics that measure the protocol&#8217;s ability to avoid inconsistencies (ensuring safety) and unnecessary aborts (enhancing throughput); Subsection 5.2.3 explains the strategies used for evaluating these metrics. The evaluations in Subsection 5.2.4 establish that the protocol can offer both integrity guarantees and sound performance when the value of its parameter is chosen<BR>appropriately.<BR>The Deterministic Reciprocal Consistency Protocol is then presented in Section 5.3, the key distinction between itself and the Delta protocol is that Reciprocal Consistency is preserved under all eventualities. This is achieved using a collision detection mechanism</P>
<P>which leverages metadata associated with each update of an edge pointer to identify violations. When violations are detected the offending transactions are aborted.<BR>We then extend Deterministic Reciprocal Consistency Protocol in Section 5.4 to pre- vent violations of Edge-Order Consistency, a different type of conflict that can arise when transactions update multiple edges during their lifetime. The Deterministic Edge Consis- tency Protocol combines collision detection with a mechanism called order arbitration, to enforce Reciprocal Consistency for distributed edge and Edge-Order Consistency between transactions respectively. Collision detection is applied at every update and may trigger an immediate abort. Transactions that survive collision detection may, if necessary, go to order arbitration. The latter may also result in an abort. To investigate the performance of the Deterministic Edge Consistency Protocol protocol, we develop an approximate model that allows for the computation of performance measures, including the fraction of aborted transactions and average transaction response time. The accuracy of the approximations is assessed by comparing them with simulations, for a variety of parameter settings.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Delta Protocol</P>
<P>As stated in Subsection 2.2.3, a violation of Reciprocal Consistency is a manifestation of a Dirty Write in the context of a distributed graph database. A straightforward solution to preventing dirty writes is for transactions to take long duration write locks [11], releasing them only after the acquiring transaction has committed or aborted. To prevent deadlock, a policy such as NO-WAIT deadlock avoidance is used, which was shown to be the optimal policy in a distributed, partitioned database [54].<BR>The Delta protocol employs principles behind all these well-tested strategies but has two crucial differences: (i) no locks are used, and (ii) a write operation need not wait until the preceding write commits, but can proceed if at least &#8710; duration (measured by local clock) has elapsed. These differences lead to several advantages in the context of graph databases</P>
<P>which are characterized by the following two aspects. First, there is usually a subset of edges that are traversed and modified with a very high frequency, e.g., critical sections of motorway in a road network, leading to high contention. Secondly, graph transactions tend to be longer-lived than those in other databases. So, having to wait for earlier writes to terminate while transactions are long lived will severely limit the scope for concurrent processing and reduce throughput in a highly contentious environment.<BR>With these concerns in mind the Delta protocol was developed as a protocol that neither uses locks nor requires transactions to wait until earlier transactions terminate.1 The Delta protocol aims at preventing edges from becoming half-corrupted and hence quashing the seed of corruption whilst keeping performance at an acceptable level.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.1 Protocol Description<BR>Recall from Subsection 2.2.3, each end of a distributed edge maintains start sequence number (ssn) to indicate the number of transactions that modified the edge starting from its end. It also maintains a finish sequence number ( f sn) to indicate the number of transactions that modified the edge starting from the other end.<BR>The Delta protocol has five rules:</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. A transaction Ti&#8217;s write on an end pointer of an edge is initially tentative which would become permanent only if that transaction is permitted to commit.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. A tentative write at the final end is possible only if the end pointer is either in a committed state or the immediately preceding tentative write was done at least &#8710; time before (where time is measured as per local clock) and f sn = ssni &#8722; 1 at the server, where ssni is the sequence number Ti obtained when it started modifying the distributed edge.<BR>1The Delta protocol is a concurrency control mechanism only for distributed edge updates; it does not concern itself with updates on vertices.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. If Ti performs all its tentative writes, then it is permitted to commit; otherwise, it must abort.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. A transaction commits when all its tentative writes are made permanent, e.g., by using an atomic commit protocol.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5. Tentative writes of an aborting transaction are ignored. An ignored tentative write can make a new transaction abort unnecessarily for up to &#8710; time after it was created; it is harmless thereafter and can be garbage collected at any time.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.2 Correctness Reasoning</P>
<P>Let us define &#948; as the bound estimate on the interval that may elapse between a transaction completing its update at one end of a distributed edge at one server and starting its update at the other end of the same edge at another server. To aid readability Figures 2.4(a) and 2.4(b) are reproduced in Figure 5.1.<BR>Si&nbsp;S j<BR>&nbsp;&nbsp;&nbsp; (a) (b)</P>
<P>Fig. 5.1 Interleavings of concurrent writes to a distributed edge by transactions Tx and Ty.</P>
<P>Let &#8710; be chosen such that &#8710; &gt; &#948; . Now consider the interleaving in Figure 2.4(b) and let tx be the (global) time when Tx starts at server Si; similarly ty be the time Ty starts at server S j. Since Tx starts after Ty in Figure 2.4(b), tx &gt; ty, i.e. (tx &#8722; ty) &gt; 0.<BR>Say Ty reaches Si at time ty + dy, where dy is the actual time elapsed between Ty com- pleting a tentative write at one end and starting at another end, let us assume that dy &#8804; &#948; .</P>
<P>When Ty arrives at Si it will find a tentative write already done at time tx. In this case, ty + dy &#8722;tx = dy &#8722;(tx &#8722;ty) &lt; dy &#8804; &#948; &lt; &#8710;; so, Ty will abort, preventing writes from interleaving and half-corrupting the edge. Similar arguments can be made for the scenario in Figure 2.4(a),<BR>if dx &#8804; &#948; where dx is the actual time elapsed between Tx completing a tentative write at one end and starting at another end. Note that ty &gt; tx and Tx will abort because it will find out, on reaching server S j for its next tentative write, that tx + dx &#8722; ty = dx &#8722; (ty &#8722; tx) &lt; dx &#8804; &#948; &lt; &#8710;.<BR>Assume that dx &gt; &#948; or dy &gt; &#948; is possible, i.e., the estimate &#948; can fail to hold. In<BR>Figure 2.4(b), interleaving writes are avoided only if ty + dy &#8722; tx = dy &#8722; (tx &#8722; ty) &lt; dy &lt;<BR>&#8710;; otherwise, Reciprocal Consistency can be violated. Similarly, interleaving writes of Figure 2.4(a) are avoided only if tx + dx &#8722; ty = dx &#8722; (ty &#8722; tx) &lt; dx &lt; &#8710;; otherwise, Reciprocal Consistency can be violated. Thus, the probability of reciprocal consistency not being<BR>guaranteed is the probability that &#8710; &#8804; max {dx, dy}.<BR>In summary, the Delta protocol eliminates interleaving of transactions during edge writes, so long as &#8710; remains larger than the interval d that elapses between a transaction completing its write at one end of a distributed edge and starting at the other end. Since the exact value of d taken by a transaction cannot be known in advance, its bound &#948; is estimated with the best effort and &#8710; is chosen to be &#8710; &gt; &#948; .<BR>The larger the value of &#8710; used, the more likely it is that &#8710; &gt; d holds and half-corruption and thereby operational corruption are averted; also, on the downside, the more likely it is that non-interleaving transactions will find their tentative writes within &#8710; time of each other and the later ones choose to abort unnecessarily. In the extreme case, choosing a very<BR>large &#8710; (&#8710; &#8776; &#8734;) totally eliminates any risk of Reciprocal Consistency violation but does<BR>not allow any tentative write until the preceding one is made permanent. It is equivalent to enforcing NO-WAIT policy, wherein a requesting transaction that finds the requested record being locked, must abort.</P>
<P>Our performance evaluation in Subsection 5.2.3 will therefore measure the following two metrics for various values of &#8710;:<BR>&nbsp;&nbsp;&nbsp; &#8226; Time to operational corruption: time taken for &#947;% of a large database to be corrupted,</P>
<P>&nbsp;&nbsp;&nbsp; &#8226; Abort rate: number of transactions aborted per second.<BR>If the actual time between completing at one end and starting at the other, d, is exponentially distributed with mean 1/&#181;, the probability of d exceeding &#8710; is e(&#8722;&#181;&#8710;). The values of &#8710; chosen will explore a range of probabilities of &#8710; being exceeded.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.3 Performance Evaluation Strategies</P>
<P>To assess the time to operational corruption, the model developed in [41] is adapted for the Delta protocol. For ease of the reader, the model is explained here before we discuss the adaptation. We note that an empirical evaluation of the time to operational corruption using a real system would be impractical due to the sheer length of time and the cost it would take to run the experiment. (For certain values of &#8710;, the model predicts anywhere between 1-75 years for operational corruption!)</P>
<P>Degradation Model As stated in Subsection 2.2.3, when an edge is updated, both reciprocal entries in the adjacency lists of the source and the target nodes must be updated. From a modeling perspective, an update to a local edge is assumed to be instantaneous. For a distributed edge, a write operation is carried out first on one of its servers and then, after a small but non-zero delay, d, on the other. This implementation of distributed writes makes it possible for edge records to become half-corrupted. The interleavings illustrated in Figure 2.4 will be referred to as conflicts.<BR>One end of a half-corrupted edge may be considered correct, but an external observer cannot tell which is which. The question of exactly which half is corrupt is decided by what</P>
<P>happens subsequently. Suppose that a future transaction first reads one edge pointer of a half-corrupted edge say, at vertex a. At that moment, the transaction implicitly chooses the order and thereby invalidates the other order that prevails at vertex b. Thus, from that moment onward, the b end of an edge ab becomes the corrupt end and, conversely, the a end becomes the correct end.<BR>A subsequent transaction which happens to read the correct entry of a half-corrupted edge, and completes a write operation for it without a conflict, will repair the fault and make the edge record clean again. Else, if it reads the incorrect entry and writes any edge, it causes the target to become semantically corrupted, or simply corrupted. The correct and incorrect reads are equally likely, so each occurs with probability 1/2.<BR>Any edge can become corrupted by being written on the basis of reading incorrect information. Corrupted edges cannot be repaired, since there is no post-facto solution to the graph repair problem in the general case. Transactions that update edges arrive in a Poisson stream with rate &#955; per second (TPS). We assume that each transaction contains a random number of read operations, K, followed by one write operation. This is a conservative assumption, since more than one write per transaction would increase the rate of corruption.<BR>The variable K can have an arbitrary distribution, P(K = k) = rk, k &#8805; 0. In practice K tends<BR>to be at least 2, i.e., r0 = r1 = 0. The K edges read, and the one written by the transaction are assumed to be independent of each other (but note below that they are not equally likely).<BR>The edges in the database are divided into T types, numbered 1, 2, ..., T . The probability that a read or a write operation accesses an edge of type i is pi, with p1 &gt; p2 &gt; ... &gt; pT . That is, type 1 is most popular, type 2 is the second most popular and so on. The number of edges of type i is Ni, and typically N1 &lt; N2 &lt; ... &lt; NT . The total number of edges is N. In every type, a fraction f of the edges are distributed and the rest are local. The probability of accessing a particular edge of type i, for either reading or writing, is pi . At time 0, all edges are clean (free from corruption). When a certain fraction, &#947; (e.g., &#947; = 0.1), of all</P>
<P>edges become corrupted, the database itself is said to be corrupted for practical purposes, i.e., operationally corrupt. The object of the model is to provide an accurate estimate of the length of time that it takes for this to happen. In summary, at any moment in time, an edge belongs to one of the following four categories: category 0: local and clean, category 1: distributed and clean, category 2: half-corrupted, and category 3: semantically corrupted. Figure 5.2 visualizes the various edge states and possible transitions between them.<BR>a0,3</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>a2,1&nbsp;a2,3</P>
<P>Fig. 5.2 Edge transitions between clean, half-corrupted and semantically corrupt states.</P>
<P>Only distributed edges can be in category 2, but any edge, including local ones, can be in category 3. Denote by ni, j(t) the number of type i edges that are in category j at time t. The set of vectors ni = [ni,0(t), ni,1(t), ni,2(t), ni,3(t)], for i = 1, 2, ..., T , defines the state of the database at time t. At all times, the elements of vector ni add up to Ni. Any state such that</P>
<P>T<BR>&#8721; ni,3(t)&nbsp;&#947;N&nbsp;(5.1)<BR>i=1</P>
<P>will be referred to as an absorbing state. The absorbing states correspond to an operationally corrupt database. The value of interest is U , the average first passage time from the initial<BR>state where ni = [(1 &#8722; f )Ni, f Ni, 0, 0] (i.e., a clean database), to an absorbing state. The above<BR>assumptions and definitions imply that a read operation performed at time t would return a correct answer with probability &#945;, given by</P>
<P>&#945;&nbsp;T&nbsp;&nbsp; pi&nbsp;1<BR>= &#8721;&nbsp;[ni,0(t) + ni,1(t) +&nbsp;ni,2(t)]&nbsp;(5.2)<BR>i=1</P>
<P>Note that the last term accounts for the read operation choosing the correct end of a half-corrupted, category 2 edge which happens with probability 1/2. The probability, &#946; , that all the read operations in a transaction arriving at time t return correct answers, is equal to</P>
<P>&#8734;<BR>&#946; = &#8721; rk&#945;k&nbsp;(5.3)<BR>k=1<BR>Suppose that the distribution of K is geometric with parameter r, starting at k = 2; in other words, rk = (1 &#8722; r)k&#8722;2r. Then the above expression becomes</P>
<P>&#945;2r<BR>&#946; = 1 &#8722; &#945;(1 &#8722; r)&nbsp;(5.4)<BR>Consider now the probability, qi, that a transaction accessing an edge of type i arriving at time t and taking a time d to complete a write operation, will be involved in a conflict. That is the probability that another transaction accessing an edge of type i arrives between t and t + d and writes the same edge, but starting at its other end. This can be expressed as</P>
<P>q = 1 &#8722; e&#8722; 1 &#955; pid/Ni ; i = 1, 2, ..., T&nbsp;(5.5)</P>
<P>If the time to complete a distributed write is not constant, but is distributed exponentially with mean d, then the conflict probability would be</P>
<P>qi = &#8722;&nbsp;&#955; pid&nbsp;; i = 1, 2, ..., T&nbsp;(5.6)<BR>When d is small, there is very little difference between these two expressions. An incoming transaction that is involved in a conflict would change the category of a dis- tributed edge from 1 to 2, provided that all read operations of both queries return correct results. Hence, the instantaneous transition rate, ai,1,2, from state [ni,0, ni,1, ni,2, ni,3] to state [ni,0, ni,1 &#8722; 1, ni,2 + 1, ni,3] can be written as</P>
<P>&nbsp;</P>
<P>ai,1,2<BR>= &#955; pini,1 qi&#946; 2&nbsp;(5.7)<BR>Ni</P>
<P>Conversely, an incoming transaction writing a category 2 edge can change it to a category 1 edge, provided that all its read operations return correct results and it is not involved in a conflict. Hence, the instantaneous transition rate, ai,2,1, from state [ni,0, ni,1, ni,2, ni,3] to state<BR>[ni,0, ni,1 + 1, ni,2 &#8722; 1, ni,3(t)], is given by</P>
<P>ai 2 1 = &#955; pini,2 (1 &#8722; qi)&#946;&nbsp;(5.8)<BR>, ,&nbsp;Ni</P>
<P>The other possible transitions convert an edge of category 0, 1 or 2 into an edge of category 3. This happens when a transaction writes after receiving an incorrect answer to at least one of its reads. Denoting the corresponding instantaneous transition rates by ai, j,3, for j = 0, 1, 2, we have</P>
<P>ai j 3 = &#955; pini, j (1 &#8722; &#946; )&nbsp;(5.9)<BR>, ,&nbsp;Ni</P>
<P>Using these transition rates, one can simulate the process of corrupting the database and obtain both point estimates and confidence intervals for the average time to operational corruption, U&#947; .</P>
<P>Delta Protocol Adaptation.&nbsp;The Delta protocol reduces the conflict probabilities qi, 1 &#8804;<BR>i &#8804; T and thus we now compute qnew. The new probabilities qnew are used in the model<BR>and simulations of [41] to estimate U&#947; . The arrival time, a, of a conflicting transaction, T , is assumed exponentially distributed with rate &#961;. Where &#961; = &#955; pi is the probability a given operation accesses the incorrect record of a half-corrupted edge of type i. Recall, the time<BR>interval, d, that elapses between transaction T completing its tentative write at one end of a distributed edge and starting at another end, is assumed exponentially distributed with rate &#181;.</P>
<P>Consider the interleaving in Figure 2.4(a) and assume Tx arrives at Si at time 0. Then, Tx arrives at S j after dx. Assume Ty arrives at S j at some time a. Then, Ty arrives at Si at time a + dy. Half-corruption occurs under the following conditions:<BR>&nbsp;&nbsp;&nbsp; (i) At S j, dx &gt; a + &#8710;</P>
<P>&nbsp;&nbsp;&nbsp; (ii) At Si, a + dy &gt; &#8710;</P>
<P>The conflict probability, qnew, for edge type i is given by,</P>
<P><BR>qnew = P [(dx &gt; a + &#8710;) &#8745; (dy &gt; &#8710; &#8722; a)]<BR>&#8747; &#8710; &#955; pi e&#8722; &#955; pi ae&#8722;&#181;(&#8710;+a)e&#8722;&#181;(&#8710;&#8722;a)da&nbsp;&#8747; &#8734; &#955; pi e&#8722; &#955; pi ae&#8722;&#181;(&#8710;+a)da<BR>e&#8722;2a&#181;<BR>&nbsp; &nbsp;&#181;&nbsp;! e&#8722;( &#955; pi +2&#181;)a&nbsp;(5.10)</P>
<P>=&nbsp;&#8722;&nbsp;&#955; pi<BR>2ni<BR>2ni</P>
<P>From the perspective of computing qnew, Figure 2.4(b) is equivalent to Figure 2.4(a) and the above expression holds. Then, qnew for each edge type can be used with the transition rates to simulate the process of corrupting the database under the Delta protocol, obtaining estimates of the average time to operational corruption, U&#947; .</P>
<P>Number of aborts per second. To evaluate this metric for various values of &#8710;, a second simulation that focuses specifically on the subset of most frequently accessed distributed edges was performed.<BR>Note that both metrics that we set out to evaluate will be influenced by several parameters that characterize the database and other aspects:<BR>&nbsp;&nbsp;&nbsp; &#8226; Database Size. Size is expressed by the total number of edges N, and the fraction f of distributed edges.</P>
<P>&nbsp;&nbsp;&nbsp; &#8226; Workload. Measured as transactions per second (TPS). Significant for measuring U&#947; are: the fraction of this load that writes after reads and the number of reads that precede a write.<BR>&nbsp;&nbsp;&nbsp; &#8226; Distributed Write Delays and Choosing &#8710;. The smaller the delays the less likely the bound &#8710; is violated. Conversely, smaller &#8710; is the more likely the bound &#8710; is violated. We describe how to choose &#8710; below.</P>
<P>Choosing &#8710;.&nbsp;To choose to bound &#8710; consider the probability of the message delay exceeding<BR>&#8710;:</P>
<P><BR>P [M &gt; &#8710;] = 1 &#8722; e&#8722;&#948;&#8710;<BR>1 &#8722; e&#8722;&#948;&#8710; = 1 &#8722; &#949;<BR>e&#8722;&#948; &#8710; = &#949;<BR>&#8710; = &#8722;ln(&#949;)</P>
<P>&nbsp;</P>
<P><BR>(5.11)</P>
<P><BR>Substituting &#8710; into the above equation yields the conflict probability for a given &#949;.<BR>For example, &#949; = 0.001 gives a 0.1% probability the message delay exceeds &#8710;, for this<BR>&#8710; = 0.035s,</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2.4 Evaluation<BR>The following parameter choices are inline with industry experiences. The graph ana- lyzed consisted of seven edge types, n1 = 104, n2 = 105, n3 = 106, n4 = 107, n5 = 108, n6 = 109, n7 = 1010, totaling 11 billion edges, with access probabilities p1 = 0.5, p2 = 0.25, p3 = 0.13, p4 = 0.06, p5 = 0.03, p6 = 0.02 and p7 = 0.01; a graph of this size would have approx- imately 1 billion vertices. The number of read operations before a write per transaction is<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Deterministic Reciprocal Consistency Protocol&nbsp;129</P>
<P>geometrically distributed starting at 2, with an average of 15. In all edge types, f = 0.3 are<BR>distributed, the remainder are local, in proportion with a good graph partitioning algorithm.<BR>The delay d between a transaction completing a tentative write at one end and starting at another end is exponentially distributed with a mean of 5ms. The database is initially clean and considered to be operationally corrupted when 10% (&#947; = 0.1) of all edges are semantically corrupted. The time taken until operational corruption, U , is measured in days. We consider a range of transaction arrival rates, &#955; = (1000,..., 10000); a typical graph workload comprises of 90% read-only transactions and 10% read-write transactions [4], hence the chosen range reflects a total workload (10000,..., 100000). The following &#8710; values were considered &#8710; = 50, 75, 100ms. For each &#8710; the probability that d exceeds &#8710; is P(d &gt; &#8710;) = 4.5&#8722;5, 3.1 &#215; 10&#8722;7, 2.1 &#215; 10&#8722;9 respectively.<BR>The results for measuring the impact of &#8710; on the time until operational corruption are given in Figure 5.3 (where the y-axis is in log scale). With no concurrency control (U =NA), U ranges between 50-500 days. For &#8710; = 50ms, U increases to 1-75 years. For &#8710; = 75, 100ms the time to corruption is significantly large as shown in Figure 5.3.<BR>To evaluate the number of aborts that occurred per second (&#945;), simulations were run for 10 seconds for each arrival rate, &#955; = (1000,..., 10000). Figure 5.4 reports the fraction &#945; for various values of &#955; . This fraction is also the probability that an incoming transaction is aborted due to the Delta protocol. For &#8710; = 50 ms, the abort probability is between 1 &#8722; 5%, this increases to between 1 &#8722; 7% for &#8710; = 75 ms and 1 &#8722; 9% for &#8710; = 100 ms.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Deterministic Reciprocal Consistency Protocol</P>
<P>The Deterministic Reciprocal Consistency Protocol preserves Reciprocal Consistency in all eventualities, in other words, there is zero probability for the violation of Reciprocal Consistency. This is achieved through a mechanism, referred to as collision detection, that enforces Reciprocal Consistency for distributed updates.</P>
<P>&nbsp;</P>
<P>30</P>
<P>&nbsp;</P>
<P>25</P>
<P><BR>20&nbsp;&#61508; (ms):<BR>NA 50<BR>15&nbsp;75<BR>100</P>
<P>10</P>
<P>0.09</P>
<P>0.08</P>
<P>0.07</P>
<P>0.06</P>
<P>0.05</P>
<P>0.04</P>
<P>0.03</P>
<P>0.02</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>&#61508; (ms):<BR>50<BR>75<BR>100</P>
<P>5</P>
<P>0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1000&nbsp; 2000&nbsp;&nbsp; 3000&nbsp;&nbsp; 4000&nbsp;&nbsp; 5000&nbsp;&nbsp; 6000&nbsp;&nbsp; 7000&nbsp;&nbsp; 8000&nbsp;&nbsp; 9000&nbsp; 10000<BR>TPS (&#61548;)</P>
<P>0.01</P>
<P>0.00</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1000&nbsp; 2000&nbsp;&nbsp; 3000&nbsp;&nbsp; 4000&nbsp;&nbsp; 5000&nbsp;&nbsp; 6000&nbsp;&nbsp; 7000&nbsp;&nbsp; 8000&nbsp;&nbsp; 9000&nbsp; 10000<BR>TPS (&#61548;)<BR>Fig. 5.3 Time until operational corruption (logU measured in days).</P>
<P>Fig. 5.4 Fraction of aborts.</P>
<P>Like many concurrency control protocols in the literature (e.g., [38, 66]), the Deterministic Reciprocal Consistency Protocol treats updates as being provisional initially. They become permanent only if, and when, the transaction that contains them is allowed to commit.<BR>Provisional updates on a given record can occur only one at a time and each is time- stamped using the local server&#8217;s clock. Thus, when a transaction attempts to update a given record, it can identify all other transactions, called predecessors (if any), that have earlier updated that record provisionally (similarly to the Wait-Hit Protocol in Chapter 3). Read operations receive the latest committed version of a record and ignore any provisionally updated values.</P>
<P>5.3.1&nbsp;Protocol Description</P>
<P>Collision detection. Recall from Subsection 2.2.1 that an update operation by a transaction Tx for a distributed edge has a part 1, carried out at the first server visited (e.g., Si in Figure 5.1), and part 2, performed at the second server (e.g., S j in Figure 5.1) after a network delay. The corresponding provisionally updated records are now given label 1 and label 2, respectively, and are associated with the id of that transaction.</P>
<P>These labels act as history meta-data indicating the server where a transaction started and completed its update. After performing part 1 at say Si, Tx remembers the labels of all predecessors that have operated on the edge at that server before it. When arriving at S j to perform part 2, Tx retrieves the labels of transactions (predecessors) that have operated on that edge at S j before it. The following rule is applied at S j:<BR>Cancellation rule: If, by the time part 2 is performed, a previous provisional update labeled 1 has been observed for a predecessor, but the corresponding label 2 has not been observed, then this update is canceled. In other words, an update is canceled if it has observed the start of a previous attempt by a transaction, but not its completion. When an update is canceled, the transaction containing it is aborted and all its provisional records are erased.<BR>According to the this rule, Tx in Figure 5.1(a) is canceled because it observes a predecessor Ty with label 1 in S j, but has not observed Ty&#8217;s label 2 in Si. Whether Ty is canceled or not depends on whether Tx&#8217;s provisional updates remain or have been erased by the time Ty performs part 2.<BR>We now describe an extension to the Deterministic Reciprocal Consistency Protocol that additionally ensures Edge-Order Consistency (see Subsection 2.2.3) before developing an analytical model and evaluating the protocol&#8217;s performance.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4 Deterministic Edge Consistency Protocol</P>
<P>The Deterministic Edge Consistency Protocol extends the Deterministic Reciprocal Consis- tency Protocol to ensure there is also zero probability for the violation of Edge-Order Consis- tency. Deterministic Edge Consistency Protocol employs two mechanisms: (i) Deterministic Reciprocal Consistency Protocol&#8217;s collision detection to enforce Reciprocal Consistency for distributed updates, and (ii) order arbitration to enforce Edge-Order Consistency between transactions.</P>
<P>As before, the Deterministic Edge Consistency Protocol treats updates as being provi- sional initially and they become permanent only if, and when, the transaction that contains them is allowed to commit. Also, again provisional updates on a given record can occur only one at a time and each is time-stamped using the local server&#8217;s clock. Hence, when updating a record, a transaction can identify all predecessors (if any), that have earlier updated that record provisionally. Read operations receive the latest committed version of a record and ignore any provisionally updated values.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4.1 Protocol Description</P>
<P>For a transaction, collision detection is applied at every update it executes and may trigger an immediate abort. Transactions that survive collision detection may, if necessary, go to order arbitration. The latter may also result in an abort.<BR>Order arbitration. The purpose of this mechanism is to detect and prevent edge-order inconsistencies between transactions. It only applies to transactions that contain more than one distributed update. Those with a single update that have not been aborted by collision detection are allowed to commit and depart.<BR>Using the records relating to provisional updates, each multi-update transaction maintains a predecessor list containing all predecessor transactions it encountered during its provisional updates. If that list is empty when the transaction successfully completes all its provisional updates, then it commits and departs. Otherwise it goes to arbitration.<BR>The arbiter is a special service, assumed here to have been implemented in a dedicated server. A list called the hit-list is maintained. It contains transactions which, if allowed to commit, risk violating edge-order consistency. Transactions arriving for arbitration join a queue and are served in order of arrival. If the transaction at the head of the queue is not present in the hit-list, it commits. All transactions in its predecessor list are added to the hit list if not already there. If it is in the hit list, it aborts and all its provisional updates are</P>
<P>erased. What has happened in this case is an overtaking: the current transaction was named as a predecessor by a transaction that committed earlier.<BR>Note that order arbitration is a simple version of the Wait-Hit Protocol in Chapter 3. Additionally, it is also possible here to adopt a pattern similar to the Distributed Wait-Hit Protocol from Section 3.6 and execute a validation and commit phase, instead here we opt for a centralized arbiter.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>t1&nbsp;t2&nbsp;t3&nbsp;t4&nbsp;time Fig. 5.5 Edge-Order Consistency violation</P>
<P>We can informally argue that our approach is correct by considering the edge-order inconsistency depicted in Figure 2.6 (reproduced from Figure 2.6 for convenience). It can be seen that TA will have TB in its predecessor list while updating e&#8242;, and TB will observe TA as a predecessor while updating e. Both TA and TB must approach the arbiter because they update more than one edge and have a non-empty predecessor list. If the first transaction to be processed by the arbiter is allowed to commit, the second one will be entered in the hit list and will abort. Thus only one of TA and TB, but not both, can commit and edge order inconsistency is always avoided.<BR>Note that this approach to arbitration is pessimistic. It aborts a transaction as soon as it detects a risk of edge-order violation, even though the actual violation may not occur. Consequently, some transactions are aborted unnecessarily, just because they are overtaken by their successors. To eliminate unwarranted aborts, the arbiter would have to keep much more detailed information about the updates performed by all transactions, and would have to do considerably more processing.</P>
<P>We now proceed to the task of evaluating certain performance measures, such as the average number of transactions that are aborted per unit time, the offered load at the arbiter, and the average time a transaction remains in the system. Since the processes involved are rather complex, such an evaluation will inevitably entail approximations. That, in turn, will necessitate an assessment of the accuracy of those approximations.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4.2 Approximate Model</P>
<P>We are concerned with updates performed on distributed edges in a distributed graph database (i.e., edges whose source and destination nodes are stored on different servers). These edges are divided into T types, numbered 1, 2, . . ., T . The number of edges of type i is Ni, and the probability that an update operation is aimed at an edge of type i is pi. All edges of a given type are equally likely to be addressed, so that the probability of accessing a particular edge of type i is pi/Ni.<BR>Transactions arrive into the system in a Poisson stream, at the rate of &#955; per second. Each<BR>transaction performs a random number, K, of updates for different distributed edges. The distribution of K is arbitrary: P(K = k) = rk (k = 1, 2, . . .). The average number of updates per transaction is &#954;. Thus, the arrival rate of updates at a particular distributed edge of type i, &#958;i, is equal to<BR>&#958;i = &#954;&#955; pi<BR>Ni<BR>; i = 1, 2, . . . , T .&nbsp;(5.12)</P>
<P>The first approximation is to assume that the arrival process of updates for a particular distributed edge of type i is Poisson with rate &#958;i.<BR>We wish to estimate the probability, ui, that a transaction, Tx, containing an update for an edge of type i, is cancelled due to a collision with another transaction, Ty, containing an update for the same edge. That is, either Ty arrives in the opposite server during the network delay of Tx (Figure 2.4 case (a)), or Tx arrives in the opposite server during the network</P>
<P>delay of Ty (case (b)), or Ty arrives in the same server during the network delay of Tx and its network delay completes before that of Tx (case (c)).<BR>Assume that the network delays are i.i.d random variables distributed exponentially with parameter &#948; (mean 1/&#948; ). This may or may not be an approximation.<BR>Updates for a particular distributed edge of type i arrive in one of the two servers involved in storing the edge at rate &#958;i/2. Moreover, a given network delay completes before another<BR>with probability 1/2. Hence, we can estimate the probabilities of cases (a), (b), and (c), u(a),<BR>u(b) and u(c), as</P>
<P>u(a) = u(b) =&nbsp;&nbsp;&nbsp; &#958;i&nbsp;</P>
<P><BR>;&nbsp; u(c) = 1&nbsp;&#958;i&nbsp;</P>
<P>&nbsp;</P>
<P>; i = 1, 2, . . . , T ,&nbsp;(5.13)<BR>i&nbsp;i&nbsp;&#958;i + 2&#948;&nbsp;i&nbsp;2 &#958;i + 2&#948;</P>
<P>where &#958;i is given by (Equation (5.12)) and &#948; is the parameter of the network delay. The overall probability, ui, that at least one of those events will happen, is</P>
<P>ui = 1 &#8722; (1 &#8722; u(a))(1 &#8722; u(b))(1 &#8722; u(c)) &#8776; 2.5&#958;i <BR>; i = 1, 2, . . . , T .&nbsp;(5.14)<BR>i&nbsp;i&nbsp;i<BR>&#958;i + 2&#948;</P>
<P>The last approximation in the right-hand side holds when the rate &#958;i is small compared to &#948; .<BR>The unconditional probability, u, that an arbitrary update is cancelled by the collision detection mechanism, is given by<BR>T<BR>u = &#8721; piui .&nbsp;(5.15)<BR>i=1<BR>The probability, vk, that a transaction containing k updates is aborted because one of them is involved in a collision, is equal to</P>
<P>vk = 1 &#8722; (1 &#8722; u)k ,&nbsp;(5.16)</P>
<P>and the unconditional probability, v, that a transaction is aborted due to a collision is given by</P>
<P>&#8734;<BR>v = &#8721; rkvk .&nbsp;(5.17)<BR>k=1</P>
<P>Now consider the average run time, ak, of a transaction that contains k update operations. Assume that each update takes time b, on the average. Those times include read operations<BR>and computations, as well as network delays. If the first j &#8722; 1 provisional updates are<BR>completed successfully but the j-th update is cancelled as a result of a collision, then the average run time would be jb. Hence, ak is given by</P>
<P>k<BR>ak = &#8721; jb(1&nbsp;u) j&#8722;1u + kb(1&nbsp;u)k ,&nbsp;(5.18)<BR>j=1</P>
<P>where u is given by (Equation (5.15))<BR>With a little manipulation, this expression can be simplified to</P>
<P>k<BR>a&nbsp;b&nbsp;1<BR>u j&#8722;1<BR>b 1 &#8722; (1 &#8722; u)k</P>
<P><BR>(5.19)<BR>k =&nbsp;&#8721;(&nbsp;)&nbsp;=&nbsp;.<BR>j=1</P>
<P>The unconditional average run time of a transaction, a, is equal to</P>
<P>&#8734;<BR>a = &#8721; rkak .&nbsp;(5.20)<BR>k=1</P>
<P>If all provisional updates in a transaction are completed successfully, and if either there was only one update, or there were no predecessors, then the transaction commits. Otherwise it goes to the arbiter. The time a transaction spends queueing and being served by the arbiter will be referred to as the arbitration time.<BR>Assume (this is another approximation) that each transaction joins the arbiter queue with probability &#945;, independently of the others. That is, the arrival process is Poisson, with rate</P>
<P>&#955;&#945;. The arbiter&#8217;s average service time, s, is a given parameter. Thus the offered load at the arbiter is &#961; = &#955;&#945;s.<BR>Treating the arbiter as an M/M/1 queue, we estimate the average arbitration time, w, as</P>
<P>w =&nbsp;s<BR>1 &#8722; &#961;<BR>,&nbsp;(5.21)</P>
<P>provided that &#961; &lt; 1. If &#961; &#8805; 1, then w = &#8734;. The total average time that a transaction spends in the system is<BR>W = a + &#945;w ,&nbsp;(5.22)</P>
<P>where a is given by (Equation (5.20)).<BR>We shall now develop an iterative fixed-point approximation for &#945;. Denote by d j,k the average lifetime of the j-th update within a transaction containing k updates, excluding any possible arbitration time. By an argument similar to the one that led to (Equation (5.19)), we obtain<BR>d j,k = b<BR>k+1&nbsp;&nbsp; j<BR>&#8721;<BR>i=1<BR>(1 &#8722; u)i&#8722;1<BR>1&nbsp;(1&nbsp;u)k+1&#8722; j<BR>= b&nbsp;u&nbsp;.&nbsp;(5.23)<BR>The lifetime of a randomly chosen update within a transaction containing k updates, dk<BR>(again excluding arbitration), is given by</P>
<P>d&nbsp;1 k d<BR>b (k + 1)u + (1 &#8722; u)k+1 &#8722; 1</P>
<P><BR>(5.24)<BR>k =&nbsp;&#8721;<BR>j=1<BR>j,k =<BR>ku2&nbsp;.</P>
<P>Hence, the total average time spent in the system by an arbitrary update (including the<BR>arbitration time), d, is equal to<BR>&#8734;<BR>d = &#8721; rkdk + &#945;w ,&nbsp;(5.25)<BR>k=1<BR>where w is given by (Equation (5.21)).<BR>Now, let &#947;i be the probability that an update of type i has a predecessor, i.e., the probability that such an update arrives while a preceding update for the same edge is still in the system.</P>
<P>Assuming that the update residence times are distributed exponentially with mean d given by (Equation (5.25)), this can be approximated as<BR>&#947;i =&nbsp;&nbsp; &#958;id&nbsp;&nbsp;&nbsp; ,&nbsp;(5.26)<BR>1 + &#958;id</P>
<P>where &#958;i is given by (Equation (5.12)).<BR>The unconditional probability, &#947;, that an arbitrary update has a predecessor, is</P>
<P>T<BR>&#947; = &#8721; pi&#947;i .&nbsp;(5.27)<BR>i=1</P>
<P>If a transaction contains k updates, the probability that at least one of them has a prede- cessor, &#945;k, is<BR>&#945;k = 1 &#8722; (1 &#8722; &#947;)k .&nbsp;(5.28)<BR>Remembering that a transaction goes to the arbiter if it has more than one update and all updates avoid collisions and at least one of them has a predecessor, we write<BR>&#8734;<BR>&#945; = &#8721; rk(1&nbsp;u)k&#945;k .&nbsp;(5.29)<BR>k=2</P>
<P>Note that the right-hand side of (Equation (5.29)) depends on &#945;, via (Equation (5.25)) and (Equation (5.26)). In other words, we have a fixed-point equation of the form</P>
<P>&#945; = f (&#945;) .&nbsp;(5.30)</P>
<P>This can be solved by a simple iterative scheme. Start with an initial guess, &#945;0, say &#945;0 = 0. At iteration n, compute<BR>&#945;n = f (&#945;n&#8722;1) ,&nbsp;(5.31)<BR>stopping when two consecutive iterations are sufficiently close to each other.</P>
<P>The probability &#945; allows us to evaluate the offered load at the arbiter queue, and hence estimate the average response time of a transaction, W . Another important performance measure is the rate of aborts, R, i.e. the average number of transactions that are aborted per unit time. Note that a transaction may be aborted due to a collision, with probability v given by (Equation (5.17)), or it may be aborted because it finds itself on the arbiter&#8217;s hit list. Denoting the probability of the latter occurrence by &#946; , we can write</P>
<P>R = &#955; [v + (1 &#8722; v)&#946; ] .&nbsp;(5.32)</P>
<P>To find an expression for the probability &#946; , note that a transaction, A, is aborted by the arbiter if (i) A goes to the arbiter and (ii) another successfully committing transaction, B, which arrived at the arbiter before A, had A in its list of predecessors (A would then have been added to the hit list). That is, B arrives in the system during the run time of A, tries to update one of the edges that A has updated, completes before A, goes to the arbiter and is allowed to commit.<BR>Suppose that A contains k updates, and let t be the instant when the j-th of those updates is attempted. The average interval from t until the completion of A, given that all updates<BR>succeed, is (k + 1 &#8722; j)b. If the j-th update is of type i, let hi be the average interval from t<BR>until the completion of the next transaction, B, that updates the same edge and then goes to the arbiter. That average can be estimated as<BR>hi = 1 + &#954; &#8722; r1&nbsp; b ,&nbsp;(5.33)<BR>&#958;i&#945;&nbsp;2(1 &#8722; r1)</P>
<P>where &#945; is given by (Equation (5.29)). The multiplier of b in the right-hand side is half of the average number of updates in a transaction, given that there are more than one.<BR>Denote by &#946;i jk the probability that B arrives after the j-th update out of the k in A, and completes before A, and A goes to the arbiter but is aborted because B commits, given that</P>
<P>the j-th update is of type i. We write</P>
<P>&#946;&nbsp;= &#945;(1&nbsp;&#946; ) (k + 1 &#8722; j)b&nbsp;.&nbsp;(5.34)<BR>hi + (k + 1 &#8722; j)b<BR>Removing the conditioning on the type of update, we get the probability, &#946;jk, that A is aborted by the arbiter due to the j-th of its k updates:</P>
<P>T<BR>&#946;jk = &#8721; &#946;i jk pi .&nbsp;(5.35)<BR>i=1</P>
<P>The probability, &#946;k, that at least one of the k updates will cause A to be aborted, is</P>
<P>k<BR>&#946;k = 1&nbsp;&#8719;(1&nbsp;&#946;jk) .&nbsp;(5.36)<BR>j=1</P>
<P>Finally, the unconditional probability, &#946; , that an arbitrary transaction is aborted by the<BR>arbiter, can be expressed as<BR>&#8734;<BR>&#946; = &#8721; &#946;krk .&nbsp;(5.37)<BR>k=2<BR>The right-hand side of this equation depends on &#945;, which has already been computed, and also on &#946; . Thus, we have another fixed-point equation which can be be solved by an iterative procedure of the type (Equation (5.31)).<BR>One might wish to measure the performance of the system by a cost function of the form</P>
<P>C = c1W + c2R ,&nbsp;(5.38)</P>
<P>where c1 and c2 are some coefficients reflecting the relative importance given to the average response time and number of aborts. There are trade-offs that may need to be controlled. If, for example, the arbiter is overloaded, leading to large or infinite response times, a &#8216;voluntary abort&#8217; policy may be introduced. If a transaction cannot commit upon completion (because</P>
<P>its predecessor&nbsp; list is&nbsp; non-empty), it&nbsp; tosses a&nbsp; biased&nbsp; coin and, with&nbsp; probability &#963; , aborts<BR>instead of going to the arbiter. The offered load at the arbiter queue would then be reduced to &#961; = &#955;&#945;(1 &#8722; &#963; )s. The optimal value of &#963; would be chosen so as to minimize the cost function C.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4.3 Evaluation</P>
<P>The purpose of this section is to assess the accuracy of the model estimates by comparing them with simulations. In order to reduce the number of parameters to be set, we focus on the smallest and most frequently accessed class of edges, ignoring the larger classes where conflicts are very unlikely to occur. The examples we have chosen contain a single class with N distributed edges, each of which is equally likely to be the target of an update. The size and traffic parameters are typical of a large scale-free graph database (see also [119]).<BR>In the first example, N is varied between 5000 and 25000 edges. The arrival rate is fixed<BR>at &#955; = 1000 transactions per second. The average network delay is assumed to be 5ms (i.e., &#948; = 200). That is also the value of b (the average time per update). The distribution of the number of updates in a transaction is geometric, with mean &#954; = 5. The average arbiter service time is s = 0.01 and that value will be kept fixed in the following examples.<BR>In Figure 5.6, the total average number, R, of transaction aborted per unit time by the collision detection and by the order arbitration parts of the protocol, is plotted against the number of edges. The estimated points are computed by the algorithm described in Subsec- tion 5.4.2, while each simulated point represents the result of a simulation run where one million transactions pass through the system.<BR>Intuitively, we expect that when the number of edges increases, there will be fewer collisions and instances of overtaking, and therefore fewer aborts. Indeed, that is what is observed. The model consistently underestimates the number of aborts, but the relative errors are not large. They vary from 9% at N = 5000 to 5% at N = 25000. That underestimation is</P>
<P>40&nbsp;Model estimates +&nbsp; <BR>35&#215;<BR>30+<BR>25<BR>R 20<BR>15&nbsp;&#215;+<BR>Simulations&nbsp;&#215;<BR>10&nbsp;&#215;+<BR>5<BR>0<BR>&#215;+&nbsp;&nbsp; &#215;<BR>5000&nbsp;10000&nbsp;15000&nbsp;20000&nbsp;25000<BR>N<BR>Fig. 5.6 Abort rate as a function of N<BR>&#955; = 1000, &#954; = 5, &#948; = 200, b = 0.005, s = 0.01</P>
<P>probably caused by the simplifying assumptions used in deriving the approximate estimates. On the other hand, the times taken to produce the two plots were vastly different: the model plot took a small fraction of a second to compute, while the simulation runs were several orders of magnitude slower.<BR>From now on, the number of edges will be fixed at N = 10000 and the effect of different<BR>parameters will be explored. In the second example, the arrival rate &#955; is varied between 700 and 1200 transactions per second, while the other parameters are kept as before.<BR>In Figure 5.7, the average number of aborted transactions per second, R, is plotted against the arrival rate &#955; , using both the model approximation and simulations. Each simulated point is again the result of a run where one million transactions pass through the system. Once more, we observe that the model slightly under-estimates the values of R, but the relative errors are quite small; they are of the order of 6% or less, over the entire range.<BR>The average response time of a transaction, W , was about 25ms; its value changed very little over this range of arrival rates.</P>
<P>25</P>
<P>20</P>
<P>15<BR>R<BR>10</P>
<P>5</P>
<P>0<BR>700</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>800&nbsp;900&nbsp;1000&nbsp;1100<BR>&#955;<BR>Fig. 5.7 Abort rate as a function of &#955; &#954; = 5, &#948; = 200, b = 0.005, s = 0.01</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>1200</P>
<P><BR>For these parameter values, the model predicts that the arbiter queue becomes unstable<BR>when the arrival rate is about &#955; = 1500. The simulation agrees. The observed rate at which transactions join the arbiter queue exceeds the service rate, &#181; = 100, for that value of &#955; .<BR>For the next experiment, the average network delay is doubled to 10ms, &#948; = 100.&nbsp; Intu-<BR>itively, this should have the effect of increasing the rate at which transactions are aborted, and also should increase the offered load at the arbiter queue.<BR>Figure 5.8 confirms our intuition. The relative errors of the model estimates are still<BR>quite low, of the order of 9% or less. The arrival rate is now varied between &#955; = 600 and &#955; = 1000. Both the model and the simulation agree that the arbiter queue becomes unstable when &#955; = 1100.<BR>In the fourth experiment, the network delay is back to 5ms, but the number of updates in a transaction, K, has a different distribution and mean. The assumption now is that K is uniformly distributed on the range [1,19], with a mean of 10. The results are illustrated in Figure 5.9.</P>
<P>40</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>R</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>0<BR>600</P>
<P>650&nbsp;700&nbsp;750&nbsp;800&nbsp;850&nbsp;900&nbsp;950&nbsp;1000<BR>&#955;<BR>Fig. 5.8 Larger network delays<BR>&#954; = 5, &#948; = 100, b = 0.01, s = 0.01</P>
<P>25 Model estimates +&nbsp; <BR>Simulations<BR>20</P>
<P>15</P>
<P>&#215;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&#215;+<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#215;+<BR>R&nbsp;&#215;+<BR>10<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +</P>
<P>5</P>
<P>0<BR>400&nbsp;420&nbsp;440&nbsp;460&nbsp;480&nbsp;500&nbsp;520<BR>&#955;<BR>Fig. 5.9 Different distribution of updates<BR>&#954; = 10, &#948; = 200, b = 0.005, s = 0.01</P>
<P>The larger number of updates per transaction leads to both higher likelihood of collisions and more visits to the arbiter. The saturation point for the arbiter queue is now a little below &#955; = 550. As Figure 5.9 illustrates, the model approximation is still accurate, with relative errors of the order of 8% or less.</P>
<P>It is perhaps worth noting that in the last three examples, the rate of aborts increases roughly linearly with &#955; . For all arrival rates in example 2, between 1% and 2% of the incoming transactions are aborted. In example 3 that fraction is between 2% and 3%, while in example 4 it is between 3% and 4%.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5 Conclusion</P>
<P>Database concurrency control has been a long researched area, however to the best of our knowledge this is first attempt at developing protocols specific for distributed graph databases. The Delta protocol is a lightweight protocol for providing Reciprocal Consistency and mitigating the problem of high contention in a distributed graph database. It leverages the fact that writes to distributed edges always consist of two sequential writes to entries in the adjacency lists of vertices that are connected by the edge. Since it is concerned only with edges (and not vertices) in a graph, it provides guarantees weaker than Read Uncommitted isolation (the weakest ANSI isolation level). The Delta protocol, we believe, is valuable in practice given the popularity of NoSQL distributed graph databases and the rate at which semantic corruption can spread if Reciprocal Consistency is left unchecked. Simulations indicate that when &#8710; values are chosen to be reasonably large, the protocol rules out corruption resulting from half-corrupted distributed edges while keeping the abort rate<BR>considerably small.<BR>The Deterministic Edge Consistency Protocol is comprised of two distinct mechanisms: collision detection from Deterministic Reciprocal Consistency Protocol and order arbitra- tion between transactions to provide Reciprocal Consistency and Edge-Order Consistency. The order arbitration mechanism is built on the same principles as the Wait-Hit Protocol from Chapter 3. To evaluate the protocol&#8217;s impact on system performance, in terms of aborted transactions and load on the arbiter an approximate model was developed and solved. It provides estimates for the average number of transactions that are aborted per unit time, the</P>
<P>probability that a transaction will need to go to arbitration, and the average response time of a transaction. The accuracy of the solution was examined by comparisons with simulations and was found to be very high under a variety of parameter settings.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5.1 Further Work</P>
<P>Complex Constraints. The current generation of GDBMSs generally does not support constraints much more complex than Reciprocal Consistency; sometimes domain and primary key constraints (in case indexes are supported). In the future, it is anticipated that GDBMSs will evolve to support complex constraints. Beyond equivalents of the relational ones, graph databases might introduce graph-specific constraints, such as (partial) compliance to a schema formulated on top of property graphs, rules that guide the presence of labels or structural graph constraints such as connectedness of the graph, absence of cycles, or arbitrary well- formedness constraints [94]. Each poses an interesting challenge to database architects on how to ensure performance whilst maintaining correctness.</P>
<P>Empirical Performance Evaluation. The protocols can be implemented in a distributed graph database to assess the validity of the simulations and to enable comparisons with other concurrency control protocols.</P>
<P>Stronger Isolation. Moreover, we plan on investigating the suitability of higher isolation levels in distributed graph databases, Read Atomic isolation [9] seems particularly well suited and has been implemented in Facebook&#8217;s graph store TAO [18]. Similar to the edge- order inconsistency examined here, there may also be node-order inconsistency, occurring when transactions interfere while updating the same set of nodes. Eliminating node-order inconsistencies will be addressed in future work. It is well known in the database literature that there is a hierarchy of approaches which achieve various degrees of isolation (see [2]). Selecting a level for a given application typically involves a trade-off between consistency</P>
<P>requirements and performance, which levels are suitable for distributed graph databases without introducing significant overheads or violating graph integrity remains an open question. For example, it is not uncommon for distributed graph transactions to access multiple partitions (often 5+ [17]) and it was established in [54] athat s the number of partitions accessed by MPTs increases performance severely degrades. Potentially, this makes Serializable isolation unfeasible. Another interesting direction is incorporating awareness of application-level invariants and types of common graph operations into the protocols to determine which can be preserved without coordination and which require it [10]. For example, concurrent edge insertion between two nodes is commutative and does not require coordination.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Chapter 6</P>
<P>A Performance Study of Epoch-based Commit Protocols</P>
<P>Summary<BR>Distributed OLTP systems execute the high-overhead 2PC protocol at the end of every distributed transaction. Epoch-based commit proposes that 2PC be executed only once for all transactions processed within a time interval called an epoch. Increasing epoch duration allows more transactions to be processed before the common 2PC. It thus reduces 2PC overhead per transaction, increases throughput but also increases average transaction latency. Therefore, required is the ability to choose the right epoch size that offers the desired trade-off between throughput and latency. To this end, this chapter develops two analytical models to estimate throughput and average latency in terms of epoch size taking into account load and failure conditions. Simulations affirm their accuracy and effectiveness. This chapter then presents epoch-based multi-commit which, unlike epoch-based commit, seeks to avoid all transactions being aborted when failures occur, and also performs identically when failures do not occur. Our performance study identifies workload factors that make it more effective in preventing transaction aborts and concludes that the analytical models can be equally useful in predicting its performance as well.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Introduction</P>
<P>When a single-node database reaches its capacity limits, a common option is to partition data across multiple nodes forming a distributed database [97]. High levels of scalability ensue when transactions access data within a single node, there is no need for any coordination, and hence communication, between nodes. However, when workloads contain distributed transactions accessing data from multiple nodes, an atomic commitment protocol, typically (2PC) [12], needs to be executed so that distributed transactions are guaranteed of atomicity and durability when nodes are prone to failures.</P>
<P>As discussed in Subsection 2.1.5, executing 2PC generally extracts a high performance cost [52, 54]. It involves two sequential network round trips, one for each of its phases, and two sequential durable writes. Depending on hardware deployment, when 2PC is executed at the end of each distributed transaction, it increases the transaction latency by an additional delay of up to 10ms. In addition, as 2PC execution prolongs the lifetime of a distributed transaction within the database, the potential for data contention among transactions (distributed or otherwise) intensifies which, in turn, leads to further performance degradation. For these reasons, a significant strand of distributed transactions research focuses on minimizing the costs inflicted by 2PC executions (see Subsection 2.1.6).<BR>A recent practical proposal is epoch-based commit [72]. Based on the widely-held notion that node failures are getting less frequent with modern hardware, it takes the view that 2PC need not be executed at the end of every distributed transaction. Instead, 2PC is executed once for all transactions that arrive and get processed within a time interval called an epoch. Thus, an epoch is the base unit for doing 2PC and all transactions processed within it either commit or abort through one common 2PC execution. (This idea is a distributed extension of group-commit proposed in [28] to reduce disk I/O latency for single-node databases.)<BR>The cost of one 2PC execution is thus amortized over multiple transactions processed within an epoch. Moreover, transactions whose processing is completed can release their locks instead of waiting until they commit at the epoch end; this reduces scope for contention. They can also asynchronously log their writes to persistent storage. The effects of all these features are two-fold. On the up side, throughput increases substantially &#8211; by four times (4x) as per the experiments conducted in [72]. On the down side, a transaction&#8217;s results cannot be released until the epoch end when all its writes have become durable. This means that the earlier a transaction arrives during an epoch, the longer it waits before its results can be released; i.e., the average latency of transactions increases.</P>
<P>It is important to note that amortization of 2PC cost over multiple transactions also tends to increase the throughput as epoch size increases, when nodes do not fail. However, if a node fails, then the number of transactions aborted at the end of an epoch (which is when that failure is globally detected) will be large if the epoch was chosen to be long; this wasted work obviously reduces throughput. Thus, the possibility of node failures favors shorter epochs. In fact, there is an optimum epoch length at which throughput is maximized which can be used if increased latency is a minor concern. (Increased latency is acceptable for many workloads, see [21, 80, 110].) On the other hand, a user may want a reasonably high throughput as well as an acceptably moderate latency. Such a trade-off requires the means to choose appropriate epoch length. These requirements for adopting epoch-based commit in clusters were left as future work in the original paper [72] and are being fully addressed here.<BR>We derive analytical solutions for estimating throughput and average latency in terms of epoch length and some system and load parameters. Estimating average latency accounts for aborted transactions being completed in subsequent epochs. Our derivations make certain simplifying assumptions, such as times to node failures and recovery are exponentially distributed and the cluster has at most one failed node at any time which holds if failures are independent and less frequent and if a failed node recovers before another node fails.<BR>The protocol of [72] proposes that all transactions executed during an epoch be aborted in case of a node failure. This assumes that each node has directly or indirectly accessed the failed node at some time during the epoch and therefore all transactions it executed accessed some data now lost due to failure. We examine and find this assumption to be overly pessimistic and develop an epoch-based multi-commit version. Each node maintains a list of nodes it interacted with, and uploads the list to the 2PC coordinator. The latter then constructs disjoint commit groups such that a node within a given commit group would have interacted, directly or transitively, only with other nodes in that group during the epoch. In</P>
<P>case of a node failure, only those nodes in the commit group containing the failed node will abort their transactions and the rest will commit the transactions they processed.<BR>It is easy to see that when a workload contains no distributed transaction, each commit group will have just one node and all operative nodes can commit their transactions in the event of a failure. At the other extreme, if the workload has many distributed transactions that cause every node to interact with every other node during an epoch, then there is only one commit group and a failure will cause all operative nodes to abort all their transactions as in the epoch-based commit protocol. Thus, our multi-commit version can automatically take advantage of favorable workload conditions in the event of a failure and avoid excessive aborts, while performing identically to the original version during failure-free epochs.<BR>This chapter makes three major contributions. Analytical models for estimating through- put and average latency are developed in Section 6.2 for the epoch-based commit protocol. They allow an appropriate epoch size to be judiciously chosen for maximum throughput or minimum latency or seeking a trade-off between the two. Secondly, the epoch-based multi-commit protocol is presented in Section 6.3, together with a popular benchmarking case study carried out to support its core design rationale. Finally, a range of simulation experiments involving a cluster of 64 nodes operating for a 100-day period are carried out. They (i) demonstrate the accuracy and efficacy of the models of Section 6.2 for choosing the appropriate epoch size, (ii) point to workload features that can aid the multi-commit protocol in minimizing aborts when failures occur, and (iii) affirm the effectiveness of the models in selecting the right epoch size also for multi-commit version. Section 6.4 presents the strategies adopted for performance study, followed by the presentation and discussion of results in Section 6.5. Section 6.6 concludes the chapter and summarizes future work.</P>
<P>&nbsp;</P>
<P>Coordinator</P>
<P><BR>Node N1</P>
<P><BR>Node N2</P>
<P><BR>Fig. 6.1 Work and 2PC intervals of a cycle in the epoch-based commit protocol.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2 Analytical Models for Epoch-Based Commit</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.1 Protocol Description</P>
<P>The epoch-based commit protocol is briefly described before presenting analytical solutions for estimating protocol performance. For greater detail on the protocol, readers are referred to [72].<BR>Consider a distributed system consisting of a coordinator node and N, N &gt; 1 participant nodes that are simply called nodes. A large OLTP database is partitioned among the latter which execute transactions and 2PC under the control of the coordinator. A (participant) node can fail in a fail-stop manner: it functions correctly when operative and fails only by ceasing to be operative. The coordinator, however, is assumed to be built reliably and never fails. More precisely, we assume that the coordinator is internally replicated (primary-backup or state machine replication using atomic broadcast such as [82] or [68]), maintaining a single server abstraction.<BR>The coordinator starts a cycle by starting an epoch timer for an interval of a and instructs the nodes to work on transactions. The epoch is referred to as work interval in Figure 6.1 during which transactions are executed but not made durable and their results also not released to end-users. A transaction will release the locks it holds at the end of its execution</P>
<P>even though it is not committed. This reduces lock contention but makes the effects of earlier transactions visible to the later ones. This is not a concern as all transactions of a given work interval commit or abort together at the end.<BR>At the end of epoch timer a, the coordinator calls for an execution of 2PC which, as shown in Figure 6.1, has two phases: prepare and commit. In the prepare phase, the coordinator sends a Prepare message1 to each participant node. (Messages exchanged during 2PC are shown by blue arrows in Figure 6.1.)<BR>When a node receives Prepare, it force-logs: (i) ids of all ready-to-commit transactions it executed, and (ii) current epoch number. (These durable writes by participants are indicated in Figure 6.1 by red squares labeled A.) It then responds with Prepare-Ack to the coordinator. It is expected that the underlying concurrency control protocol has durably logged the individual writes by the ready-to-commit transactions before the prepared write record is logged. Note, some transactions executed within the epoch may have aborted earlier due to conflicts and violation of application-level integrity constraints. These aborted transactions do not cause the whole epoch to fail, but the result of them is still not released until the epoch terminates.<BR>In the commit phase, the coordinator collects responses from participant nodes. If any node has not replied with a Prepare-Ack message, all nodes are instructed to abort all transactions they executed during the work interval. Else, the coordinator force-logs a commit record with the current epoch number (shown as a red square labeled B in Figure 6.1) and sends a Commit message to all participant nodes. When a node receives a Commit message, it commits the transactions it executed and releases the results to clients. It then sends Commit-Ack to the coordinator and awaits the latter to start the next cycle of work and 2PC intervals.<BR>In summary, multiple transactions are executed during the work interval of each cycle; they are committed (or aborted) in a common 2PC execution. The design rationale behind<BR>1Message types are indicated using mono-spaced font.</P>
<P>this approach is that the time taken to execute even a distributed transaction is negligibly small compared to the mean time before failure (MTBF) of nodes. So, the probability of several transactions being executed without encountering any node failure is fairly high. When the common 2PC execution leads to commit, its overhead per committed transaction becomes very small which, in turn, leads to an increased throughput.<BR>Several studies [15, 48, 111] analyzing node failures in clusters confirm the assumption of MTBF being considerably larger than transaction processing times. For example, Garraghan et al. [48] analyze Google Cloud Platform traces and find 5056 out of 12532 nodes exhibiting 8954 failure events over a 29 day period. A node&#8217;s MTBF turns out to be 12.71 hours! Another metric analyzed in [48] is also relevant for our analytical models: the mean time to repair (MTTR) is around 6-30 times smaller than MTBF. So, a failed node is most likely to have recovered well before another failure occurs.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.2 Modeling Assumptions</P>
<P>We make two assumptions in our analytical modeling and derivation of expressions for estimating maximum throughput and average latency.<BR>Assumption 1: A failed node recovers before another node in the system fails; i.e., between any two consecutive node failures, there is a recovery event of the first failed node.<BR>A common observation in the literature (e.g., [15, 48, 111]) is that MTBF of nodes is much larger compared to their MTTR: a failed node is almost certain to be repaired before the next failure occurs. This near certainty is here assumed to be a certainty for the number of nodes typically found in a distributed OLTP system. Thus, there is at most one failed node in the system at any time.<BR>Assumption 2: As soon as a node receives a Prepare message from the coordinator, it<BR>instantly completes any ongoing transaction execution and enters the 2PC execution.</P>
<P>In reality, some non-zero amount of time will elapse for a node to complete its ongoing execution (if any) and then respond with Prepare-Ack. This assumption can lead to an overestimation of throughput.<BR>Our simulations do not have these assumptions and therefore will assess the accuracy of the expressions derived.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.3 Maximum Throughput</P>
<P>We derive an analytical expression for estimating maximum attainable throughput by assum- ing that a node always has a transaction to execute and is never idle.<BR>A participant node fails at exponentially distributed intervals with a low rate, &#958; . The repair times are also distributed exponentially with a higher rate &#951; &gt;&gt; &#958; . Recall that nodes go through cycles, each consisting of a work interval, U , followed by a random 2PC interval V in which 2PC executed. To start with, we will regard U as random (as opposed to being fixed as a constant a as explained earlier). Denote the probability density functions of U and<BR>V by fU (x) and fV (x), respectively. Let also fW (x) be the convolution of fU (x) and fV (x),<BR>i.e., the p.d.f. of the sum U + V .<BR>During a work interval, transactions are served at rate N&#181; (transactions per unit time), if all nodes are operative, and at rate (N &#8722; 1)&#181; if one of them has failed. At the end of a work and 2PC cycle, either the commit operation completes successfully and all transactions<BR>executed during the work interval U depart from the system, or a node failure has occurred and all transactions executed during U are aborted and remain in the system for re-execution.<BR>The probability, &#945;, that N operative nodes complete one cycle successfully (i.e., with none of them failing), is<BR>&#945; = &#8747; &#8734; f</P>
<P>(x)e&#8722;N&#958;xdx = w&#732;(N&#958; ) = u&#732;(N&#958; )v&#732;(N&#958; ) ,&nbsp;(6.1)<BR>0</P>
<P><BR>J1 D1<BR>J2 D2<BR>J3<BR>0<BR>0<BR>D3</P>
<P><BR>Observation Period</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Failure i&nbsp;Repair i&nbsp;Failure i + 1<BR>Fig. 6.2 Observation period with full cycles having N&nbsp;1 operative nodes, N&nbsp;1 and N<BR>operative nodes, and N operative nodes.</P>
<P>where u&#732;(s), v&#732;(s) and w&#732;(s) are the Laplace transforms of&nbsp; fU (x),&nbsp; fV (x) and&nbsp; fW (x), respectively. Consider an interval between two consecutive node failures, to be referred to as the observation period, indicated in Figure 6.2. The probability that exactly m consecutive work<BR>and 2PC cycles are completed successfully during the observation period is</P>
<P>pm = &#945;m(1 &#8722; &#945;) .&nbsp;(6.2)</P>
<P>Hence, the average number of successful cycles during that period is</P>
<P>m&#175;&nbsp; =&nbsp;&nbsp;&nbsp; &#945;&nbsp; .&nbsp;(6.3)<BR>1 &#8722; &#945;<BR>The observation period begins with the repair period of the node that had failed. During that period there are only N &#8722; 1 operative nodes. By analogy with (Equation (6.1)), the probability, &#946; , that they will complete one work and 2PC cycle before the failed node recovers, is</P>
<P>&#946; = &#8747; &#8734; f<BR>(x)e&#8722;&#951;xdx = u&#732;(&#951;)v&#732;(&#951;) .&nbsp;(6.4)<BR>0</P>
<P>Consequently, the average number of consecutive cycles during the repair period (colored<BR>red in Figure 6.2) is<BR>m&#175; 1 =&nbsp;&nbsp;&nbsp; &#946;&nbsp; .&nbsp;(6.5)<BR>1 &#8722; &#946;<BR>The total average number of transactions departing during these cycles, J1, is equal to</P>
<P>J1 = m&#175; 1E(U )(N &#8722; 1)&#181; ,&nbsp;(6.6)</P>
<P>where E(U ) = &#8722;u&#8242;(0) is the average length of work intervals.<BR>Then we have a cycle that overlaps the repair completion instant (marked in parts with red and green in Figure 6.2). In addition, if the repair instant falls during the work interval of that cycle, then for the remainder of that interval there is an extra node (i.e., the repaired one) available. The average number of transactions, J2, departing during this cycle, given that the repair is completed within it, can be expressed as<BR>J2 = E(U )(N &#8722; 1)&#181; + &#181;&nbsp;E(U &#8722; R) ,&nbsp;(6.7)<BR>where E(U &#8722; R) is the expected remaining work interval, given that the repair completes within the cycle. Averaging over the distribution of U , we can write</P>
<P>E U &#8722; R<BR>&#8747; &#8734; f<BR>x &#8747; x<BR>x &#8722; y &#951;e&#8722;&#951;ydydx</P>
<P>(6.8)</P>
<P><BR>After carrying out the integration and substituting the result into (Equation (6.7)), the latter<BR>becomes<BR>J2 = E(U )(N &#8722; 1)&#181; +&nbsp;&nbsp;&nbsp; &#181;&nbsp;&nbsp; E(U ) &#8722; 1 &#8722; u&#732;(&#951;)&nbsp; ,&nbsp;(6.9)<BR>1 &#8722; &#946;&nbsp;&#951;<BR>where u&#732;(s) is the Laplace transform of fU (x).</P>
<P>Finally, the total average number of departures during fail-free cycles with N operative nodes (marked green in Figure 6.2), with their average number being m&#175; &#8722; m&#175; 1 &#8722; 1, is given by<BR>J3 = (m&#175; &#8722; m&#175; 1 &#8722; 1)E(U )N&#181; .&nbsp;(6.10)</P>
<P>The system throughput, T , defined as the average number of departures per unit time, is obtained by dividing the total average number of departures during the observation period by the average length of the observation period:</P>
<P>T = (J1 + J2 + J3)N&#958; .&nbsp;(6.11)</P>
<P>Now consider the total average number of transactions, D, that are lost during the observation period. Losses occur either during the initial repair period, when transactions attempt to access the failed node, or when the last cycle in the observation period is interrupted by the next node failure.<BR>As soon as a transaction is admitted into a node, it produces a list of a random number, k, of other nodes that it would need to access. If the failed node is on that list, the transaction is instantaneously dismissed and is lost. That procedure is repeated with the transaction that follows, so following a service completion there may be a series of transactions lost instantaneously.<BR>The probability, &#947;, that a transaction admitted into an operative node has the failed node<BR>on its list is<BR>&#947; = E(k)<BR>N &#8722; 1<BR>,&nbsp;(6.12)</P>
<P>where E(k) is the average size of the list. This is a given parameter. The average number of transactions lost instantaneously following a service completion during the repair period is therefore equal to &#947;/(1 &#8722; &#947;).</P>
<P>We conclude that the average number of transactions lost during the m&#175; 1 successful work and commit cycles within the repair period is<BR>D1 = J1&nbsp; &#947;&nbsp;,&nbsp;(6.13)<BR>1 &#8722; &#947;</P>
<P>where J1 is given by (Equation (6.6)).<BR>To find the average number, D2, of transactions lost during the work and 2PC cycle overlapping the repair instant, we proceed as in the derivation of (Equation (6.7)), but count only the transactions completed before the repair instant. This leads to the following expression:<BR>D2 = (N &#8722; 1)&#181;&nbsp;&#947;&nbsp;&nbsp;&nbsp; 1 [1 &#8722; u&#732;(&#951;)] &#8722; u&#732;&#8242;(&#951;)&nbsp; .&nbsp;(6.14)<BR>1 &#8722; &#946;&nbsp;1 &#8722; &#947;&nbsp;&#951;</P>
<P>Finally, we need the average number, D3, of transactions that are lost from the last work and commit cycle in the observation period, the one that is interrupted by the next failure instant. Since all transactions executed during that cycle are lost, we can write</P>
<P>D3 = E(U )N&#181; .&nbsp;(6.15)</P>
<P>The overall rate of transaction losses, D, is given by the total average number of losses during the observation period, divided by the average length of the observation period:</P>
<P>D = (D1 + D2 + D3)N&#958; .&nbsp;(6.16)</P>
<P>The work interval U is set by the control policy as a constant, a. On the other hand, the commit operation is affected by communication delays, so it is more natural to assume that V is random, possibly distributed exponentially with mean b. In that case, we would have</P>
<P>u&#732;(s) = e&#8722;as ;<BR>v&#732;(s) =&nbsp; 1&nbsp;.&nbsp;(6.17)<BR>1 + bs</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.4 Average response time</P>
<P>We will no longer regard that nodes are always busy but assume that transactions arrive in a Poisson stream with rate &#955; and, if there are no available nodes, wait in an external first in first out (FIFO) queue. After being processed, a transaction does not depart immediately but is held in the queue until the end of the current cycle. If commit is the 2PC outcome, all transactions that were executed during the work interval depart together. Otherwise, they are aborted and continue to remain in the queue. The performance measure of interest here is the steady-state average response time, W , defined as the interval between the arrival of a transaction into the system and its departure.<BR>This type of system has been referred to in the literature as a queue with bulk services. At certain service instants, batches of transactions are removed from the queue. More precisely, if the size of the current batch is m and the number of transactions present just before the<BR>service instant was n, then just after the service instant there are n &#8722; min(n, m) transactions<BR>present. A model where the intervals between service instants have a general distribution and all batches have the same fixed size was analyzed by Bailey [7] more than half a century ago.<BR>Bailey&#8217;s result cannot be used in our case because the number of transactions departing at a service instant, i.e., when 2PC execution completes, depends on whether a breakdown<BR>occurred during the cycle or not, and also on whether there were N or N &#8722; 1 operative servers.<BR>We propose two estimates for W : the first is pessimistic and can be treated as an upper bound on the response time; the second is clearly optimistic and will provide a lower bound.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.5 Upper bound, Wu<BR>The first estimate is obtained by assuming that the consecutive intervals between service instants are i.i.d. random variables distributed exponentially with mean a + b, where a is the average work interval and b is the average 2PC interval. The parameter of that distribution will be denoted by &#957; = 1/(a + b). The reason why this is a pessimistic assumption is that</P>
<P>the coefficient of variation of the exponential distribution is 1, while in practice the work interval is likely to be constant, or nearly constant. The 2PC interval tends to be much smaller than the work interval, so even if it is random, the coefficient of variation of a full cycle (comprising both work and 2PC intervals) would tend to be closer to 0 than to 1.<BR>Under the exponential assumption, the probability that a full cycle is not interrupted by a node failure is now approximated by</P>
<P>&#945; =&nbsp;&#957;&nbsp;.&nbsp;(6.18)<BR>N&#958; + &#957;</P>
<P>When N&#958; is small, this value is very close to the one produced by (Equation (6.1)).<BR>Hence, a service batch is of size 0 with probability q0 = 1 &#8722; &#945;.<BR>Since the average period during which there are N &#8722; 1 operative servers is 1/&#951; and the average period during which there are N operative servers is 1/N&#958; , we can say that a 2PC interval has N &#8722; 1 operative servers with probability q1 = &#945;N&#958;/(N&#958; + &#951;), and has N servers<BR>with probability q2 = &#945;&#951;/(N&#958; + &#951;). In the former case, an average of m1 = a(N &#8722; 1)&#181;<BR>transactions are served during the cycle, and in the latter case m2 = aN&#181; transactions are served.<BR>The above arguments support an assumption that the service batch size, m, is equal to</P>
<P><BR>m = &#63730; m1&nbsp;with probability q1<BR>&#63732;&#63731; m2&nbsp;with probability&nbsp;&nbsp;&nbsp; q2<BR>If m1 and m2 are not integers, their integer parts are taken.<BR>The average batch size, B, is</P>
<P>.&nbsp;(6.19)</P>
<P>B = m1q1 + m2q2 = aN&#181;&#945; (N &#8722; 1)&#958; + &#951; .&nbsp;(6.20)<BR>+</P>
<P>The necessary and sufficient condition for the stability of the bulk service queue is that the transaction arrival rate should be strictly less than the average service capacity:</P>
<P>&#955; &lt; &#957;B .&nbsp;(6.21)</P>
<P>When the failure rate is small and the repair rate is significantly higher, the right-hand side of this inequality is very close to the maximum throughput, T , obtained in the previous section. Thus, requirement (Equation (6.21)) is almost identical to the more accurate stability condition &#955; &lt; T .<BR>Let &#960;n be the steady-state probability that there are n transactions present in the queue. Because of the bulk service assumption, any transactions that are in fact being served, are considered to be in the queue until the next service instant. The number n increases by 1 at arrival instants, and it decreases by 0, m1 or m2 at service instants. Equating the up and down<BR>transition rates across the boundary between states n and n + 1, we obtain the following set<BR>of balance equations.</P>
<P>&#955;&#960;&nbsp;&#957; " m1&nbsp;&nbsp;&nbsp; q&nbsp;q&nbsp; &#960;<BR>m2&nbsp;q &#960;&nbsp;#; n&nbsp;0 1</P>
<P>(6.22)<BR>n =&nbsp;&#8721;( 1 +<BR>j=1<BR>2) n+ j +<BR>&#8721;<BR>j=m1+1<BR>2&nbsp; n+ j<BR>= , , . . .</P>
<P>We shall obtain the general solution to this set of equations in geometric form:</P>
<P>&#960;n = Czn ,&nbsp;(6.23)</P>
<P>where C and z1 are some positive constants. Substituting (Equation (6.23)) into (Equa- tion (6.22)), we find that the equations are satisfied as long as z is a zero of the following polynomial of degree m2.</P>
<P>P z&nbsp;&#955;<BR>&#957; " m1&nbsp;&nbsp;&nbsp; q&nbsp;q z j<BR>m2&nbsp;q z j#</P>
<P>(6.24)<BR>( ) =&nbsp;&#8722;<BR>&#8721;( 1 + 2)<BR>j=1<BR>+&nbsp;&#8721;&nbsp;2&nbsp;.<BR>j=m1+1</P>
<P>In addition, in order that we may obtain a probability distribution, z1 must be a positive real number in the interval 0 &lt; z1 &lt; 1.<BR>We have P(0) = &#955; &gt; 0 and P(1) = &#955; &#8722; &#957;(m1q1 + m2q2) &lt; 0, according to (Equa-<BR>tion (6.21)). Therefore, P(z) has a real zero, z1, in the interval (0, 1). This provides a normalizable solution to the set of balance equations and allows us to write</P>
<P>&#960;n = (1 &#8722; z1)zn<BR>; n = 0, 1, . . . .&nbsp;(6.25)</P>
<P><BR>It is possible to prove formally that P(z) has no other zeros in the interior of the unit disk,<BR>but this also follows from the fact that an ergodic Markov process cannot have more than one normalizable distribution.<BR>The steady-state average number of transactions in the system, L, is obtained from<BR>(Equation (6.25)):<BR>L&nbsp;&#8734;&nbsp;z1 <BR>= &#8721; n&#960;n = 1 &#8722; z1 .&nbsp;(6.26)<BR>The upper bound of the average response time, Wu, is then provided by Little&#8217;s theorem:</P>
<P>Wu = L .&nbsp;(6.27)</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2.6 Lower bound, Wd<BR>A very simple lower bound is derived by making two optimistic assumptions. The first is that the work interval and 2PC interval are constant, of lengths a and b, respectively. The second is that the transactions arriving during the work interval are cleared at the end of that cycle, while those arriving during the commit operation are cleared during the next cycle, provided that no breakdown occurs in the meantime. That would be a reasonable assumption if the total average number of arrivals during work and 2PC intervals of a cycle is</P>
<P>smaller than the average number that can be served by N &#8722; 1 servers during a work interval:<BR>&#955; (a + b) &lt; a(N &#8722; 1)&#181;.<BR>When the cycle duration is constant at (a + b), the probability that a cycle does not involve a node failure is<BR>&#945; = e&#8722;N&#958;(a+b) .&nbsp;(6.28)<BR>In that case a transaction arriving during a work interval remains in the system for an average of half a work interval plus 2PC interval, while an arrival during a 2PC interval remains in the system for an average of half 2PC interval plus a full cycle. The probabilities that an incoming transaction arrives during a work interval or a 2PC interval are a/(a + b) and b/(a + b), respectively. Hence, the average sojourn time given that the cycle is failure-free can be written as<BR>a&nbsp;a&nbsp;b&nbsp;b&nbsp;a + 3b</P>
<P><BR>In the event of a node failure, the average sojourn time is half a cycle plus a full cycle.<BR>Thus, the lower bound on the response time becomes</P>
<P>Wd = &#945; a + 3b + (1 &#8722; &#945;)3(a + b) .&nbsp;(6.29)<BR>2&nbsp;2</P>
<P>Remember that the average work interval, a, is not a system characteristic but is set by the operating policy. It is natural to ask therefore, how should that interval be chosen in order to minimize W ? On the one hand, increasing a may improve the throughput (although that effect is mitigated by an increase in the probability of a node failure during a full cycle). On the other hand, transactions are kept in the system for longer. Intuitively, there should be an optimal value for a.<BR>Note that the lower bound (Equation (6.29)) tends to be an increasing function of a. Therefore, if Wd is taken as a criterion, the optimal a is the smallest value that justifies the assumption made above, i.e. that the average number of transactions arriving during a cycle</P>
<P>should be significantly smaller than the number that N &#8722; 1 servers can serve during a work interval. We suggest as an empirical rule of thumb that one should chose the smallest a that satisfies the inequality &#955; (a + b) &lt; 0.8a(N &#8722; 1)&#181;. This yields the value, a&#8727;, that minimizes<BR>Wd as<BR>a&#8727; =&nbsp;&nbsp;&#955;b&nbsp;.&nbsp;(6.30) 0.8(N &#8722; 1)&#181; &#8722; &#955;<BR>If the upper bound is minimized, the optimal work interval will, in general, be different.<BR>These differences will be investigated experimentally.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3 Epoch-based Multi-commit</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3.1 Rationale and Approach</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Coordinator</P>
<P><BR>Node N1</P>
<P><BR>Node N2</P>
<P><BR>Node N3</P>
<P><BR>Fig. 6.3 A cycle in epoch-based multi-commit. A node&#8217;s epoch-dependency list is indicated as dep.</P>
<P>Epoch-based commit assumes that uncommitted data within any node is used, directly or indirectly, by every other node during a work interval. Hence, all transactions executed are aborted in case of a node failure. In reality, this assumption is pessimistic and does not</P>
<P>always hold. For example, if a node is to fail shortly after it starts its work interval, it would be very unlikely that each operative node executes a distributed transaction in that short duration preceding the failure and uses uncommitted data held by the node that goes on to fail later.<BR>Epoch-based multi-commit avoids, where possible, aborting all transactions and thereby seeks to improve throughput and reduce average latency. Interactions between nodes are monitored in a lightweight, low-overhead manner to determine whether a node needs to abort its transactions, when another node is found to have failed. Interactions that call for aborting of transactions need not just be directly with the failed node but can also be transitive in nature, as explained in the scenario below.<BR>Suppose that node Nk updates an object; these updates do not become durable until Nk commits and will be lost if Nk fails before that. Let Nj process a (distributed) transaction by reading an uncommitted update at NK and updating some local objects which are in turn read by a third Ni while processing another transaction. If Nk crashes, Nj must abort its transactions as some of them involved reading dirty data from Nk; this in turn leads to cascading aborts at Ni even though there is no direct interaction between Ni and Nk.<BR>We will express the pattern of node interactions during each work interval as a symmetric and transitive binary relation between nodes. This relation is called epoch dependency and is defined as follows: Node Ni has epoch dependency with Nj during a given work interval iff:<BR>(i) Ni accesses data from Nj during that work interval or vice versa, or (ii) there is another Nk<BR>for that work interval such that Ni has epoch dependency with Nk and Nk with Nj.<BR>Two remarks are in order. First, the above definition does not distinguish whether a node interaction involves accessing uncommitted or committed data, even though the latter does not call for cascading aborts. Despite some advantages in making this distinction, we avoid it in order to keep the overhead of monitoring node interactions as small as possible.</P>
<P>Secondly, epoch dependency is also reflexive by definition, as each node accesses data from itself. So, it is an equivalence relation defined on participant nodes. It therefore partitions the nodes into disjoint subsets which we call commit groups: each node is in exactly one group, any two nodes of a group are related by epoch dependency, and no node has epoch dependency with any node not in its own group. Therefore, if a commit group has no failed node, then its member nodes can commit during 2PC; otherwise, they must abort. Referring to Figure 6.3, we see nodes N1 and N2 interacting with each other during the work interval and N3 executing no distributed transactions. So, N1 and N2 have epoch<BR>dependency with each other and N3 only with itself; thus, two commit groups, c0 = {N1, N2}<BR>and c1 = {N3}, emerge. If N3 fails, N1 and N2 can commit their transactions because they did not access any data from N3. If N1 fails, N2 &#8712; c0 must abort its transactions, while N3 is unaffected.<BR>A failed node can thus prevent nodes of only one group from committing. That is, if node interactions during a work interval lead to multiple commit groups emerging at the end, then all nodes do not have to abort their transactions in the event of a failure. On the other hand, if only one commit group exists at the end of a work interval, then a node failure will cause all operative nodes to abort, i.e., the epoch-based multi-commit defaults to the original, epoch-based commit. For example, had N1 and/or N2 in Figure 6.3 interacted with<BR>N3 during the work interval, then c0 = {N1, N2, N3} would be the only commit group and any<BR>node failure would mean all other nodes aborting their transactions.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3.2 Motivation: TPC-C Case Study</P>
<P>For multi-commit protocol to minimize aborts, multiple commit groups should emerge at the end of a work interval. Such an outcome depends primarily on three workload characteristics:<BR>(i) proportion of distributed transactions, (ii) average number of remote nodes accessed by distributed transactions, and (iii) node affinity or the likelihood of a transaction in a given</P>
<P>node accessing data in another given node due to correlations between data partitions hosted by the two nodes.<BR>The smaller the proportion in (i), the larger is the likelihood of multiple groups. In the limit, if there are no distributed transactions at all, then each commit group is a singleton with a distinct node - as c1 in Figure 6.3. The smaller the average in (ii) and the stronger the affinity, the more likely it is that multiple groups emerge even if the workload has a higher proportion of distributed transactions. In what follows, we consider a canonical benchmark system to motivate the multi-commit scheme can be very useful in practical settings and its performance is worthy of a detailed evaluation.<BR>TPC-C is the canonical benchmark for evaluating performance of OLTP databases [109]. It models a warehouse order-processing application and consists of five transaction types. Only two types, Payment and NewOrder, involve accessing remote nodes. A Payment transaction involves updating the payment amounts for a given warehouse and then updating customer information. The customer belongs to remote warehouse on another server with a 15% probability. In short, the Payment transaction accesses at most two partitions. A NewOrder transaction updates 5-15 items in the stock table. Of these items, 99% are local to its home partition, while 1% are at a remote partition.</P>
<P>60</P>
<P><BR>40</P>
<P><BR>20</P>
<P><BR>0<BR>0.00</P>
<P>0.05</P>
<P>0.10</P>
<P>0.15<BR>proportion of distributed transactions</P>
<P>Fig. 6.4 Number of commit groups vs proportion of distributed transactions. The red line indicates the threshold after which single commit group is the only outcome.</P>
<P>In our experiment, the epoch size was fixed to 10ms and a throughput of 300K transactions per second in a cluster with 64 servers was assumed. Figure 6.4 depicts the number of commit groups formed when the the proportion of distributed transactions is varied from 1% to 15%. Commit groups are numerous when the proportion of distributed transactions does not exceed 8% which is the typical proportion of distributed transactions encountered in practical settings. Thus, multi-commit is indeed a practical alternative to the original scheme.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3.3 Multi-Commit Protocol Description</P>
<P>This protocol is identical to epoch-based commit described in Section 6.2, except for the following two additions.<BR>Monitoring node interactions. When node Ni executes a distributed transaction and sends a Remote-Op message to another node Nj, it enters the remote Nj in its local epoch- dependency list. Similarly, when Nj receives Remote-Op message from Ni, it enters the latter in its list. Thus, interacting nodes have each other in their epoch-dependency lists.<BR>Computing commit groups. This is done by the coordinator node if some participant node has not responded to it with Prepare-Ack in the prepare phase of 2PC execution. Recall that the coordinator executes 2PC at the end of work interval by sending a Prepare message to all participant nodes. (2PC messages are shown in blue in Figure 6.3.) Each operative node will, in turn, respond to the coordinator by sending a Prepare-Ack message which will now include its epoch-dependency list as indicated in Figure 6.3.<BR>If the coordinator receives Prepare-Ack from all nodes, then a Commit message is sent to all nodes. Otherwise, it constructs a graph where a vertex represents a participant node and an edge represents an epoch-dependency as reported within the epoch-dependency lists it received. Commit groups are then found by using Tarjan&#8217;s algorithm to identify strongly connected components [101]. Participant nodes of commit groups that contain no failed node are sent a Commit message and the rest an Abort message. Based on the descriptions</P>
<P>in Subsection 6.3.1, it is easy to see that a node is sent Commit, if and only if it has not interacted with a failed node directly or transitively, during the work interval.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.4 Performance Evaluation Strategies</P>
<P>The principal aim of simulations is two-fold: to assess the accuracy of the analytical models and explore circumstances in which the epoch-based multi-commit can perform better than the original epoch-based commit. The former will assist database administrators in choosing a suitable work interval, a, to accomplish their performance targets and the latter will help to demonstrate that the multi-commit protocol is a practical alternative to the original. Recall that multi-commit cannot perform worse than the original in any circumstance; this is because the former differs from the latter only when a node fails at which time the coordinator expends a small computational cost on trying to minimize aborts.<BR>In assessing the efficacy of the expression for maximum throughput, our discrete event- based simulations [78] will follow the experimental setup in [72]: nodes will spawn a new transaction as soon as they finish executing the current one. Thus, the nodes are never idle and the resulting throughput will be the maximum attainable. In measuring the average latency, simulations will consider such values for transaction arrival rates that the system is kept in a steady state; i.e., the number of transactions waiting to be processed will not grow monotonically with time. Under this steady state condition, throughput is the same as the arrival rate and hence not measured.<BR>In our simulations, incoming transactions are distributed ones with 10% probability which is larger than the threshold 8% observed in Figure 6.4 for having multiple commit groups at the end of a work interval. Thus, we seek to explore the effects of node-affinity when the proportion of distributed transactions does not favor the emergence of multiple groups.<BR>A distributed transaction interacts with one remote node and we consider two policies for choosing that remote node: random and paired affinity. In the former, the remote node is<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.4 Performance Evaluation Strategies&nbsp;173</P>
<P>randomly chosen; in the latter, nodes are paired and a distributed transaction originating in a given node accesses the paired node with 90% probability and a randomly chosen one with 10% probability. Node-pairing captures data correlations between the partitions hosted by the paired nodes.<BR>Note that if a remote node chosen for data access is crashed, then processing of that transaction ceases and all effects of having processed it are undone. Such a transaction is called &#8216;dropped&#8217; in Subsection 6.2.3 and not counted in throughput. Also, 10% of transactions accessing one remote node sets E(&#954;) = 0.1 in (Equation (6.12)).<BR>Table 6.1 Parameters of the analytical models and simulation.</P>
<P>Symbol<BR>Meaning<BR>Values<BR>N<BR>Number of participant nodes<BR>64<BR>a<BR>Work interval (ms)<BR>4-1800<BR>b<BR>Mean time to commit (ms)<BR>1.7<BR>&#181;<BR>Node&#8217;s transaction service rate (txn/ms)<BR>1<BR>1/&#958;<BR>Mean time between failure (hr)<BR>12<BR>1/&#951;<BR>Mean time to repair (min)<BR>30<BR>E(&#954;)<BR>Remote servers accessed by transactions<BR>0.1<BR>&#955; &#8224;<BR>Transaction arrival rate (txn/s)<BR>30000,40000<BR>&#8224; Average response time model only.</P>
<P>The parameters of the analytical models and their values used in simulations are summa- rized in Table 6.1. The selected values were chosen in a way to be representative of a practical OLTP database configuration and workload. A cluster size of 64 nodes and one coordinator is similar to that used in the experimental analysis of concurrency control protocols in [54]. The choice of &#181; = 1 is guided by the fact that OLTP transactions&#8217; useful work consumes about one millisecond and they seldom have user stalls, rather they are executed as stored procedures [97]. The mean time to execute 2PC is represented by b. To estimate b, it was assumed a disk flush takes 10&#181;s and database nodes are co-located within the same datacenter with a round trip time of 1ms. Thus, as 2PC operations involve 2 sequential disk writes and<BR>1.5 network calls before results are released back to clients, b is set to 1.7ms. Following [48],</P>
<P>the mean time between failure 1/&#958; and the mean time to repair 1/&#951; are taken to be 12 hours and 30 minutes respectively.<BR>Given that N = 64, it is possible to encounter more than one failed node in simulations<BR>even though 1/&#958; &#8811; 1/&#951;. (Note: the larger the value of N, the less likely it is for Assumption<BR>1 to hold.) Thus, any loss of accuracy due to Assumption 1 in Subsection 6.2.2 is assessed. Simulations measure the following metrics:<BR>System throughput (T ): Number of transactions committed per second.<BR>Lost transaction rate (D): Rate at which transactions are being aborted or dropped due to failures.<BR>Committed transactions during failures (CT f ): Average number of transactions committed in cycles with failures.<BR>Average Response time (W): The response time is measured from the point when a transac- tion enters the system, to the point when it departs, after potentially several retries. Since transactions are processed in batches and some may not be committed in their first execution, the average value (in ms) is computed over the simulation period.<BR>Operational commit groups (CG): the number of commit groups that do not contain a failed node, given that node failure has occurred in a given cycle. It is zero for the epoch-based commit where nodes always form one single commit group.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5 Evaluation<BR>Each simulation run took approximately 12 hours to complete and simulated a cluster operational period of 100 days in order to have thousands of cycles with node failures. (Note: Experiments in [72] did not study the impact of node failures.) For example, when a = 40 ms, 10,972 cycles had failures out of a total of 207 million cycles simulated. So, the number of operational commit groups reported here would be an average of at least 10,000 values obtained. We observed up to 10 cycles having multiple node failures when a = 1800 ms.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>model&nbsp;multi&nbsp;original</P>
<P>&nbsp;</P>
<P>61.0 K</P>
<P>&nbsp;</P>
<P>60.0 K</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 59.0 K</P>
<P><BR>0&nbsp;250&nbsp;500&nbsp;750&nbsp;1000&nbsp;1250&nbsp;1500&nbsp;1750<BR>work interval (a)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) System throughput vs. a in ms.</P>
<P>multi&nbsp;original</P>
<P>0.15</P>
<P>&nbsp;</P>
<P>0.10</P>
<P>&nbsp;</P>
<P>0.05</P>
<P>&nbsp;</P>
<P>0.00<BR>0</P>
<P><BR>250</P>
<P><BR>500</P>
<P><BR>750</P>
<P><BR>1000</P>
<P><BR>1250</P>
<P><BR>1500</P>
<P><BR>1750<BR>work interval (a)<BR>&nbsp;&nbsp;&nbsp; (b) Operational commit groups vs. a in ms.</P>
<P>Fig. 6.5 Maximum throughput as work interval a varied from 40 to 1800 ms.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5.1 Maximum Throughput</P>
<P>Figure 6.5(a) plots the maximum attainable throughput values against the work interval a which is varied from 40 to 1800 ms. Throughput estimated using the expression in Sub- section 6.2.3 is referred to as &#8216;model&#8217; and those measured in simulations for epoch-based commit and epoch-based multi-commit are labeled as &#8216;original&#8217; and &#8216;multi&#8217; respectively. Simulations used the random assignment policy when a distributed transaction sought to interact with a remote node.<BR>We can make three observations from Figure 6.5(a). First, the estimated throughput very closely tracks the simulated values at all a. In fact, the maximum difference ever observed was around 3.8%. This suggests that Assumptions 1 and 2 of Subsection 6.2.2 have a negligible impact on the accuracy and the analytical model is nearly exact.</P>
<P>model</P>
<P><BR>61.0 K</P>
<P><BR>60.0 K</P>
<P><BR>59.0 K</P>
<P>0&nbsp;250&nbsp;500&nbsp;750&nbsp;1000 1250&nbsp;&nbsp; 1500&nbsp;&nbsp; 1750<BR>work interval (a)</P>
<P>Fig. 6.6 System throughput estimates vs. a. Green dotted lines indicate regions with different gradients.</P>
<P>Secondly, throughput values of both protocols are nearly identical. This is explained by Figure 6.5(b) that presents the number of operative commit groups (CG) formed in cycles with node failures. CG takes the maximum value of just 0.15 and rapidly falls as a increases. Such insignificantly small values of CG in multi-commit are due to the proportion of distributed transactions (10%) in the workload and the random policy employed for choosing the node</P>
<P>to interact with. These two factors lead to almost all operative nodes interacting, directly or indirectly, with the failed node by the time the work interval a completes. This effect is more pronounced for larger a values. Consequently, multi-commit cannot perform significantly better than the original when nodes fail. Moreover, even this minute performance advantage of multi-commit during cycles with failures nearly vanishes when average throughput is taken over all cycles, because failure-free cycles far out-number those with failures. (Recall, when there are no failures, multi-commit performance is identical to the original.)<BR>Finally, the analytical expression of Subsection 6.2.3 can be reliably used in choosing appropriate aT when maximum throughput is the primary concern. For convenience, Fig- ure 6.5(a) is reproduced in Figure 6.6 without simulated throughput values so that throughput<BR>estimates for various a are clear. Referring to Figure 6.6, we observe that throughput does not decrease until a = 1500 ms; thus, optimal a&#8727;T&nbsp; is 1500 ms. For some workloads, a work interval around 1500 ms will offer unsatisfactory latency and be unacceptable. Thus, finding<BR>a smaller aT that still offers an acceptable maximum throughput may be desirable and can be guided by the observation that increasing a need not fetch a proportional increase in throughput. Figure 6.6 shows four distinct regions where the rate of throughput increase is markedly different: the gradient is very large, fairly large, small and very small when a &#8712; [40, 100), a &#8712; [100, 300), a &#8712; [300, 500) and a &#8712; [500, 1500] ms respectively.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5.2 Average Response Time<BR>Our second set of experiments focuses on assessing the effectiveness of the average response time analytical models in Subsections 6.2.5 and 6.2.6. Simulations retain the random assignment policy for distributed transactions; transaction arrival rate per second is taken to be &#955; = 30, 000 which is approximately 90% of the maximum throughput when a = 5ms to ensure the system is in a steady state. Figure 6.7 plots the model estimates of the lower (Wd) and upper(Wu) bounds and the simulation values measured for the epoch-based commit</P>
<P><BR>original&nbsp;Wd&nbsp;Wu</P>
<P>40</P>
<P><BR>30</P>
<P><BR>20</P>
<P><BR>10</P>
<P>4&nbsp;8&nbsp;12&nbsp;16&nbsp;20<BR>work interval (a)</P>
<P>Fig. 6.7 Average response time (ms) in epoch-based commit vs. a in ms.</P>
<P>protocol as a is varied from 4 to 20 ms. (The range choice for a is guided by [72] where experiments used a = 10 ms.)<BR>The response times measured in simulations are well within the upper and lower bound<BR>estimates. The latter increase linearly with a as expected. The Wu plot predicts the optimum a for minimizing the average response time as: a&#8727; = 6 ms which is consistent with simulations. Though the analytical expression for Wu (Subsection 6.2.5, Equation (6.27)) identifies a&#8727; reasonably accurately, the actual response times are much closer to Wd as a increases and the maximum difference we observed was 6 ms. So, to summarize, analytical expressions for Wu and Wd are reasonably accurate in predicting a&#8727; and the actual response times for a &#8805; a&#8727; respectively. The simulation response times obtained for multi-commit were very close to those presented for the original for reasons explained in Subsection 6.5.1 and hence they are not shown.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.5.3 Paired Affinity</P>
<P>We ran the maximum throughput simulation experiment using the paired affinity selection policy for distributed transactions. Figure 6.8(a) displays the number of operational commit groups (CG) formed whenever failures occurred in a cycle. In sharp contrast to Figure 6.5(b),</P>
<P><BR>multi&nbsp;original</P>
<P><BR>30</P>
<P><BR>20</P>
<P><BR>10</P>
<P><BR>0<BR>25&nbsp;50&nbsp;75<BR>work interval (a)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Commit groups vs. a.</P>
<P><BR>100</P>
<P>multi&nbsp;original</P>
<P><BR>40</P>
<P>30</P>
<P>20</P>
<P>10</P>
<P>0<BR>25&nbsp;50&nbsp;75<BR>work interval (a)</P>
<P>100<BR>(b) Lost transaction rate vs. a in ms.</P>
<P>multi&nbsp;original</P>
<P>4000</P>
<P>3000</P>
<P>2000</P>
<P>1000</P>
<P>0<BR>25&nbsp;50&nbsp;75<BR>work interval (a)</P>
<P>100<BR>(c) Transactions committed in cycles with node failure CTf vs. a.</P>
<P>Fig. 6.8 Simulations under paired-affinity as work interval a varied from 10 to 100 ms.</P>
<P>CG for multi-commit starts at a much larger value of 35 when a = 10 ms and falls steadily<BR>to 5 as a increases to 100 ms. This suggests a strong potential for reducing the number of aborts when failures occur.<BR>Figure 6.8(b) plots the lost transaction rates (D) for both protocols and shows that D for multi-commit is consistently smaller than the original. For small a, the difference is small, because the number of transactions lost due to failures is small for both protocols. But as<BR>a increases, D for the original increases almost linearly while that for multi-commit does not start increasing significantly until a = 50 ms; at that point, multi-commit shows 83% reduction in D with the corresponding CG being around 17 in Figure 6.8(a). As a increases<BR>further, CG for multi-commit starts dropping significantly in Figure 6.8(a) and consequently<BR>D for multi-commit starts increasing rapidly.<BR>Finally, Figure 6.8(c) shows the average number of transactions committed by the pro- tocols in those cycles where node failures occur (CTf ). From Figure 6.8(c) it is clear that multi-commit avoids a significant number of aborts; note that CTf = 0 in the original. How- ever, these differences in CTf make insignificant difference when throughput and latency are averaged over the simulation period, because cycles without failures far outnumber (by four orders of magnitude) those with failures and both protocols perform identically in fail-free cycles. Thus, the average maximum throughput and average latency for both protocols, plotted in Figures 6.9(a) and 6.10(a), are almost identical. This implies that the analytical expressions obtained for epoch-based commit under the random policy have a wide applicability: comparing Figures 6.9(a) and 6.10(b) with Figures 6.9(b) and 6.7 respectively suggests that those expressions are equally applicable for (i) obtaining appropriate a for epoch-based multi-commit, and (ii) epoch-based protocols under paired affinity policy.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>multi&nbsp;original</P>
<P><BR>60.0 K</P>
<P><BR>58.0 K</P>
<P><BR>56.0 K</P>
<P><BR>54.0 K</P>
<P><BR>52.0 K</P>
<P><BR>25&nbsp;50&nbsp;75<BR>work interval (a)</P>
<P><BR>100<BR>&nbsp;&nbsp;&nbsp; (a) Simulations using paired affinity.</P>
<P>model</P>
<P><BR>60.0 K</P>
<P><BR>58.0 K</P>
<P><BR>56.0 K</P>
<P><BR>54.0 K</P>
<P><BR>52.0 K</P>
<P><BR>25&nbsp;50&nbsp;75<BR>work interval (a)</P>
<P><BR>100<BR>(b) Model using random affinity.</P>
<P>Fig. 6.9 Maximum throughput as work interval a varied from 10 to 100 ms.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>multi&nbsp;original</P>
<P>60</P>
<P>50</P>
<P>40</P>
<P>30</P>
<P>20</P>
<P>25&nbsp;50&nbsp;75&nbsp;100<BR>work interval (a)<BR>&nbsp;&nbsp;&nbsp; (a) Simulations with paired affinity as a is varied from 10 to 100 ms (&#955; = 40, 000).</P>
<P>multi&nbsp;Wd&nbsp;Wu</P>
<P>40</P>
<P><BR>30</P>
<P><BR>20</P>
<P><BR>10</P>
<P><BR>4&nbsp;8&nbsp;12&nbsp;16&nbsp;20<BR>work interval (a)<BR>&nbsp;&nbsp;&nbsp; (b) Multi-commit with paired affinity and models with random affinity as a is varied from 4 to 20 ms (&#955; = 30, 000).</P>
<P>Fig. 6.10 Average response time (ms).</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.6 Conclusion</P>
<P>In this chapter two analytical models for the epoch-based commit protocol have been devel- oped which allow database operators to maximize throughput, minimize average response time, or seek a trade-off between them. The accuracy of these models has been validated through a simulation study that considered a cluster of 64 nodes operating for 100 days. We also developed epoch-based multi-commit, which aims to minimize transaction aborts in the event of node failures, but performs identically to the original version under other circumstances. Our simulation study affirms that multi-commit performs better when dis- tributed transactions originating at a given node tend to access specific other nodes in their remote interactions. When failures are rare, the analytical expressions derived for the original protocol can also be used in determining the right epoch intervals for the multi-commit version as well. Thus, we offer a practical alternative to epoch-based commit and analytical solutions to efficiently tune the parameter of epoch-based commit protocols in practical settings.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.6.1 Further Work</P>
<P>Dynamic work intervals. For a variety of reasons, users often operate their databases at a near full load. So when a failure occurs they can experience a significant spike in transactions&#8217; average latency whilst the problem is resolved. However, these systems seldom implement an appropriate back-pressure mechanism, i.e., the rate of new transactions into the work queue remains unchanged. As a consequence, the system takes a long time to return to the average latency prior to the failure event (if plotted, average latency would follow a short spike followed by a slow decline to normal behaviour). This is unaligned with user expectations: they desire a return to &#8220;normal&#8221; behaviour swiftly. A potential avenue for future work is to explore the efficacy of dynamically adjusting the size of the work interval in response to the occupancy levels of the external work queue. Such a mechanism could</P>
<P>prove useful in smoothing average latency in response to failures. Additionally, workloads vary over time due to a number of factors. An example of this is demonstrated in the LDBC Interactive workload [37], which features spiking trends, increased activity in the social network in response to real world events, e.g., elections and natural disasters. Thus, dynamic epoch-based commit may be also of practical use during normal operation.</P>
<P>Decentralized epoch-based multi-commit. A drawback of epoch-based commit is its sensitivity to imbalanced workloads, a straggler node processing a long-running transaction can prevent the whole epoch from committing, as a result increasing the average latency of all transactions. Future work could explore how to decentralize the coordinator node across the cluster. In a decentralized iteration of the protocol, each participant node would manage its own local epoch that it periodically attempts to increment. In effect, different nodes would move through epochs at different rates. Multi-commit&#8217;s epoch dependency tracking mechanism would remain the same, with the difference that now when a node tries to increment its local epoch counter it must determine its epoch commit group and elect a leader to manage the commitment of said epoch. This could be achieved by a simple approach such as the node with the highest process id being elected leader. Note, due to the transitive nature of epoch dependencies, leader election could take a long time as nodes discover the members of the commit group. This approach would allow nodes that have not interacted with any straggler nodes processing long-running transactions to continue to make forward progress, which should lower the average latency of transactions.</P>
<P>Geo-distributed epoch-based multi-commit. Geo-distributed databases replicate data across the world to keep data close to users, for fault-tolerance, and for regulatory require- ments, e.g., GDPR. Discussions in this chapter thus far have been restricted to deployment in a single datacenter. Another area for further work is exploring how multi-commit could be used in a geo-distributed database. Consider a company that operates in three jurisdictions,</P>
<P>US, Germany, and Japan. The company&#8217;s customer data is partitioned to reflect users in each area, i.e., partitions contain data solely from one location. These partitions are then replicated across three data-centers, one in each location. One anticipates that the majority of transactions would access data for a single location. Therefore, an epoch coordinator could be deployed in each datacenter, primarily responsible for coordinating commit over the partitions holding data for that location. If transactions access data from multiple locations then some degree of coordination would be needed between each location&#8217;s coordinators in a similar vein to decentralized multi-commit.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><BR>Chapter 7 Conclusions</P>
<P>Summary<BR>This chapter summarizes the work presented in this thesis, addressing the inefficiencies and limitations of existing research in database concurrency control and distributed atomic commitment. The contributions and limitations of these works are outlined, before we discuss open challenges in the area.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.1 Thesis Summary</P>
<P>In this thesis we have explored concurrency control in the context of many-core and dis- tributed OLTP databases. Additionally, we have investigated how the limitations of existing work on distributed atomic commitment can be addressed.<BR>In Chapter 3, we developed the wait-hit protocol, an optimistic Serializable concurrency control protocol that scales vertically, with the core count, and horizontally, with the cluster size. The motivation behind this work was an attempt to create a concurrency control protocol catered for modern cloud environments that does not need to be re-adapted for different scale points, thus is better suitable to an environment in which users can easily scale from a many-core machine, to a deployment spread across data centers. We demonstrated that the Wait-Hit Protocol has comparable performance with the state-of-the-art many-core concurrency control protocols.<BR>Next in Chapter 4 we focused on efficient implementation of weak isolation on servers with many-cores. Mixed Serialization Graph Testing minimizes the number of unnecessary aborts, and crucially permits concurrent transactions to be executed at a range of isolation lev- els. The performance of MSGT is demonstrated using several popular transaction processing benchmarks.<BR>Chapter 5 focuses on concurrency control in specialized distributed graph databases, specifically the design of two protocols that preserve Reciprocal Consistency and one which<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.2 Limitations&nbsp;189</P>
<P>preserves Edge-Order Consistency are presented. The Delta protocol provides probabilistic guarantees of data integrity, whereas Deterministic Reciprocal Consistency Protocol and Deterministic Edge Consistency Protocol ensure there is no chance of inconsistencies. De- terministic Edge Consistency Protocol builds on the same principles as those presented in Chapter 3. Approximate models were developed for each protocol to allow for a compre- hensive performance evaluation.<BR>Lastly, in Chapter 6 we investigated epoch-based distributed databases and developed two analytical models of epoch-based commit. These models facilitate the choosing of an epoch size that maximizes throughput, minimizes average response time, or seeks a trade-off between them. We also developed epoch-based multi-commit which can minimize transaction aborts in the event of node failures depending on various workload characteristics.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.2 Limitations</P>
<P>The lack of performance evaluation of the Distributed Wait-Hit Protocol, either through mod- eling, simulations, or implementation, is an important limitation of this thesis as it prevents us from truly ascertaining the performance benefits compared to Distributed Serialization Graph Testing. However, in the future a full implementation of Distributed Wait-Hit Protocol is planned and will allow for comparison with a range of protocols, similar to the study executed in [54].<BR>A key limitation of the work performed in Chapter 4 is the lack of comparison with other mixed concurrency protocols such as Mixed-2PL. Additionally, there still remains a question mark over the practical utility of such a protocol and of weak isolation in general [99].<BR>Another limitation of the work conducted in the thesis is concerned with the protocols developed in Chapter 5. All three protocols are concerned only with maintaining the consis- tency of one class of objects, edges, however, any practical graph databases must preserve</P>
<P>the integrity of nodes, labels, and properties. It remains unclear how the protocols developed in Chapter 5 could be efficiently integrated into a wider concurrency control protocol.<BR>Additionally, the work on the Distributed Wait-Hit Protocol in Chapter 3 and the edge consistency protocols in Chapter 5 do not consider replication. Any practical system typically employs some level of replication to enable fault-tolerance or availability. It is unclear how these protocols would be efficiently integrated into a replicated DBMS.<BR>Lastly, the benefits of epoch-based multi-commit appear limited to specific situations. Additionally, the work in Chapter 6 does not address other shortcomings of epoch-based commit, such as increased latency owing to stragglers and/or unbalanced workloads.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7.3 Future Research Directions</P>
<P>In this section, future areas of research are outlined arising from the lessons learnt throughout the PhD.</P>
<P>Additional Implementation and Experiments Extending our evaluation framework de- scribed in Chapter 2 would allow for the implementation of distributed concurrency control protocols. Therefore, the performance of the (i) Distributed Wait-Hit Protocol, (ii) Delta protocol, (iii) Deterministic Reciprocal Consistency Protocol, and (iv) Deterministic Edge Consistency Protocol could be empirically evaluated and the results used to corroborate the conclusions drawn in the thesis. Additionally, it would be useful to expose each protocol developed in this thesis to a wider range of workloads, such as, TPC-C [109] or RAMP-TAO [18].</P>
<P>Comparing Distributed Transactions Approaches One observation made when conduct- ing the work in the thesis was the challenge of comparing different distributed transaction protocols. There are several obvious dimensions on which protocols differ, e.g., isolation<BR>7.3 Future Research Directions&nbsp;191</P>
<P>level provided, but across protocols that provide the same guarantees it is hard to disam- biguate between what can be attributed to superior protocol design, and what is merely a better implementation. This is not helped by the partial evaluation of the breadth of protocols in most studies, with each typically comparing a subset of existing approaches with their &#8220;new&#8221; approach. The challenge is compounded as different papers use different hardware and system configurations, it truly is oranges vs apples comparison.<BR>Therefore, an interesting area of future research would be developing a framework to reason about the space of distributed transaction protocols. This would help better understand the trade-offs of each design decision and help to better quantify the magnitude of the performance gain each optimization could have. For example, if the read/write access set of a workload can be known, how much will the throughput and latency improve by if I factor this into my protocol design.<BR>Such a framework would allow database designers to compose protocols from the set of optimizations bespoke for their workload or data model, which could lead to the discovery of several interesting new distributed transaction protocols.</P>
<P>Understanding Graph Transactions and Workloads&nbsp;Transactions are important in graph databases, however determining what consistency requirements they need provide to avoid violating graph integrity remains an open question and an interesting area for future research. Read Atomic isolation [9] was found suitable for TAO [18], but it is unclear how gener- alizable this is; Sortledon, a recent transactional graph data structure provides Serializable isolation [47]. The uncertainty here can be largely attributed to the lack of publicly available graph transactional workloads which inhibits workload-driven protocol design, existing<BR>transactional workloads, e.g., LDBC Interactive [37], only include insert operations.<BR>Another interesting direction is incorporating awareness of application-level invariants and types of common graph operations into the protocols to determine which can be preserved without coordination, e.g., concurrent edge insertion between two nodes is commutative, but</P>
<P>concurrent node deletion and edge insertion requires coordination. Extending the CRDT developed in [10] to a partitioned graph database is an appealing direction.</P>
<P>Epoch-based Databases Epoch-based databases are a promising direction for achieving high performance distributed transactions, but current designs have several drawbacks: (i) they are sensitive to imbalanced workloads, one long-running transaction can block the complete epoch, (ii) they are sensitive to straggler nodes, one node slow to reply to a Prepare message can increase latency for all transactions, (iii) all transactions executed in an epoch are aborted if one nodes fails, leading to a lot of wasted work, and (iv) retrying the work of a failed epoch could result in a metastable failure [57] if users are operating their databases at a near full load.<BR>One direction for ameliorating drawbacks (i) and (ii) is decentralizing the coordinator node across the cluster, this would allow database nodes that have not interacted with any straggler nodes or ones processing long-running transactions to continue to potentially make forward progress. Case (iii) was addressed in Chapter 6 through epoch-based multi-commit. As regards case (iv), dynamically adjusting the size of the epoch in response to the occupancy levels of the external work queue could help smooth average latency in response to failures.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>References</P>
<P>&nbsp;&nbsp;&nbsp; [1] Adya, A. (1999). Weak Consistency: A Generalized Theory and Optimistic Implementa- tions for Distributed Transactions. PhD Thesis.<BR>&nbsp;&nbsp;&nbsp; [2] Adya, A., Liskov, B., and O&#8217;Neil, P. E. (2000). Generalized isolation level definitions. In Lomet, D. B. and Weikum, G., editors, Proceedings of the 16th International Conference on Data Engineering, San Diego, California, USA, February 28 - March 3, 2000, pages 67&#8211;78. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [3] Alomari, M., Cahill, M. J., Fekete, A. D., and R&#246;hm, U. (2008). The cost of serializability on platforms that use snapshot isolation. In Alonso, G., Blakeley, J. A., and Chen, A. L. P., editors, Proceedings of the 24th International Conference on Data Engineering, ICDE 2008, April 7-12, 2008, Canc&#250;n, Mexico, pages 576&#8211;585. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [4] Angles, R., Antal, J. B., Averbuch, A., Boncz, P. A., Erling, O., Gubichev, A., Haprian, V., Kaufmann, M., Larriba-Pey, J., Mart&#237;nez-Bazan, N., Marton, J., Paradies, M., Pham, M., Prat-P&#233;rez, A., Spasic, M., Steer, B. A., Sz&#225;rnyas, G., and Waudby, J. (2020). The LDBC social network benchmark. CoRR, abs/2001.02299.<BR>&nbsp;&nbsp;&nbsp; [5] Angles, R., Arenas, M., Barcel&#243;, P., Boncz, P. A., Fletcher, G. H. L., Guti&#233;rrez, C., Lindaaker, T., Paradies, M., Plantikow, S., Sequeda, J. F., van Rest, O., and Voigt, H. (2018). G-CORE: A core for future graph query languages. In Das, G., Jermaine,<BR>C. M., and Bernstein, P. A., editors, Proceedings of the 2018 International Conference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018, pages 1421&#8211;1432. ACM.<BR>&nbsp;&nbsp;&nbsp; [6] Apache Cassandra (2019). Apache cassandra. <A href="http://cassandra.apache.org/">http://cassandra.apache.org/</A>. (Accessed on 11/12/2019).<BR>&nbsp;&nbsp;&nbsp; [7] Bailey, N. T. J. (1954). On queueing processes with bulk service. Journal of the Royal Statistical Society. Series B (Methodological), 16(1):80&#8211;87.<BR>&nbsp;&nbsp;&nbsp; [8] Bailis, P. et al. (2013). Highly available transactions: Virtues and limitations. VLDB.<BR>&nbsp;&nbsp;&nbsp; [9] Bailis, P., Fekete, A., Ghodsi, A., Hellerstein, J. M., and Stoica, I. (2016). Scalable Atomic Visibility with RAMP Transactions. ACM Trans. Database Syst., 41(3):15:1&#8211; 15:45.<BR>&nbsp;&nbsp;&nbsp; [10] Balegas, V., Duarte, S., Ferreira, C., Rodrigues, R., and Pregui&#231;a, N. M. (2018). IPA: invariant-preserving applications for weakly consistent replicated databases. Proc. VLDB Endow., 12(4):404&#8211;418.</P>
<P>&nbsp;&nbsp;&nbsp; [11] Berenson, H., Bernstein, P. A., Gray, J., Melton, J., O&#8217;Neil, E. J., and O&#8217;Neil, P. E. (1995). A Critique of ANSI SQL Isolation Levels. In Carey, M. J. and Schneider, D. A., editors, Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, San Jose, California, USA, May 22-25, 1995, pages 1&#8211;10. ACM Press.<BR>&nbsp;&nbsp;&nbsp; [12] Bernstein, P. A., Hadzilacos, V., and Goodman, N. (1987). Concurrency Control and Recovery in Database Systems. Addison-Wesley.<BR>&nbsp;&nbsp;&nbsp; [13] Besta, M. et al. (2019a). Practice of streaming and dynamic graphs: Concepts, models, systems, and parallelism. CoRR, abs/1912.12740.<BR>&nbsp;&nbsp;&nbsp; [14] Besta, M., Peter, E., Gerstenberger, R., Fischer, M., Podstawski, M., Barthels, C., Alonso, G., and Hoefler, T. (2019b). Demystifying graph databases: Analysis and taxonomy of data organization, system designs, and graph queries. CoRR, abs/1910.09017.<BR>&nbsp;&nbsp;&nbsp; [15] Birke, R., Giurgiu, I., Chen, L. Y., Wiesmann, D., and Engbersen, T. (2014). Failure analysis of virtual and physical machines: Patterns, causes and characteristics. In 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2014, Atlanta, GA, USA, June 23-26, 2014, pages 1&#8211;12. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [16] Bosshart, P., Daly, D., Gibb, G., Izzard, M., McKeown, N., Rexford, J., Schlesinger, C., Talayco, D., Vahdat, A., Varghese, G., and Walker, D. (2014). P4: programming protocol-independent packet processors. Comput. Commun. Rev., 44(3):87&#8211;95.<BR>&nbsp;&nbsp;&nbsp; [17] Cheng, A., Shi, X., Kabcenell, A. N., Lawande, S., Qadeer, H., Chan, J., Tin, H., Zhao, R., Bailis, P., Balakrishnan, M., Bronson, N., Crooks, N., and Stoica, I. (2022). Taobench: An end-to-end benchmark for social networking workloads. Proc. VLDB Endow., 15(9):1965&#8211;1977.<BR>&nbsp;&nbsp;&nbsp; [18] Cheng, A., Shi, X., Pan, L., Simpson, A., Wheaton, N., Lawande, S., Bronson, N., Bailis, P., Crooks, N., and Stoica, I. (2021). RAMP-TAO: layering atomic transactions on facebook&#8217;s online TAO data store. Proc. VLDB Endow., 14(12):3014&#8211;3027.<BR>&nbsp;&nbsp;&nbsp; [19] Cheng, R., Hong, J., Kyrola, A., Miao, Y., Weng, X., Wu, M., Yang, F., Zhou, L., Zhao, F., and Chen, E. (2012). Kineograph: taking the pulse of a fast-changing and connected world. In Felber, P., Bellosa, F., and Bos, H., editors, European Conference on Computer Systems, Proceedings of the Seventh EuroSys Conference 2012, EuroSys &#8217;12, Bern, Switzerland, April 10-13, 2012, pages 85&#8211;98. ACM.<BR>&nbsp;&nbsp;&nbsp; [20] Cooper, B. F., Silberstein, A., Tam, E., Ramakrishnan, R., and Sears, R. (2010). Benchmarking cloud serving systems with YCSB. In Hellerstein, J. M., Chaudhuri, S., and Rosenblum, M., editors, Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC 2010, Indianapolis, Indiana, USA, June 10-11, 2010, pages 143&#8211;154. ACM.<BR>&nbsp;&nbsp;&nbsp; [21] Crooks, N., Burke, M., Cecchetti, E., Harel, S., Agarwal, R., and Alvisi, L. (2018). Obladi: Oblivious serializable transactions in the cloud. In Arpaci-Dusseau, A. C. and Voelker, G., editors, 13th USENIX Symposium on Operating Systems Design and Imple- mentation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018, pages 727&#8211;743. USENIX Association.</P>
<P>&nbsp;&nbsp;&nbsp; [22] Crooks, N., Pu, Y., Alvisi, L., and Clement, A. (2017). Seeing is Believing: A Client- Centric Specification of Database Isolation. In Schiller, E. M. and Schwarzmann, A. A., editors, Proceedings of the ACM Symposium on Principles of Distributed Computing, PODC 2017, Washington, DC, USA, July 25-27, 2017, pages 73&#8211;82. ACM.<BR>&nbsp;&nbsp;&nbsp; [23] Curino, C., Zhang, Y., Jones, E. P. C., and Madden, S. (2010). Schism: a workload- driven approach to database replication and partitioning. Proc. VLDB Endow., 3(1):48&#8211;57.<BR>&nbsp;&nbsp;&nbsp; [24] Das, S., Agrawal, D., and Abbadi, A. E. (2010). G-store: a scalable data store for transactional multi key access in the cloud. In Hellerstein, J. M., Chaudhuri, S., and Rosenblum, M., editors, Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC 2010, Indianapolis, Indiana, USA, June 10-11, 2010, pages 163&#8211;174. ACM.<BR>&nbsp;&nbsp;&nbsp; [25] Dashti, M., John, S. B., Shaikhha, A., and Koch, C. (2017). Transaction repair for multi- version concurrency control. In Salihoglu, S., Zhou, W., Chirkova, R., Yang, J., and Suciu, D., editors, Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, pages 235&#8211;250. ACM.<BR>&nbsp;&nbsp;&nbsp; [26] DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., Sivasubramanian, S., Vosshall, P., and Vogels, W. (2007). Dynamo: amazon&#8217;s highly available key-value store. In Bressoud, T. C. and Kaashoek, M. F., editors, Proceedings of the 21st ACM Symposium on Operating Systems Principles 2007, SOSP 2007, Stevenson, Washington, USA, October 14-17, 2007, pages 205&#8211;220. ACM.<BR>&nbsp;&nbsp;&nbsp; [27] Deutsch, A., Francis, N., Green, A., Hare, K., Li, B., Libkin, L., Lindaaker, T., Marsault, V., Martens, W., Michels, J., Murlak, F., Plantikow, S., Selmer, P., Voigt, H., van Rest, O., Vrgoc, D., Wu, M., and Zemke, F. (2021). Graph pattern matching in GQL and SQL/PGQ. CoRR, abs/2112.06217.<BR>&nbsp;&nbsp;&nbsp; [28] DeWitt, D. J., Katz, R. H., Olken, F., Shapiro, L. D., Stonebraker, M., and Wood, D. A. (1984). Implementation techniques for main memory database systems. In Yormark, B., editor, SIGMOD&#8217;84, Proceedings of Annual Meeting, Boston, Massachusetts, USA, June 18-21, 1984, pages 1&#8211;8. ACM Press.<BR>&nbsp;&nbsp;&nbsp; [29] Dgraph (2021). Dgraph. <A href="https://www.dgraph.io/">https://www.dgraph.io/</A>. (Accessed on 16/12/2021).<BR>&nbsp;&nbsp;&nbsp; [30] Dhulipala, L., Blelloch, G. E., and Shun, J. (2019). Low-latency graph streaming using compressed purely-functional trees. In McKinley, K. S. and Fisher, K., editors, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, pages 918&#8211;934. ACM.<BR>&nbsp;&nbsp;&nbsp; [31] Difallah, D. E., Pavlo, A., Curino, C., and Cudr&#233;-Mauroux, P. (2013). Oltp-bench: An extensible testbed for benchmarking relational databases. Proc. VLDB Endow., 7(4):277&#8211; 288.<BR>&nbsp;&nbsp;&nbsp; [32] Ding, B., Kot, L., and Gehrke, J. (2018). Improving optimistic concurrency control through transaction batching and operation reordering. Proc. VLDB Endow., 12(2):169&#8211; 182.</P>
<P>&nbsp;&nbsp;&nbsp; [33] Du, J., Elnikety, S., and Zwaenepoel, W. (2013). Clock-si: Snapshot isolation for partitioned data stores using loosely synchronized clocks. In IEEE 32nd Symposium on Reliable Distributed Systems, SRDS 2013, Braga, Portugal, 1-3 October 2013, pages 173&#8211;184. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [34] Durner, D. and Neumann, T. (2019). No false negatives: Accepting all useful schedules in a fast serializable many-core system. In 35th IEEE International Conference on Data Engineering, ICDE 2019, Macao, China, April 8-11, 2019, pages 734&#8211;745. IEEE.<BR>&nbsp;&nbsp;&nbsp; [35] Eifrem, E. (2016). Graph databases: The key to foolproof fraud detection? Computer Fraud &amp; Security, 2016:5&#8211;8.<BR>&nbsp;&nbsp;&nbsp; [36] Elnikety, S., Zwaenepoel, W., and Pedone, F. (2005). Database replication using generalized snapshot isolation. In 24th IEEE Symposium on Reliable Distributed Systems (SRDS 2005),26-28 October 2005, Orlando, FL, USA, pages 73&#8211;84. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [37] Erling, O. et al. (2015). The LDBC Social Network Benchmark: Interactive workload. In SIGMOD, pages 619&#8211;630. ACM.<BR>&nbsp;&nbsp;&nbsp; [38] Escriva, R., Wong, B., and Sirer, E. G. (2015). Warp: Lightweight multi-key transactions for key-value stores. CoRR, abs/1509.07815.<BR>&nbsp;&nbsp;&nbsp; [39] Eswaran, K. P., Gray, J., Lorie, R. A., and Traiger, I. L. (1976). The notions of consistency and predicate locks in a database system. Commun. ACM, 19(11):624&#8211;633.<BR>&nbsp;&nbsp;&nbsp; [40] Ezhilchelvan, P. D., Mitrani, I., Waudby, J., and Webber, J. (2019). Design and evalua- tion of an edge concurrency control protocol for distributed graph databases. In Gribaudo, M., Iacono, M., Phung-Duc, T., and Razumchik, R., editors, Computer Performance Engineering - 16th European Workshop, EPEW 2019, Milan, Italy, November 28-29, 2019, Revised Selected Papers, volume 12039 of Lecture Notes in Computer Science, pages 50&#8211;64. Springer.<BR>&nbsp;&nbsp;&nbsp; [41] Ezhilchelvan, P. D., Mitrani, I., and Webber, J. (2018). On the degradation of distributed graph databases with eventual consistency. In Bakhshi, R., Ballarini, P., Barbot, B., Castel- Taleb, H., and Remke, A., editors, Computer Performance Engineering - 15th European Workshop, EPEW 2018, Paris, France, October 29-30, 2018, Proceedings, volume 11178 of Lecture Notes in Computer Science, pages 1&#8211;13. Springer.<BR>&nbsp;&nbsp;&nbsp; [42] Faleiro, J. M., Abadi, D., and Hellerstein, J. M. (2017). High performance transactions via early write visibility. Proc. VLDB Endow., 10(5):613&#8211;624.<BR>&nbsp;&nbsp;&nbsp; [43] Fan, H. and Golab, W. M. (2019). Ocean vista: Gossip-based visibility control for speedy geo-distributed transactions. Proc. VLDB Endow., 12(11):1471&#8211;1484.<BR>&nbsp;&nbsp;&nbsp; [44] Fekete, A., Liarokapis, D., O&#8217;Neil, E. J., O&#8217;Neil, P. E., and Shasha, D. E. (2005). Making snapshot isolation serializable. ACM Trans. Database Syst., 30(2):492&#8211;528.<BR>&nbsp;&nbsp;&nbsp; [45] Francis, N. et al. (2018). Cypher: An evolving query language for property graphs. In<BR>SIGMOD, pages 1433&#8211;1445. ACM.<BR>&nbsp;&nbsp;&nbsp; [46] Fraser, K. (2004). Practical lock-freedom. PhD thesis, University of Cambridge, UK.</P>
<P>&nbsp;&nbsp;&nbsp; [47] Fuchs, P., Giceva, J., and Margan, D. (2022). Sortledton: a universal, transactional graph data structure. Proc. VLDB Endow., 15(6):1173&#8211;1186.<BR>&nbsp;&nbsp;&nbsp; [48] Garraghan, P., Townend, P., and Xu, J. (2014). An empirical failure-analysis of a large-scale cloud computing environment. In 15th International IEEE Symposium on High-Assurance Systems Engineering, HASE 2014, Miami Beach, FL, USA, January 9-11, 2014, pages 113&#8211;120. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [49] Giraph (2021).&nbsp; Giraph.&nbsp; <A href="https://giraph.apache.org/">https://giraph.apache.org/</A>. (Accessed on 16/12/2021).<BR>&nbsp;&nbsp;&nbsp; [50] Graphx (2021). Graphx. <A href="https://spark.apache.org/graphx/">https://spark.apache.org/graphx/</A>. (Accessed on 16/12/2021).<BR>&nbsp;&nbsp;&nbsp; [51] Gray, J., Lorie, R. A., Putzolu, G. R., and Traiger, I. L. (1976). Granularity of Locks and Degrees of Consistency in a Shared Data Base. In Nijssen, G. M., editor, Modelling in Data Base Management Systems, Proceeding of the IFIP Working Conference on Modelling in Data Base Management Systems, Freudenstadt, Germany, January 5-8, 1976, pages 365&#8211;394. North-Holland.<BR>&nbsp;&nbsp;&nbsp; [52] Guerraoui, R. and Wang, J. (2017). How fast can a distributed transaction commit? In Sallinger, E., den Bussche, J. V., and Geerts, F., editors, Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2017, Chicago, IL, USA, May 14-19, 2017, pages 107&#8211;122. ACM.<BR>&nbsp;&nbsp;&nbsp; [53] Guo, J., Cai, P., Wang, J., Qian, W., and Zhou, A. (2019). Adaptive optimistic concur- rency control for heterogeneous workloads. Proc. VLDB Endow., 12(5):584&#8211;596.<BR>&nbsp;&nbsp;&nbsp; [54] Harding, R., Aken, D. V., Pavlo, A., and Stonebraker, M. (2017). An evaluation of distributed concurrency control. Proc. VLDB Endow., 10(5):553&#8211;564.<BR>&nbsp;&nbsp;&nbsp; [55] Huang, D., Liu, Q., Cui, Q., Fang, Z., Ma, X., Xu, F., Shen, L., Tang, L., Zhou, Y., Huang, M., Wei, W., Liu, C., Zhang, J., Li, J., Wu, X., Song, L., Sun, R., Yu, S., Zhao, L., Cameron, N., Pei, L., and Tang, X. (2020a). Tidb: A raft-based HTAP database. Proc. VLDB Endow., 13(12):3072&#8211;3084.<BR>&nbsp;&nbsp;&nbsp; [56] Huang, J. and Abadi, D. (2016). LEOPARD: lightweight edge-oriented partitioning and replication for dynamic graphs. Proc. VLDB Endow., 9(7):540&#8211;551.<BR>&nbsp;&nbsp;&nbsp; [57] Huang, L., Magnusson, M., Muralikrishna, A. B., Estyak, S., Isaacs, R., Aghayev, A., Zhu, T., and Charapko, A. (2022). Metastable failures in the wild. In Aguilera, M. K. and Weatherspoon, H., editors, 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, pages 73&#8211;90. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [58] Huang, Y., Qian, W., Kohler, E., Liskov, B., and Shrira, L. (2020b). Opportunities for optimism in contended main-memory multicore transactions. Proc. VLDB Endow., 13(5):629&#8211;642.<BR>&nbsp;&nbsp;&nbsp; [59] Huppler, K. (2009). The art of building a good benchmark. In Nambiar, R. O. and Poess, M., editors, Performance Evaluation and Benchmarking, First TPC Technology Conference, TPCTC 2009, Lyon, France, August 24-28, 2009, Revised Selected Papers, volume 5895 of Lecture Notes in Computer Science, pages 18&#8211;30. Springer.</P>
<P>&nbsp;&nbsp;&nbsp; [60] Issa, S., Viegas, M., Raminhas, P., Machado, N., Matos, M., and Romano, P. (2020). Exploiting symbolic execution to accelerate deterministic databases. In 40th IEEE Interna- tional Conference on Distributed Computing Systems, ICDCS 2020, Singapore, November 29 - December 1, 2020, pages 678&#8211;688. IEEE.<BR>&nbsp;&nbsp;&nbsp; [61] Janusgraph (2020). Janusgraph. <A href="https://janusgraph.org/">https://janusgraph.org/</A>. (Accessed on 10/08/2020).<BR>&nbsp;&nbsp;&nbsp; [62] Kim, K., Wang, T., Johnson, R., and Pandis, I. (2016). ERMIA: fast memory-optimized database system for heterogeneous workloads. In &#214;zcan, F., Koutrika, G., and Madden, S., editors, Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, pages 1675&#8211;1687. ACM.<BR>&nbsp;&nbsp;&nbsp; [63] Kimura, H. (2015). FOEDUS: OLTP engine for a thousand cores and NVRAM. In Sellis,<BR>T. K., Davidson, S. B., and Ives, Z. G., editors, Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, Melbourne, Victoria, Australia, May 31 - June 4, 2015, pages 691&#8211;706. ACM.<BR>&nbsp;&nbsp;&nbsp; [64] Kraska, T., Pang, G., Franklin, M. J., Madden, S., and Fekete, A. D. (2013). MDCC: multi-data center consistency. In Hanz&#225;lek, Z., H&#228;rtig, H., Castro, M., and Kaashoek,<BR>M. F., editors, Eighth Eurosys Conference 2013, EuroSys &#8217;13, Prague, Czech Republic, April 14-17, 2013, pages 113&#8211;126. ACM.<BR>&nbsp;&nbsp;&nbsp; [65] Kumar, P. and Huang, H. H. (2020). Graphone: A data store for real-time analytics on evolving graphs. ACM Trans. Storage, 15(4):29:1&#8211;29:40.<BR>&nbsp;&nbsp;&nbsp; [66] Kung, H. T. and Robinson, J. T. (1981). On optimistic methods for concurrency control.<BR>ACM Trans. Database Syst., 6(2):213&#8211;226.<BR>&nbsp;&nbsp;&nbsp; [67] Kyrola, A., Blelloch, G. E., and Guestrin, C. (2012). Graphchi: Large-scale graph computation on just a PC. In Thekkath, C. and Vahdat, A., editors, 10th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2012, Hollywood, CA, USA, October 8-10, 2012, pages 31&#8211;46. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [68] Lamport, L. (1998). The part-time parliament. ACM Trans. Comput. Syst., 16(2):133&#8211; 169.<BR>&nbsp;&nbsp;&nbsp; [69] Lim, H., Kaminsky, M., and Andersen, D. G. (2017). Cicada: Dependably fast multi- core in-memory transactions. In Salihoglu, S., Zhou, W., Chirkova, R., Yang, J., and Suciu, D., editors, Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, pages 21&#8211;35. ACM.<BR>&nbsp;&nbsp;&nbsp; [70] Lin, Q., Chang, P., Chen, G., Ooi, B. C., Tan, K., and Wang, Z. (2016). Towards a non-2pc transaction management in distributed database systems. In &#214;zcan, F., Koutrika, G., and Madden, S., editors, Proceedings of the 2016 International Conference on Man- agement of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, pages 1659&#8211;1674. ACM.<BR>&nbsp;&nbsp;&nbsp; [71] Lu, Y., Yu, X., Cao, L., and Madden, S. (2020). Aria: A fast and practical deterministic OLTP database. Proc. VLDB Endow., 13(11):2047&#8211;2060.</P>
<P>&nbsp;&nbsp;&nbsp; [72] Lu, Y., Yu, X., Cao, L., and Madden, S. (2021). Epoch-based commit and replication in distributed OLTP databases. Proc. VLDB Endow., 14(5):743&#8211;756.<BR>&nbsp;&nbsp;&nbsp; [73] Lu, Y., Yu, X., and Madden, S. (2019). STAR: scaling transactions through asymmetric replication. Proc. VLDB Endow., 12(11):1316&#8211;1329.<BR>&nbsp;&nbsp;&nbsp; [74] Maiyya, S., Nawab, F., Agrawal, D., and Abbadi, A. E. (2019). Unifying consensus and atomic commitment for effective cloud data management. Proc. VLDB Endow., 12(5):611&#8211;623.<BR>&nbsp;&nbsp;&nbsp; [75] Maria, A. (1997). Introduction to modeling and simulation. In Andrad&#243;ttir, S., Healy,<BR>K. J., Withers, D. H., and Nelson, B. L., editors, Proceedings of the 29th conference on Winter simulation, WSC 1997, Atlanta, GA, USA, December 7-10, 1997, pages 7&#8211;13. ACM.<BR>&nbsp;&nbsp;&nbsp; [76] Melton, J. (1994). ANSI/ISO SQL-92 Specification. <A href="http://www.inf.fu-berlin.de/lehre/">http://www.inf.fu-berlin.de/lehre/</A> SS05/19517-V/FolienEtc/sql-foundation-aug94.pdf. (Accessed on 06/04/2020).<BR>&nbsp;&nbsp;&nbsp; [77] Mhedhbi, A., Lissandrini, M., Kuiper, L., Waudby, J., and Sz&#225;rnyas, G. (2021). LSQB: a large-scale subgraph query benchmark. In Kalavri, V. and Yakovets, N., editors, GRADES- NDA &#8217;21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA), Virtual Event, China, 20 June 2021, pages 8:1&#8211;8:11. ACM.<BR>&nbsp;&nbsp;&nbsp; [78] Mitrani, I. (1982). Simulation techniques for discrete event systems, volume 14 of<BR>Cambridge computer science texts. Cambridge University Press.<BR>&nbsp;&nbsp;&nbsp; [79] Mu, S., Nelson, L., Lloyd, W., and Li, J. (2016). Consolidating concurrency control and consensus for commits under conflicts. In Keeton, K. and Roscoe, T., editors, 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016, pages 517&#8211;532. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [80] Narula, N., Cutler, C., Kohler, E., and Morris, R. T. (2014). Phase reconciliation for contended in-memory transactions. In Flinn, J. and Levy, H., editors, 11th USENIX Symposium on Operating Systems Design and Implementation, OSDI &#8217;14, Broomfield, CO, USA, October 6-8, 2014, pages 511&#8211;524. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [81] Nawab, F., Arora, V., Agrawal, D., and Abbadi, A. E. (2015). Minimizing commit latency of transactions in geo-replicated data stores. In Sellis, T. K., Davidson, S. B., and Ives, Z. G., editors, Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, Melbourne, Victoria, Australia, May 31 - June 4, 2015, pages 1279&#8211;1294. ACM.<BR>&nbsp;&nbsp;&nbsp; [82] Ongaro, D. and Ousterhout, J. K. (2014). In search of an understandable consensus algorithm. In Gibson, G. and Zeldovich, N., editors, 2014 USENIX Annual Technical Conference, USENIX ATC &#8217;14, Philadelphia, PA, USA, June 19-20, 2014, pages 305&#8211;319. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [83] openCypher (2020). opencypher. <A href="https://www.opencypher.org/">https://www.opencypher.org/</A>. (Accessed on 10/08/2020).</P>
<P>&nbsp;&nbsp;&nbsp; [84] Oracle (2022). Property graph query language. <A href="https://pgql-lang.org/">https://pgql-lang.org/</A>. (Accessed on 20/04/2022).<BR>&nbsp;&nbsp;&nbsp; [85] Pavlo, A. (2017). What are we doing with our lives?: Nobody cares about our concur- rency control research. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, page 3. ACM.<BR>&nbsp;&nbsp;&nbsp; [86] Pavlo, A. and Aslett, M. (2016). What&#8217;s really new with NewSQL? SIGMOD Rec.<BR>&nbsp;&nbsp;&nbsp; [87] PostgreSQL (2023). PostgreSQL: The world&#8217;s most advanced open source relational database. <A href="https://www.postgresql.org/">https://www.postgresql.org/</A>. (Accessed on 20/06/2023).<BR>&nbsp;&nbsp;&nbsp; [88] Prasaad, G., Cheung, A., and Suciu, D. (2020). Handling highly contended OLTP workloads using fast dynamic partitioning. In Maier, D., Pottinger, R., Doan, A., Tan, W., Alawini, A., and Ngo, H. Q., editors, Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages 527&#8211;542. ACM.<BR>&nbsp;&nbsp;&nbsp; [89] Pritchett, D. (2008). BASE: an acid alternative. ACM Queue, 6(3):48&#8211;55.<BR>&nbsp;&nbsp;&nbsp; [90] Ren, K., Li, D., and Abadi, D. J. (2019). SLOG: serializable, low-latency, geo-replicated transactions. Proc. VLDB Endow., 12(11):1747&#8211;1761.<BR>&nbsp;&nbsp;&nbsp; [91] Ren, K., Thomson, A., and Abadi, D. J. (2014). An evaluation of the advantages and disadvantages of deterministic database systems. Proc. VLDB Endow., 7(10):821&#8211;832.<BR>&nbsp;&nbsp;&nbsp; [92] Robinson, I., Webber, J., and Eifrem, E. (2015). Graph databases: new opportunities for connected data. " O&#8217;Reilly Media, Inc.".<BR>&nbsp;&nbsp;&nbsp; [93] Sahu, S., Mhedhbi, A., Salihoglu, S., Lin, J., and &#214;zsu, M. T. (2020). The ubiquity of large graphs and surprising challenges of graph processing: extended survey. VLDB J., 29(2):595&#8211;618.<BR>&nbsp;&nbsp;&nbsp; [94] Semer&#225;th, O. et al. (2017). Formal validation of domain-specific languages with derived features and well-formedness constraints. Softw. Syst. Model., 16(2):357&#8211;392.<BR>&nbsp;&nbsp;&nbsp; [95] Sheng, Y., Tomasic, A., Sheng, T., and Pavlo, A. (2019). Scheduling OLTP transactions via machine learning. CoRR, abs/1903.02990.<BR>&nbsp;&nbsp;&nbsp; [96] Sovran, Y., Power, R., Aguilera, M. K., and Li, J. (2011). Transactional storage for geo-replicated systems. In Wobber, T. and Druschel, P., editors, Proceedings of the 23rd ACM Symposium on Operating Systems Principles 2011, SOSP 2011, Cascais, Portugal, October 23-26, 2011, pages 385&#8211;400. ACM.<BR>&nbsp;&nbsp;&nbsp; [97] Stonebraker, M., Madden, S., Abadi, D. J., Harizopoulos, S., Hachem, N., and Helland,<BR>P. (2007). The end of an architectural era (it&#8217;s time for a complete rewrite). In Koch, C., Gehrke, J., Garofalakis, M. N., Srivastava, D., Aberer, K., Deshpande, A., Florescu, D., Chan, C. Y., Ganti, V., Kanne, C., Klas, W., and Neuhold, E. J., editors, Proceedings of the 33rd International Conference on Very Large Data Bases, University of Vienna, Austria, September 23-27, 2007, pages 1150&#8211;1160. ACM.</P>
<P>&nbsp;&nbsp;&nbsp; [98] Taft, R., Sharif, I., Matei, A., VanBenschoten, N., Lewis, J., Grieger, T., Niemi, K., Woods, A., Birzin, A., Poss, R., Bardea, P., Ranade, A., Darnell, B., Gruneir, B., Jaffray, J., Zhang, L., and Mattis, P. (2020). Cockroachdb: The resilient geo-distributed SQL database. In Maier, D., Pottinger, R., Doan, A., Tan, W., Alawini, A., and Ngo, H. Q., editors, Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020, pages 1493&#8211;1509. ACM.<BR>&nbsp;&nbsp;&nbsp; [99] Tang, C., Wang, Z., Zhang, X., Yu, Q., Zang, B., Guan, H., and Chen, H. (2022). Ad hoc transactions in web applications: The good, the bad, and the ugly. In Ives, Z., Bonifati, A., and Abbadi, A. E., editors, SIGMOD &#8217;22: International Conference on Management of Data, Philadelphia, PA, USA, June 12 - 17, 2022, pages 4&#8211;18. ACM.<BR>&nbsp;&nbsp;&nbsp; [100] Tang, D., Jiang, H., and Elmore, A. J. (2017). Adaptive concurrency control: Despite the looking glass, one concurrency control does not fit all. In 8th Biennial Conference on Innovative Data Systems Research, CIDR 2017, Chaminade, CA, USA, January 8-11, 2017, Online Proceedings. <A href="http://www.cidrdb.org">www.cidrdb.org</A>.<BR>&nbsp;&nbsp;&nbsp; [101] Tarjan, R. E. (1972). Depth-first search and linear graph algorithms. SIAM J. Comput., 1(2):146&#8211;160.<BR>&nbsp;&nbsp;&nbsp; [102] Technology, S. I. (2011). Telecommunication Application Transaction Processing (TATP) Benchmark Description. <A href="http://tatpbenchmark.sourceforge.net/">http://tatpbenchmark.sourceforge.net/</A>. (Accessed on 26/04/2022).<BR>&nbsp;&nbsp;&nbsp; [103] Thomson, A. and Abadi, D. J. (2010). The case for determinism in database systems.<BR>Proc. VLDB Endow., 3(1):70&#8211;80.<BR>&nbsp;&nbsp;&nbsp; [104] Thomson, A., Diamond, T., Weng, S., Ren, K., Shao, P., and Abadi, D. J. (2012). Calvin: fast distributed transactions for partitioned database systems. In Candan, K. S., Chen, Y., Snodgrass, R. T., Gravano, L., and Fuxman, A., editors, Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2012, Scottsdale, AZ, USA, May 20-24, 2012, pages 1&#8211;12. ACM.<BR>&nbsp;&nbsp;&nbsp; [105] TiDB (2022). TiDB Transaction Isolation Levels. <A href="https://docs.pingcap.com/tidb/dev/">https://docs.pingcap.com/tidb/dev/</A> transaction-isolation-levels. (Accessed on 09/05/2022).<BR>&nbsp;&nbsp;&nbsp; [106] TigerGraph (2021). Tigergraph. <A href="https://www.tigergraph.com/">https://www.tigergraph.com/</A>. (Accessed on 16/12/2021).<BR>&nbsp;&nbsp;&nbsp; [107] TinkerPop, A. (2022). Gremlin query language. <A href="https://tinkerpop.apache.org/gremlin">https://tinkerpop.apache.org/gremlin</A>. html. (Accessed on 20/04/2022).<BR>&nbsp;&nbsp;&nbsp; [108] TitanDB (2020). thinkaurelius/titan: Distributed graph database. <A href="https://github.com/">https://github.com/</A> thinkaurelius/titan. (Accessed on 10/08/2020).<BR>&nbsp;&nbsp;&nbsp; [109] TPC (2010). TPC Benchmark C, revision 5.11. Technical report, TPC. <A href="http://www">http://www</A>. tpc.org/tpc_documents_current_versions/pdf/tpc-c_v5.11.0.pdf.</P>
<P>&nbsp;&nbsp;&nbsp; [110] Tu, S., Zheng, W., Kohler, E., Liskov, B., and Madden, S. (2013). Speedy transactions in multicore in-memory databases. In Kaminsky, M. and Dahlin, M., editors, ACM SIGOPS 24th Symposium on Operating Systems Principles, SOSP &#8217;13, Farmington, PA, USA, November 3-6, 2013, pages 18&#8211;32. ACM.<BR>&nbsp;&nbsp;&nbsp; [111] Wang, G., Zhang, L., and Xu, W. (2017). What can we learn from four years of data center hardware failures? In 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2017, Denver, CO, USA, June 26-29, 2017, pages 25&#8211;36. IEEE Computer Society.<BR>&nbsp;&nbsp;&nbsp; [112] Wang, J., Ding, D., Wang, H., Christensen, C., Wang, Z., Chen, H., and Li, J. (2021). Polyjuice: High-performance transactions via learned concurrency control. In Brown,<BR>A. D. and Lorch, J. R., editors, 15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021, July 14-16, 2021, pages 198&#8211;216. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [113] Wang, T. and Kimura, H. (2016). Mostly-optimistic concurrency control for highly contended dynamic workloads on a thousand cores. Proc. VLDB Endow., 10(2):49&#8211;60.<BR>&nbsp;&nbsp;&nbsp; [114] Wang, Z., Mu, S., Cui, Y., Yi, H., Chen, H., and Li, J. (2016). Scaling multicore databases via constrained parallel execution. In &#214;zcan, F., Koutrika, G., and Madden, S., editors, Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, pages 1643&#8211;1658. ACM.<BR>&nbsp;&nbsp;&nbsp; [115] Waudby, J. (2022). High performance mixed graph-based concurrency control. In Bao, Z. and Sellis, T. K., editors, Proceedings of the VLDB 2022 PhD Workshop co- located with the 48th International Conference on Very Large Databases (VLDB 2022), Sydney, Australia, September 5, 2022, volume 3186 of CEUR Workshop Proceedings. CEUR-WS.org.<BR>&nbsp;&nbsp;&nbsp; [116] Waudby, J., Ezhilchelvan, P. D., Webber, J., and Mitrani, I. (2020a). Preserving reciprocal consistency in distributed graph databases. In Fekete, A. D. and Kleppmann, M., editors, 7th Workshop on Principles and Practice of Consistency for Distributed Data, <A href="mailto:PaPoC@EuroSys">PaPoC@EuroSys</A> 2020, Heraklion, Greece, April 27, 2020, pages 2:1&#8211;2:7. ACM.<BR>&nbsp;&nbsp;&nbsp; [117] Waudby, J., Steer, B. A., Karimov, K., Marton, J., Boncz, P. A., and Sz&#225;rnyas, G. (2020b). Towards testing ACID compliance in the LDBC social network benchmark. In Nambiar, R. and Poess, M., editors, Performance Evaluation and Benchmarking - 12th TPC Technology Conference, TPCTC 2020, Tokyo, Japan, August 31, 2020, Revised Selected Papers, volume 12752 of Lecture Notes in Computer Science, pages 1&#8211;17. Springer.<BR>&nbsp;&nbsp;&nbsp; [118] Waudby, J., Steer, B. A., Prat-P&#233;rez, A., and Sz&#225;rnyas, G. (2020c). Supporting dy- namic graphs and temporal entity deletions in the LDBC social network benchmark&#8217;s data generator. In Arora, A., Salihoglu, S., and Yakovets, N., editors, GRADES-NDA&#8217;20: Proceedings of the 3rd Joint International Workshop on Graph Data Management Expe- riences &amp; Systems (GRADES) and Network Data Analytics (NDA), Portland, OR, USA, June 14, 2020, pages 8:1&#8211;8:8. ACM.<BR>&nbsp;&nbsp;&nbsp; [119] Webber, J., Ezhilchelvan, P. D., and Mitrani, I. (2019). Modeling corruption in eventually-consistent graph databases. CoRR, abs/1904.04702.</P>
<P>&nbsp;&nbsp;&nbsp; [120] Wu, Y., Chan, C. Y., and Tan, K. (2016). Transaction healing: Scaling optimistic concurrency control on multicores. In &#214;zcan, F., Koutrika, G., and Madden, S., editors, Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, pages 1689&#8211;1704. ACM.<BR>&nbsp;&nbsp;&nbsp; [121] Wu, Y. and Tan, K. (2016). Scalable in-memory transaction processing with HTM. In Gulati, A. and Weatherspoon, H., editors, 2016 USENIX Annual Technical Conference, USENIX ATC 2016, Denver, CO, USA, June 22-24, 2016, pages 365&#8211;377. USENIX Association.<BR>&nbsp;&nbsp;&nbsp; [122] Yu, X., Pavlo, A., S&#225;nchez, D., and Devadas, S. (2016). Tictoc: Time traveling optimistic concurrency control. In &#214;zcan, F., Koutrika, G., and Madden, S., editors, Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016, pages 1629&#8211;1642. ACM.<BR>&nbsp;&nbsp;&nbsp; [123] Yuan, Y., Wang, K., Lee, R., Ding, X., Xing, J., Blanas, S., and Zhang, X. (2016). BCC: reducing false aborts in optimistic concurrency control with low cost for in-memory databases. Proc. VLDB Endow., 9(6):504&#8211;515.<BR>&nbsp;&nbsp;&nbsp; [124] Zhang, I., Sharma, N. K., Szekeres, A., Krishnamurthy, A., and Ports, D. R. K. (2015). Building consistent transactions with inconsistent replication. In Miller, E. L. and Hand, S., editors, Proceedings of the 25th Symposium on Operating Systems Principles, SOSP 2015, Monterey, CA, USA, October 4-7, 2015, pages 263&#8211;278. ACM.