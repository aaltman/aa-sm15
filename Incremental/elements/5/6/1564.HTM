Abstract&#8212;A major threat to distributed software systems&#8217; reliability is vicious cycles, which are observed when an event in the distributed software system&#8217;s execution causes a system degradation, and the degradation, in turn, causes more of such events. Vicious cycles often result in large-scale cloud outages that are hard to recover from due to their self-reinforcing nature.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>DN2 HeartBeat DN3 HeartBeat<BR>NameNode<BR>This paper formally defines Vicious Cycle, and conducts the first in-depth study of 33 real-world vicious cycles in 13 widely-<BR>DN4 HeartBeat<BR>used open-source distributed software systems, shedding light on the root causes, triggering conditions, and fixing strategies of<BR>DN5 HeartBeat DN6 HeartBeat<BR>vicious cycles, with over a dozen concrete implications to combat them. Our findings show that the majority of the vicious cycles are caused by incorrect error handlers, where the handlers do<BR>DN7 HeartBeat<BR>not obtain enough information to distinguish between 1) an error induced by incoming requests and 2) an error induced by an unexpected interference from another error handler.<BR>This paper further performs a feasibility study by 1) building a monitoring tool that prevents one type of vicious cycle by collecting information to make a more informed decision in error handling, and 2) investigating the effectiveness of one commonly suggested practice&#8212;injecting exponential backoff&#8212;to prevent vicious cycles induced by unconstrained retry.<BR>Index Terms&#8212;distributed software systems, vicious cycles</P>
<P>&nbsp;&nbsp;&nbsp; I. Introduction<BR>Internet services today live on data-intensive distributed software systems such as distributed storage systems and distributed computation frameworks. Such distributed software systems are designed to be highly reliable by tolerating component failures with technologies such as data replica- tion [17], [31], [46] , and recomputation [96]. Unfortunately, cascading component failures still happen and cause severe consequences such as service outages. Such cascading failures happen because software defects and design flaws propagate failures from one component to another. Cascading failures often manifest through a Vicious Cycle (VC), in which an event in the distributed software system&#8217;s execution causes a system degradation, and the degradation, in turn, causes more of such events.<BR>An incident [41] from Amazon Web Services (AWS) shows how a temporary network failure disrupts the entire Amazon Elastic Block Store (EBS) service through a vicious cycle: Data stored on EBS are replicated on multiple storage servers. When a temporary network failure causes some storage servers to lose connection to their mirrors, they start replicating data to other storage servers. Unfortunately, these replication requests trigger a latent race condition that causes more storage servers to crash, resulting in more replication requests. This forms a<BR>DNn HeartBeat</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Fig. 1: Hadoop-572: By clogging up the NameNode&#8217;s RPC thread (TRPC), DataNode DN1&#8217;s timeout causes other DataN- odes (i.e., DN3 and DN4) to time out, which further causes additional DataNodes (i.e., DN6 and DN7) to time out, which eventually causes the entire cluster to crash. DN is DataNode. HB denotes HeartBeat.</P>
<P>self-reinforcing cycle that brought down the entire EBS service and was only stopped by manually disabling EBS replication requests and adding capacities to the cluster.<BR>Vicious Cycles are prevalent in commercial distributed software systems. At the time of writing, a third [40]&#8211;[44] of the 15 severe incidents reported by AWS [13] in the past 12 years manifest as vicious cycles. In fact, some distributed software system practitioner claims that once a system reaches a certain level of reliability, most major incidents will involve a vicious cycle [15]. Thus, there is high demand for a deep understanding of such vicious cycles so that one can build avoidance, testing, diagnosis, and recovery techniques for them. Unfortunately, postmortem reports from cloud vendors typically provide limited information. To fill this gap, we conduct the first in-depth study of 33 vicious cycles on 13 open-source distributed software systems. Our source-code level analysis reveals unique challenges in combating vicious cycles, as illustrated below.<BR>&nbsp;&nbsp;&nbsp; A. A Motivating Example<BR>Figure 1 shows Hadoop-572 [26], a real-world vicious cycle in Hadoop Distributed File System (HDFS), which has a sim-<BR>ilar characteristic as the AWS incident discussed above. The failure happens in a large Hadoop cluster of 600 DataNodes, where one DataNode holds over 10,000 data blocks, and a NameNode stores the metadata. A NameNode has a watchdog thread TWatchDog that periodically checks the timestamp of the last heartbeat from each DataNode and a remote procedure call (RPC) thread TRPC that receives and processes HeartBeat messages from DataNodes.<BR>The vicious cycle starts when NameNode misses DN1&#8217;s HeartBeat message. Its TWatchDog detects DataNode DN1&#8217;s timeout failure and marks blocks stored on DN1 as under- replicated ( 1 in Figure 1). When the NameNode receives a heartbeat from another DataNode DN2, the NameNode gener- ates replication requests for these under-replicated blocks that were stored on DN1, and wraps them in the heartbeat response to DN2 ( 2 ). Unfortunately, generating replication requests occupies the NameNode&#8217;s RPC thread (TRPC) and prevents the delivery of heartbeats from other DataNodes (i.e., DN3 and DN4) ( 3 ) to TRPC. Although the heartbeat timeout threshold is configured as one minute to allow retries, a software defect limits the number of heartbeat retries and renders the one- minute timeout threshold ineffective. The watchdog thread TWatchDog mistakenly marks the affected DataNodes (i.e., DN3 and DN4) as dead ( 4 and 5 ) and marks more blocks on DN3 and DN4 as under-replicated, causing more workload for the NameNode itself ( 6 and 10 ).<BR>Upon receiving a heartbeat message from DN5, the NameN- ode generates and sends replication requests to DN5 for data blocks on DN3 ( 6 ) and prevents the delivery of the heartbeat from other DataNodes (i.e., DN6 and DN7) through the same process described above ( 7 ), again causing more workload on the NameNode ( 8 and 9 ). The entire Hadoop cluster then falls into a vicious cycle and crashes.<BR>The root cause of Hadoop-572 lies in the error handler of DataNode failures (i.e., TWatchdog), whose recovery task causes undesired interference that results in a HeartBeat miss for another DataNode. The recovery task performed is generating the replication requests, which interferes with the delivery of the HeartBeat messages. The error handler does not have enough information to distinguish between a DataNode HeartBeat miss caused by 1) a real DataNode crash and 2) a delayed HeartBeat delivery due to interference from a previous recovery task. The confusion of the error handler makes the system susceptible to vicious cycles.<BR>To prevent this vicious cycle, developers apply a series of fixes. They 1) fix the software defect that limits DataNode heartbeat retries, 2) optimize the NameNode&#8217;s performance, and 3) avoid unnecessary replication requests when a dead DataNode is resurrected. Applying any of the fixes alone cannot avoid the vicious cycle, demonstrating the complexity of fixing vicious cycles (details below in &#8220;Hard to repair&#8221;).<BR>This example shows that vicious cycles are particularly difficult to tackle because of the following challenges:<BR>&nbsp;&nbsp;&nbsp; 1) Complex triggering conditions. Vicious cycles typically require a unique combination of workload, cluster con- figuration, and fault injection in cluster-level testing (e.g.,<BR>integration testing [75] ) to be triggered. For example, the prolonged heartbeat handling in Figure 1 requires each failed DataNode to hold over 10,000 data blocks. Unfortu- nately, thoroughly exploring the input space, configuration space, and fault injection space in cluster-level testing is extremely challenging [61], which makes vicious cycles difficult to be exposed before software release.<BR>&nbsp;&nbsp;&nbsp; 2) Hard to recover. A vicious cycle involves events and system degradations that aggravate each other. Oftentimes, it is the automated failure recovery stage itself aggravating the degradation. In addition, this process could quickly develop into a cluster outage due to amplification and leave little chance for manual intervention and recovery. For example, the vicious cycle in Figure 1 could result in a cluster outage within a few heartbeat timeout intervals.<BR>&nbsp;&nbsp;&nbsp; 3) Hard to repair. A vicious cycle typically happens due to both design flaws (e.g., poor fault isolation and perfor- mance bug) and logic errors. Thus, fixing one of them often fails to prevent similar failures in the future. For the vicious cycle in Figure 1, although fixing the software defect alone would allow the DataNode to retry the heartbeat within the one-minute timeout interval, the generation of the replication request could exceed this limit given enough under-replicated data blocks, causing the entire cluster to crash. With either of the latter two fixes alone (fixes 2) or 3)), the DataNode&#8217;s heartbeat would still be highly likely to time out, because the number of retries is incorrectly limited, reducing the probability of heartbeat delivery.<BR>&nbsp;&nbsp;&nbsp; B. Contribution<BR>This paper defines the concept of the vicious cycle (&#167;II) and provides the first comprehensive, in-depth analysis of the vicious cycle in real-world distributed software systems. Specifically, we study 33 real-world vicious cycles from<BR>13 widely-used, open-source distributed software systems, namely,&nbsp; Apache Cassandra [3], HDFS [34], HBase [6],<BR>ZooKeeper [12], Hadoop [5], Kafka [8], Flink [4], Solr [10],<BR>Ignite [1], ActiveMQ [2], Storm [11], Accumulo [1], and Ratis [9]. For each case, we carefully analyzed its report and source code to thoroughly understand the root cause of the vicious cycle, the triggering condition, and the fix strategy. In addition, we reproduced eight of the cases to better understand them. This paper makes the following contributions:<BR>&nbsp;&nbsp;&nbsp; &#8226; A symptom study (&#167;IV). We show that vicious cycles have severe consequences.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; Most (82%) vicious cycles result in node failures (58%) and elevated queue size (24%).<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; A majority (55%) of vicious cycles amplify the error, causing it to grow exponentially.<BR>&nbsp;&nbsp;&nbsp; &#8226; A root-cause study (&#167;V). We find that vicious cycles are formed either 1) (Unexpected Error) when an undetected or unhandled error propagates along a global cycle (40%) or<BR>2) (Unexpected Cycle) when error handlers unexpectedly interfere with request handling and cause other requests to fail (60%).<BR>&nbsp;&nbsp;&nbsp; &#8226; Unexpected Error: 18% of the vicious cycles are caused by undetected errors, which are highly diverse and system-specific. Another 22% are caused by unhandled errors, all happen when retrying an error-inducing input.<BR>&nbsp;&nbsp;&nbsp; &#8226; Unexpected Cycle: More than a third (36%) of vicious cycles happen due to incorrect decisions when recovering from system degradation. The remaining 24% of vicious cycles are formed due to unconstrained retries.<BR>&nbsp;&nbsp;&nbsp; &#8226; A triggering-condition study (&#167;VI). We find that vicious cycles have unique requirements to be triggered.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; Many (42%) vicious cycles require a heavy workload.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; A large portion (36%) of vicious cycles are non- deterministic, and a non-negligible amount (18%) re- quires timing constraints with very small time windows.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; 24% of vicious cycles can only be triggered when the cluster is in a special state, such as during a rolling upgrade or when nodes are holding large amounts of data.<BR>&nbsp;&nbsp;&nbsp; &#8226; A fixing-strategy study (&#167;VII). We find that vicious cycles are difficult to fix.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; A significant portion (61%) of the vicious cycles are fixed by major redesigns, including 1) reducing the work- load (18%), 2) separating heavy workload (6%), and 3)<BR>system-specific redesigns (37%).<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702; A surprising amount (27%) of fixes are considered workarounds by developers but not complete fixes.<BR>&nbsp;&nbsp;&nbsp; &#8226; A feasibility study (&#167;VIII) to prevent vicious cycles through well-informed error handling and exponential backoff.<BR>&nbsp;&nbsp;&nbsp; &#8226; The first data set [45] of 33 real-world vicious cycle bugs, from 13,455 bug reports, in 13 open-source projects.<BR>&nbsp;&nbsp;&nbsp; &#8226; Actionable implications and guidelines to prevent vicious cycles. These include suggestions for runtime detection and prevention techniques, testing techniques, and good practices in software development to avoid vicious cycles.<BR>&nbsp;&nbsp;&nbsp; II. Definitions<BR>In this section, we give a concrete definition of the vicious cycle, as, to the best of our knowledge, such a definition does not exist in any previous works. This definition is necessary to identify vicious cycles and is applied to each studied case. An example of applying our definition to Hadoop-572 (&#167;I) is presented later in this section.<BR>Definition 1 A vicious cycle is a global iterative execution in which each iteration results in a system degradation and the degradation in turn causes one or multiple future iterations.<BR>When one iteration in a vicious cycle causes multiple (as opposed to one) future iterations (through the degradation), the vicious cycle has an amplification behavior. A vicious cycle with an amplification behavior typically has the most severe consequences.<BR>A global iterative execution is defined using a causality graph that represents the execution of the entire distributed software system and captures the causal relationships between system degradations and iterative executions. We first the define causality graph as follows:<BR>Definition 2 A causality graph, G(V, E), is a directed acyclic graph with each vertex v &#8712; V representing an event that hap- pened during the execution and each edge e &#8712; E representing a causal relationship between a pair of events. Each vertex is<BR>labeled with the corresponding event, and each edge is labeled with the corresponding causal relationship.<BR>Events and causal relationships are defined with enumera- tive definitions in &#167;II-A. One example of an event is receiving a network message. One example of the causal relationship is the happens-before relationship [72] between a pair of sending and receiving a network message.<BR>With the definition of causality graph, we define global iterative execution as follows:<BR>Definition 3 An iterative execution is formed in a causality graph when there are multiple isomorphic subgraphs con- nected through edges. Each subgraph is called an iteration. The isomorphism comparison considers subgraphs&#8217; structure, as well as their vertices&#8217; and edges&#8217; labels. A global iterative execution is an iterative execution in which events of each isomorphic subgraph are spread across multiple threads or nodes.<BR>The notion of system degradation involved in a vicious cycle could range from general notions such as an increasing number of crashed nodes to system-specific notions such as an increasing number of missing file blocks in a distributed file system. We define a system degradation using a set point and a measurement function, both of which can be system-specific.<BR>Definition 4 A set point (nsp) is a numeric value correspond- ing to a property that the system should satisfy in an ideal state. This property is in the form of a predicate over a measurable system state (nst) and the set point: nst == nsp. A measurement function ( fm()) measures the distance between<BR>the current system state and the set point: fm() = nst &#8722; nsp. A<BR>system degradation happens when the result of executing fm()<BR>over the current state is larger than that of a previous state: fm() &#8722; f j () &gt; 0.<BR>In Hadoop-572 [26] (&#167;I), the property that the system should satisfy in an ideal state is crashed&nbsp; nodes.size() == 0 (i.e., nst = crashed&nbsp; nodes.size()), and the set point nsp == 0. The measurement function is fm() = crashed&nbsp; nodes.size() &#8722; 0.<BR>&nbsp;&nbsp;&nbsp; A. Events and Causal Relationships<BR>Events. Events in a causality graph belong to the following two categories:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1) Errors event that results in system degradation. Such events could be system-specific and should be handled properly by an error handler.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2) Normal execution logic such as request handling, RPC call procedures, etc.<BR>Causal relationships. Causal relationships in a distributed software system refer to the happens-before relationship [72] and its variants [77], [78], [84]. Specifically, the causal rela- tionships include the following four categories:<BR>&nbsp;&nbsp;&nbsp; 1) Inter-node network communication Sending a network message through sockets happens before its reception.<BR>&nbsp;&nbsp;&nbsp; 2) Intra-node multi-thread communication Submitting a task to a thread pool happens before its execution, and its execution happens before its join. Similarly, multi-thread communication includes the happens-before relationship between thread creation, thread execution, and thread join.<BR>&nbsp;&nbsp;&nbsp; 3) Intra-thread happens-before An event happens before an- other event that occurs later in the same thread. When the thread is an event handler thread, this relationship only applies to events in the same invocation of the handler.<BR>&nbsp;&nbsp;&nbsp; 4) Resource contention A task submission to a thread pool has a potential resource contention with later task submissions.<BR>&nbsp;&nbsp;&nbsp; B. Identifying Vicious Cycles<BR>To determine whether a studied case involves a vicious cycle, we construct its causality graph and analyze whether its causality graph contains a global iterative execution with a system degradation. We illustrate this process with the real- world vicious cycle in our introduction (&#167;I): Hadoop-572 [26]. Figure 1 shows a simplified causality graph we constructed for Hadoop-572. Events are presented with boxes (e.g., the timeout error event of DataNodes) labeled with the event name. Some event boxes (e.g., the heartbeat from DataNodes) are omitted due to space limitations. Causal relationships are presented with lines and arrows. Solid lines represent happens- before relationships. Solid arrows are constructed from inter- node (e.g., DataNode heartbeat triggering RPC handler) and inter-thread communication (e.g., timeout error of a DataNode cause RPC handler to replicate the DataNode&#8217;s blocks). Red cross with dashed arrows represent resource contention (e.g., heartbeat from a DataNode is not delivered due to contention<BR>and causing its timeout error).<BR>With this causality graph, we identify two isomorphic subgraphs: [ 1 , 2 , 3 ] and [ 4 , 6 , 7 ], and the execution in each subgraph involves the NameNode and multiple DataN- odes. Therefore, Hadoop-572 is a global iterative execution (Definition 3), and each subgraph makes up of one iteration. Note that three more incomplete iterations exist in Figure 1: [ 5 , 10 ], [ 8 ], and [ 9 ].<BR>The system degradation (Definition 4) is quantified by the number of crashed nodes in the cluster, with the set point being 0. Within each iteration, one more DataNode is marked as dead by the NameNode. Therefore, each global iteration results in a system degradation (Definition 1), and Hadoop- 572 is a vicious cycle by our definition.<BR>&nbsp;&nbsp;&nbsp; III. Experimental Methods<BR>We conducted the study on 33 vicious cycles from a wide range of popular open-source distributed software systems, as shown in Table I, including distributed databases [3], [6], [7], key-value stores [1], [12], filesystem [34], streaming pro-<BR>cessing systems [2], [4], [8], [11], computing framework [5], consensus library [9], and search platform [10].<BR>We selected the set of failures from the issue trackers of the distributed software systems above. We search for<BR>TABLE I: The number of vicious cycles in each system.</P>
<P>System&nbsp;#&nbsp;System&nbsp;#&nbsp;System&nbsp;#</P>
<P>HBase [6]<BR>6<BR>Solr [10]<BR>3<BR>Accumulo [1]<BR>1<BR>HDFS [34]<BR>6<BR>Flink [4]<BR>2<BR>Ignite [7]<BR>1<BR>Hadoop [5]<BR>4<BR>Storm [11]<BR>1<BR>ZooKeeper [12]<BR>1<BR>Kafka [8]<BR>3<BR>Ratis [9]<BR>1<BR>ActiveMQ [2]<BR>1<BR>Cassandra [3]<BR>3</P>
<P><BR>Total<BR>33</P>
<P><BR>resolved and valid issues whose bug report contains a list of keywords, including &#8220;vicious cycle, vicious circle, cascade, spiral, feedback loop, multiplying effect, amplify, overwhelm, storm, bounce, snowball, chain reaction, and domino&#8221;.<BR>Our keywords are selected using a multi-round boosting strategy combined with manual review: We start with a small set of reasonable keywords and collect all the bug candidates (filtering step). Then, we manually go through the bug can- didates collected by the filtering step, excluding cases that are not a vicious cycle, and identify new keywords based on the title, description, and discussion associated with each candidate (reviewing step). With the new set of keywords, we start a new round of the filtering process. With such boosting strategy, we are able to expand the number of bugs in each round and keep the representativeness of the keywords. This process stops when we could not identify new keywords. Our manual filtering exhausted all the bug reports containing these keywords. In the end, we get 33 vicious cycles as shown in Table I from 13,455 bug reports in total.<BR>Among the 33 cases, about one-third (30%) are reported within recent five years. A majority (91%) of them have a priority of major or higher in the bug tracking system. One bug (HBase-27149 [30]) was reported in 2022 and another bug (Kafka-10888 [35]) was not resolved until 2022, showing that vicious cycles are still present in recent systems.<BR>Our analysis results for all 33 vicious cycles are available in an anonymous repository [45]. The threats to validity of our filtering process are discussed in &#167;IX.</P>
<P>&nbsp;&nbsp;&nbsp; IV. Symptoms of Vicious Cycles<BR>We first investigate the manifestations of vicious cycles, focusing on their symptoms and propagation.</P>
<P>As defined in &#167;II, one iteration of a vicious cycle with am- plification behavior can cause multiple subsequent iterations. A vicious cycle with the amplification behavior is the most severe subtype as it is usually hard to recover from and can quickly propagate the system degradation to the entire cluster. For example, Hadoop-572 [26] (&#167;I) has an amplification behavior, where the timeout of one DataNode could result in timeouts of multiple DataNodes.<BR>TABLE II: An overview of error handler&#8217;s role in the formation of vicious cycle. Definitions of each type and subtype of vicious cycles are italicized. The number in the parenthesis indicates the percentage of vicious cycles in each category.<BR>Vicious Cycle Type&nbsp;Subtype&nbsp;Interference Undetected Error (18%)<BR>Unexpected Error (40%)<BR>An error that hinders the task completion is propagated along a global cycle.</P>
<P>&nbsp;</P>
<P><BR>Unexpected Cycle (60%)<BR>The error handlers unexpectedly interfere with request handling and cause other requests to fail.<BR>The error in the cycle is silent,<BR>thus no error handler is implemented.<BR>Unhanded Error (22%)<BR>The error in the cycle is observed, but not properly handled.<BR>Incorrect Degradation Recovery (36%) The interference from the degradation recovery tasks causes further degradation.<BR>Unconstrained Retry (24%)<BR>Previous retries of a request interfere with the current retry, causing the current one to fail.<BR>N/A</P>
<P><BR>N/A</P>
<P>Performance Interference (21%)</P>
<P>Functional Interference (15%)</P>
<P>Performance Interference (24%)</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp; (a) Conceptual graph of a UE vicious cycle. The red arrows indicate a vicious cycle. The green arrow indicate a VC-free environment.</P>
<P>Vicious Cycle</P>
<P><BR>&nbsp;&nbsp;&nbsp; (b) Conceptual graph of a UC vicious cycle. The red arrows indicate a vicious cycle.<BR>Fig. 2: Conceptual graph of vicious cycles.</P>
<P>Implication: Most vicious cycles can grow exponentially in the cluster, propagating the system degradation rapidly across the cluster. Detection and prevention techniques need to intervene as soon as possible to prevent a potential whole- cluster collapse.<BR>&nbsp;&nbsp;&nbsp; V. Root Causes of Vicious Cycles<BR>The key component forming a vicious cycle is the error handlers. Thus, depending on error handlers&#8217; interaction with the global iterative execution or the absence of error handlers, we categorize vicious cycles based on their root causes into two types &#8211; Unexpected Error (UE), an error in a global cycle is undetected or unhandled, and Unexpected Cycle (UC), when error handlers unexpectedly interfere with request handling and cause other requests to fail.<BR>Figure 2a and Figure 2b illustrate the UE and UC vicious cycles respectively. Figure 2a shows a UE vicious cycle (red arrows) where an error (red box) affects normal execution which causes more errors. The green VC-free cycle where multiple normal executions (e.g., requests) continue error-free. Figure 2b shows a UC vicious cycle (red arrows). An error handler performs a recovery task (green box) to recover from the error. However, the recovery task interferes with other requests and causes them to run into the same error. The error handler lacks information to distinguish between errors caused by the interference (orange dotted box) and errors caused by<BR>the external trigger (more discussion in &#167;VI), forming a vicious cycle (the cycle formed by the red arrows).<BR>Table II shows our root cause taxonomy of two VC types (UE and UC) and four VC subtypes. Numbers in the paren- thesis are the percentage of vicious cycles belonging to each category. Two major causes of UE vicious cycles are 1) undetected error (&#167;V-A), and 2) unhandled error (&#167;V-B). We further classify the UC vicious cycles based on the error han- dler&#8217;s recovery task in each iteration: 1) incorrect degradation recovery (&#167;V-C), and 2) unconstrained retry (&#167;V-D).<BR>Column &#8220;Interference&#8221; shows the type of interferences be- tween recovery tasks and requests in a vicious cycle. For UE vicious cycles, errors are either silent or not properly handled. Thus, no interference originates from the error handler. For UC vicious cycles, we observe two types of interference&#8212; performance interference and functional interference. Below we explain these VC types and subtypes and their interaction with interference with examples.</P>
<P>An example of an undetected error is Kafka-10888 [35], where a slow node is caused by a buggy load balancer. No explicit error such as an exception is thrown, and no error handler is implemented.<BR>One example of an unhandled error is HBase-14598 [28], where an HBase client retries a bad request causing a server crash. The client blindly retries the bad request because the server crash is never properly handled.</P>
<P>For example, as in Hadoop-572 [26] (&#167;I), the error is the delay of DataNode heartbeat delivery. The error handler, replicating the under-replicated blocks, cannot differentiate between a delay caused by an actual DataNode failure (thus no<BR>heartbeat message is sent) and a contention in the RPC handler thread pool caused by the recovery task (thus a heartbeat message is received but not delivered).<BR>Interference. As shown in Table II, both performance and functional interference can cause vicious cycles. Performance interferences include contentions through CPU resources, memory resources, and a lock. For example, in Hadoop- 572 [26] (&#167;I) the recovery task interferes with the heartbeat through a CPU contention (a shared thread pool).<BR>Functional interferences include IO errors, deadlocks, and other system-specific errors caused by the recovery task and interrupt request handling. For example, in HDFS-12914 [32], the block replication (recovery task) triggers a deadlock on the DataNode, causing its block report to be rejected by the NameNode and more block replication initiated.<BR>Note that an unconstrained retry could cause functional interference, but we do not observe it in our study.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. UE Subtype 1: Undetected Error</P>
<P>For example, Cassandra-13441 [14] is a vicious cycle that happens due to a silent error propagated through Cassandra&#8217;s gossip protocol. Due to a silent logic error in Cassandra 3.0.14, when a node is upgraded with no scheme change, gossip messages [56], [70] between this node and other non-upgraded nodes indicate a schema mismatch by mistake. The schema mismatch incurs schema migration requests between the up- graded and non-upgraded nodes. Non-upgraded nodes which have detected the schema mismatch further spread information about the schema mismatch to the remaining nodes in the cluster through Cassandra&#8217;s gossip protocol, resulting in a storm of unnecessary schema migration requests. The silent error is spread to the entire cluster through the gossip protocol, forming a vicious cycle.<BR>We find that logic errors that cause silent errors are highly diverse and system-specific and, unfortunately, could not iden- tify general bug patterns. As discussed above, the logic error in Cassandra-13441 is closely coupled with the gossip protocol.<BR>Implication: The system-specific nature of the logic errors makes vicious cycles triggered by them hard to be found during the testing phase. Fortunately, logic errors usually accompany observable system degradations (&#167;IV), indicating opportunities for automated detection.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. UE Subtype 2: Unhandled Error</P>
<P>For example, HBase-14598 [28] is such a vicious cycle: The bug is caused by a problematic Scan (an HBase query)<BR>allocating a large chunk of memory, larger than that is allowed by the JVM. Such a request results in an OutOfMemoryError (OOM) and crashes the RegionServer (RS) processing the request. The problematic Scan is then retried on a new RS and causes it to fail as well.<BR>Implication: Vicious cycles caused by deadly retries result in fatal errors that are detected but not handled. The reason is the lack of the ability to infer the causal relationship between the error-inducing retried request and the error, i.e., the system does not know whom to blame when the error happens. Once this causal relationship is identified, the error-inducing request can be blocked to break the vicious cycle. More details about detecting erroneous inputs are discussed in &#167;VIII-A.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C. UC Subtype 1: Incorrect Decision in Degradation Recovery</P>
<P>Specifically, degradation recovery includes capacity recov- ery (24%) such as node restart and reconnection, and data recovery (12%) such as replicating missing file blocks. As shown in Figure 2b, such an incorrect decision typically happens when the error handler is not able to distinguish an error caused by an external trigger and an error caused by the interference of the error handling execution itself.</P>
<P>For example, Hadoop-572 [26] (&#167;I) is a vicious cycle caused by CPU contention between data recovery and heart- beat handling, resulting in a heartbeat delay. HDFS-9107 [33] is a vicious cycle caused by contention between a GC during the DataNode&#8217;s handling of FullBlockReport (FBR) requests and the sending of heartbeat messages.<BR>The vicious cycle in HDFS-12914 [32] originates from a performance bug delaying the handling of FBR requests and propagates through a deadlock on DataNode caused by HDFS block replications.<BR>Implication: To expose and detect vicious cycles, it is critical to trigger performance and functional interferences between error handling logic and request handling logic. For example, injecting delays to health-checking (or degradation-checking) operations to trigger recovery and applying resource capping to induce performance interference could expose vicious cycles. Fault injection such as injecting IO errors is also useful in mimicking functional interferences. Monitoring techniques could focus on observing and correlating system degradation with performance degradation to detect many vicious cycles.<BR>TABLE III: Triggering conditions of vicious cycles.</P>
<P>Trigger&nbsp;%&nbsp;Trigger&nbsp;%</P>
<P>Interestingly, not only a delay caused by a recovery task (e.g., CPU contention in Hadoop-572), a delay to the recovery task can also cause vicious cycles. Such a delay causes the error recovery information not being propagated to all the relevant nodes, so that other nodes handling requests can be interrupted by an error. STORM-404 [39] is such a vicious cycle: In an Apache Storm cluster, when a worker W1 sends requests to another worker W2 but W2 unfortunately crashes. The error handler of the W2 crash on the coordinator relocates the tasks served on W2 to W3. However, such a topology change will only be sent to W1 when the coordinator receives a heartbeat from W1. For a substantial period of time, W1 incorrectly thinks W2 is still alive and keeps sending requests to W2. These requests are never terminated and crash W1 through a runtime exception. W2&#8217;s status is misinterpreted by other workers who are sending requests to W2 and eventually further causing them to crash.<BR>Implication: Both the recovery task and the delay of the recovery task could interfere with request handling, resulting in vicious cycles. Testing techniques could inject delays before the recovery task to expose potential vicious cycles.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D. UC Subtype 2: Unconstrained Retry</P>
<P><BR>vicious cycles caused by contention delaying the heartbeat, the workload in each request handling logic should be reduced.<BR>Implication: Unconstrained retries could easily overload the server if no backoff or other feedback-based prevention tech- niques are in place. The server can proactively drop user requests under a high load, and the client should refrain from future retries once the request is rejected. A simpler alternative is to use exponential backoff in non-latency-sensitive requests, but the stop condition of such techniques should be scrutinized to prevent vicious cycles.<BR>&nbsp;&nbsp;&nbsp; VI. Triggering Conditions of Vicious Cycles<BR>As discussed in &#167;V and shown in the vicious cycle concep- tual graph (Figure 2b), vicious cycles need special conditions to trigger. Table III shows an overview of the triggering conditions for vicious cycles. Note that we only consider the necessary triggering conditions for each vicious cycle to reduce the testing space for automated testing tools.<BR>&nbsp;&nbsp;&nbsp; A. Special Constraints for Automated Testing Tools</P>
<P><BR>KAFKA-901 [37] is a vicious cycle caused by uncon- strained retries. When a Kafka broker (node) is down, the clients will need to access the metadata of the cluster, which is slow and cause IO pressure on the nodes holding the metadata. Moreover, the threads handling the metadata request share the same thread pool with that handling the metadata response, and a spike in the request could delay the responses due to contention, which in turn causes more retries from the clients and would eventually make the cluster unavailable.<BR>We find that commonly suggested prevention techniques such as exponential backoff are not universally applied to all retry procedures. However, as shown in our experiment (&#167;VIII-B), applying exponential backoff can be a quick mit- igation approach once the vicious cycle is discovered. Still, even for retry procedures with exponential backoff, it does not prevent vicious cycles completely because exponential backoff usually has a maximum backoff limit [89] and the same vicious cycle can still happen when the limit is reached.<BR>Though unconstrained retry also introduces contention, it is different from vicious cycles caused by contention delaying the heartbeat. To prevent vicious cycles caused by unconstrained retry, the retry procedure should be confined to reduce the number or frequency of requests. In comparison, to prevent<BR>As discussed in &#167;V, many vicious cycles are caused by prob- lematic retries that require special input constraints, including<BR>&nbsp;&nbsp;&nbsp; 1) inputs that trigger a timeout, and 2) inputs that trigger a latent logic bug or an error. For example, FLINK-12342 [21] requires an allocation request for a large number of YARN containers, whose processing time exceeds the timeout limit. Many vicious cycles start with a fault such as a node failure. For example, Hadoop-572 [26] (&#167;I) is triggered by a failure of a DataNode holding a large number of blocks. Although such a failure in turn triggers a heavy workload and violates the timing constraints of the heartbeat message, only the DataNode<BR>failure is necessary to trigger the bug.<BR>Implication: Traditional testing techniques such as fuzz test- ing [50] and fault injection [67] can help explore the input constraints and faults to expose vicious cycles. Fuzzing should prioritize commands and options vulnerable to vicious cycles, such as commands that could induce a large workload and the options controlling timeout and retry behaviors.<BR>TABLE IV: Fixing strategies to vicious cycles 1.</P>
<P>Major redesign<BR>%<BR>Other Fix<BR>%<BR>Separation of heavy workload<BR>6%<BR>Adjusting timeout<BR>9%<BR>Workload reduction<BR>18%<BR>Using asynchronous operation<BR>9%<BR>System-specific<BR>37%<BR>Improving performance<BR>6%</P>
<P><BR>Fixing logic errors<BR>21%<BR>Subtotal: Major redesign<BR>61%<BR>Subtotal: Other fix<BR>42%<BR>Workarounds<BR>27%</P>
<P>&nbsp;</P>
<P>For example, Hadoop-13738 [23] happens when the disks on many DataNodes in the cluster are full or nearly full. A logic error in the disk checker confuses the disk full with disk failures, causing a vicious cycle.<BR>Implication: Though vicious-cycle-triggering states cannot be exhausted developers could leverage the special states revealed in our study during testing to expose vicious cycles.<BR>&nbsp;&nbsp;&nbsp; B. Stress Testing is Useful to Trigger Vicious Cycles</P>
<P>Implication: Resource contention is a major interference caused by error handlers (&#167;V). Proactively limiting available resources and stress testing are useful to trigger many vicious cycles. Also, testing the systems&#8217; behavior after and beyond the overloading point is important, as the failure recovery afterwards can still trigger vicious cycles.<BR>&nbsp;&nbsp;&nbsp; C. Vicious Cycles Can Be Non-deterministic<BR>&nbsp;&nbsp;&nbsp; A. Patterns of Major System Redesigns</P>
<P>To fix Hadoop-572 [26] (&#167;I), the developers reduce the workload (generating block replication commands) in each heartbeat message by demanding a block report from any resurrected DataNodes. When a DataNode is temporarily marked dead at the NameNode side and later reconnects, the blocks available on that DataNode could be immediately available after the block report, thus greatly reducing the need for generating block replication commands.<BR>However, the fixes introduced in Hadoop-572 are not com- plete, and the same bug is captured again in a later JIRA ticket: Hadoop-923 [27]. It is fixed by a major redesign of the heartbeat handling logic, separating the unbounded workload<BR>- generating block replication requests - out of the heartbeat reply and creating a new command for this function. Combined with a better scheduling policy (e.g., FairCallQueue [19]), such a vicious cycle caused by contention is fixed.<BR>Implication: Any heavy workload in the distributed software systems should be measured for the latency and the overall turnaround time to avoid contention with time-sensitive op- erations such as the heartbeat. Such workloads can also be processed with a separate command or thread pool asyn- chronously. A better scheduling policy, such as prioritizing the heartbeat processing is also a good practice.</P>
<P>&nbsp;&nbsp;&nbsp; B. Other Fixing Patterns of Vicious Cycles</P>
<P>Exposing concurrency bugs in distributed software systems with concurrency testing [76], [78] and model checking [73], [83], [93] is extremely challenging. A combination of heavy workloads and small timing windows creates a large testing space, which challenges existing automated testing techniques. For example, as discussed in &#167;8, STORM-404 [39] is a vicious cycle that requires a fair amount of requests and a small time window. Specifically, since the delayed knowledge of the node failure directly leads to the vicious cycle, the only time window available to trigger the bug is one heartbeat interval, during which worker W2 has crashed but worker W1<BR>has not learned about the node failure yet.<BR>&nbsp;&nbsp;&nbsp; VII. Fix Strategies for Vicious Cycles<BR>We find that fixes to vicious cycles are highly complex and ad hoc. Table IV shows an overview of the fixing strategies for vicious cycles. There are two major fixing categories: 1) major redesign and 2) other fixes. The workaround row shows the percentage of fixes that are considered as a workaround instead of a complete fix by the developers themselves.<BR>1 Numbers do not add to 100%, because some vicious cycles are fixed by a combination of multiple fixing strategies.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Replacing a synchronous operation with its corresponding asynchronous version is a general pattern for fixing vicious cycles. For example, ZooKeeper-1049 [47] is fixed by making the TCP close() calls asynchronous. So that they do not block the heartbeat handlers in the same thread pool. Such a fix is considered a general pattern because it does not change the logic or the architecture of the system, compared to the sep- aration of heavy workloads, which changes request handling logic. Similarly, adjusting timeout is also a common practice, though developers often use it as a short-term workaround (&#167;VII-C). Applying performance optimization and fixing logic errors are typically system-specific.</P>
<P>&nbsp;&nbsp;&nbsp; C. Vicious Cycles Often Have Incomplete Fixes<BR>As the Hadoop-572 discussed above, vicious cycles are hard to fix. Though the root causes for each bug can be correctly identified by the developers, the fixes proposed in each bug ticket can still be incomplete, i.e., it does not prevent future occurrences of a similar vicious cycle.<BR>To get an objective result, we conservatively use the de- velopers&#8217; assessment to determine a fix&#8217;s completeness. If the fix patch is later admitted by the developers as a &#8220;short-term solution&#8221;, &#8220;workarounds&#8221;, or &#8220;mitigation&#8221;, we consider the patch incomplete. Using this criterion, we find that 27% of the fixes are incomplete, and the bug Hadoop-572 mentioned above is one of them, which is observed again in Hadoop-923.<BR>Implication: To determine the completeness of the fixes, vicious cycles can be revisited by triggering the original buggy execution. For example, if only the timeout limit is adjusted or a performance improvement is applied, developers can conduct stress testing again on the patched system.<BR>A previous study [60] found that 12% of the failure recovery bugs are not fixed completely. Our study indicates that vicious cycles are harder to be fixed than general failure recovery bugs, especially noting the fact that we only consider developers&#8217; direct confirmation of utilizing workarounds.</P>
<P>&nbsp;&nbsp;&nbsp; VIII. Preventing Vicious Cycles: A Feasibility Study<BR>We investigate the feasibility of preventing vicious cycles by 1) building a prototype tool that prevents deadly retries and<BR>&nbsp;&nbsp;&nbsp; 2) implementing a commonly suggested practice &#8211; exponential backoff &#8211; to prevent unconstrained retries. They account for 45% of the vicious cycles we studied. Their solutions also facilitate concrete discussions for more general solutions that apply to other types of vicious cycles. The limitations of our feasibility study are further discussed in &#167;IX.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. Preventing Deadly Retry<BR>Design. The key to preventing deadly retry is to identify the causal relationship between the error-inducing retried request and the error (&#167;V-B), This helps the system determine whom to blame and block the error-inducing request. Our tool identifies this causal relationship with a central monitor which correlates repeated requests with repeated errors on multiple nodes.<BR>For each node in the cluster, we deploy an agent to record recent outgoing RPC requests. The agent also forwards any error (e.g., ERROR and FATAL log messages, and crash) to the central monitor. The monitor collects such errors from each node and analyzes them to detect repeated errors across nodes. Once a repeated error is detected, the monitor broadcasts a message containing the set of nodes (S ) on which the error is repeated to the entire cluster. Each node replies with its recent outgoing RPC requests sent to the nodes in S . Upon receiving these RPC requests, the monitor calculates their signatures to identify the repeated requests sent to S and instruct the agents to block further retries of these requests.<BR>Implementation. We use AspectJ [71] to instrument the RPC library, and gRPC [22] to implement the agent-monitor<BR>TABLE V: Result on reproduced deadly retry vicious cycles. A tick in column P.? indicates that the vicious cycle is prevented. Column &#8220;Reason&#8221; explains why the vicious cycle is not prevented.</P>
<P><BR>communication. Each agent is implemented as an extra dae- mon thread, and all the RPC messages are recorded in an in-memory FIFO queue with a configurable fixed size.<BR>We use the string representation of RPC message content to identify retried RPC requests. To accommodate the potential variance of message content when the erroneous request is retried on different nodes, we identify a retried RPC message by calculating its edit distance [86] to the request signatures calculated by the monitor. If the string representation of the current outgoing RPC request is more than 90% similar to the signature, the agent will block this request from sending.<BR>A caveat of our approach is batched RPC requests. We leverage the repeated field modifier in Protocol Buffers [38] to identify a batched request. If the string representation of the repeated fields makes up more than 85% of the entire RPC message content, we consider the request as a batched RPC request. The batched RPC requests are then split into multiple sub-requests, and the identification of retried erroneous requests is carried out at the sub-request level.<BR>Though constructing string representations and computing edit distances are expensive, they are only performed when a repeated error is detected.<BR>Evaluation. We evaluate our tool on all the vicious cycles caused by deadly retry that we can reproduce. Table V shows that our tool successfully prevents HBase-14598 [28] and HBase-23076 [29] without interfering with the normal functionality of the system.<BR>Flink-10928 [20] cannot be prevented because it involves an OOM error that happens repeatedly but infrequently. If the FIFO queue size is too small, we cannot retrieve the retried request because it may have already been evicted. If the size is too large, the false positive rate will increase. The root cause of the OOM error is a memory leak. In the future, one may integrate a runtime memory leak detection tool [53], [54], [80] with our solution to prevent such vicious cycles.<BR>The remaining four cases are not reproduced due to 1) the non-determinism nature of the bug, 2) lack of information in the bug ticket, and 3) incompatible JDK version requirements.<BR>Impact on normal executions and limitation. We evaluate the overhead of our solution with six standard benchmarks from the &#8220;Yahoo! Cloud Serving Benchmark&#8221; [55] (YCSB), which is a popular benchmark for evaluating the performance of distributed software systems [57], [58], [66], [91]. Each of the benchmarks is run 5 times. The introduced overhead is 0.31% on average with a maximum of 0.98%.<BR>It is possible that our solution incorrectly blocks non-error- inducing requests if it incorrectly associates a normal request with an error (false positive). However, we do not observe such<BR>Fig. 3: An example of retry loop from Hadoop 2.4.0 [25].</P>
<P>TABLE VI: Result on reproduced unconstrained retry vicious cy- cles. A tick in column P.? indicates that the vicious cycle is prevented. Column &#8220;Reason&#8221; explains why the vicious cycle is not prevented.</P>
<P>Vicious Cycle<BR>P.?<BR>Reason<BR>Hadoop-16284 [24]<BR>G</P>
<P>HBase-27149 [30]<BR>G</P>
<P>Kafka-6028 [36]<BR>Flink-12342 [21]<BR>G<BR>Request piggybacked on heartbeat</P>
<P>cases in our evaluation. To balance between false positive and false negative rates, three parameters in the implementation are selected based on empirical experiments (thus, limited to HBase and Flink): 1) the size of the FIFO queue, 2) the threshold identifying the request to block, and 3) the threshold identifying batched requests.<BR>Discussion. Our tool shows the feasibility of preventing vicious cycles by making a more informed decision in error handling. It achieves this by collecting one type of error context &#8211; recent RPC requests &#8211; with low overhead. Preventing other vicious cycles (e.g., those that happen when the error handler cannot distinguish error induced by an external trigger and the interference of error recovery) requires different error contexts to be collected (e.g., information about the interfer- ence). In addition, the selected parameters need to be tuned for more systems and, if needed, automatically adjusted to achieve a good balance between false positive and false negative rates.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Preventing Unconstrained Retry<BR>Figure 3 shows an excerpt of a retry loop of RPC invocation in Hadoop [25]. The retry logic manifests as an infinite loop (L2), and the RPC invocation is enclosed by a try-catch block (L3). If an exception is thrown during the RPC invocation, a RetryPolicy (L6 and L7) decides 1) whether the request should be retried further (if not, rethrow the exception on L6), 2) how long should the thread wait until the next retry (L8).<BR>After reproducing each vicious cycle, we manually inspect the source code and identify the loops that can cause un- constrained retries. Then we inject exponential backoffs at the loops to test if the vicious cycle can be prevented. Our evaluation shows the feasibility of using exponential backoff to prevent vicious cycles. One promising future direction is to automate the process of identifying retry-inducing loops statically using patterns such as the one shown in Figure 3.<BR>Evaluation. As in Table VI, injecting exponential backoffs can prevent 75% of the vicious cycles reproduced. This ap- proach cannot address Flink-12342 [21] as the retried request is piggybacked on the heartbeat, which cannot be delayed.<BR>Impact on normal executions and limitation. Injecting exponential backoff may affect the correctness of the system because picking the optimal backoff parameters is challenging due to dependencies in timeout configurations. For example, in Hadoop-16284 [24], the retried request first lands on a server- side cache, when the backoff cap is set too large, the retried<BR>request always experiences cache miss, because the retrieved cache entry is evicted (timed out) before the next retry. In our experiments, we carefully inspect the source code and configuration file to make sure that the correctness of the system is not affected.<BR>Discussion. For vicious cycles induced by performance inter- ference, exponential backoff is an effective solution as long as the recovery task can be delayed. However, picking backoff pa- rameters safely without affecting the system&#8217;s correctness can be challenging. In addition, injecting exponential backoff on the client side could be undesired due to latency constraints. To automatically inject exponential backoff and prevent vicious cycles induced by performance interference, automatically identifying backoff locations, automatically setting safe back- off parameters, and dynamically adjusting backoff parameters to reduce latency are important future directions to explore.<BR>&nbsp;&nbsp;&nbsp; IX. Threats to Validity and Limitations<BR>Representativeness of bug reports from the issue tracker. There could be vicious cycles not reported to issue trackers. For instance, vicious cycles formed due to resource contention could be mitigated by changing configurations or adding resources. A mailing list can provide such solutions.<BR>Limitations of the filtering criteria. We could have missed vicious cycles whose issue reports do not contain our selected keywords for two reasons. 1) A vicious cycle could be treated as a symptom while the underlying bug forming the cycle is considered the root cause. Many reports on issue trackers mainly focus on the root causes, searching with keywords describing the symptom could result in false negatives. 2) Our multi-round boosting strategy is a best-effort strategy and could have missed some keywords.<BR>Representativeness of selected distributed software sys- tems. Our study does not include vicious cycles in closed- source systems and they could have different characteristics. Our study focuses on open-source software systems because we favor a source-code-level understanding of vicious cycles.<BR>Possible observer errors. There is a risk of observer errors. To minimize the effect, each failure was investigated by at least two inspectors with the same detailed criteria. Any disagreement is discussed in the end to reach a consensus.<BR>Limited number of vicious cycles investigated. The gen- eralizability of our findings may be affected by the number of vicious cycles included in our study. Our filtering process of the bug reports may miss some vicious cycles as many reports focus on the root cause and proposed fix, rather than detailing symptoms and logs. However, our findings cover a wide range of open-source distributed software systems on various workloads (&#167;III). This indicates that the findings are workload-agnostic and highlights the prevalence of vicious cycles in open-source systems.<BR>Limitations of the feasibility study. Our feasibility study is based on a limited number of reproduced vicious cycles related to the retry behavior of the system. For the unreproducible<BR>retry-related vicious cycles due to 1) the non-deterministic nature of the bug, 2) lack of information in the bug report, and 3) incompatible JDK or dependency version, we cannot evaluate the effectiveness of our tool on them. The feasibil- ity study also does not cover the non-retry-related vicious cycles due to their diversity in symptoms and programming paradigms. For example, the bug in Cassandra-13441 (&#167;V-A) is system-specific, and the bug in HDFS-12914 (&#167;V-C) is performance related. In addition, as explained in Section VIII, the effectiveness of our tools is affected by the parameters empirically selected.<BR>&nbsp;&nbsp;&nbsp; X. Related Work<BR>Distributed Software System Failure Study. Many stud- ies [48], [49], [59], [62]&#8211;[64], [69], [74], [79], [81], [85],<BR>[87], [88], [94], [97] have analyzed distributed software system failures. Gunawi et al. [62] conducted a comprehensive study of 3,655 high-priority issues in widely-used open-source dis- tributed software systems. Unfortunately, only one of the 3,655 issues [62] contains a vicious cycle, highlighting the difficulty of gathering such issues from open-source systems.<BR>There has been a growing interest in studying and address- ing cascading failures, such as incorrect failure recovery [65], cascading performance bugs [77], metastable failures [51], [68], and cascading virtual machine failure [92]. However, none of them defined and discussed vicious cycles in detail.<BR>Closest to our study are recent works on the metastable failure state [51], [68] when a system is under permanent overload due to a work amplification. While insightful, only resource contention and unconstrained retry are studied as the root cause. Our study reveals vicious cycles are much more diverse. Also, all 21 metastable failures were from proprietary cloud systems such as Amazon AWS, which hinders their reproduction and analysis for future research.<BR>Unlike Huang et al.&#8217;s [68] observation of high-level system behavior, our definition of vicious cycles focuses on the exe- cution trace of the system. Our approach provides a detailed understanding of the system behavior, especially the behavior of the error handler, which is useful for developing automated testing and runtime monitoring tools.<BR>Error handlers, such as exception handlers and crash re- covery procedures, have garnered substantial research inter- est [52], [60], [95] in recent years. However, none of them study vicious cycles. Our study complements these studies by analyzing vicious cycles in detail and reveals the reason why error handlers can lead to vicious cycles: the inability to distinguish errors caused by an external trigger and the interference due to lack of information (&#167;V). Our study also reveals that vicious cycles do not always involve explicit errors (e.g., exceptions and crashes) and incorrect handlers, but sometimes happen due to silent errors and missing handlers (&#167;V-A). In addition, our bug dataset [45] does not overlap with the crash recovery bug [16] and exception-related bug datasets [18].<BR>Guo et al. [65] provided a case study of 4 cascading failures that happen in the failure recovery stage in production<BR>commercial clusters. Our study corroborates their result. On the other hand, our study provides a deeper analysis of vicious cycles in open-source systems and proposes actionable suggestions to combat vicious cycles.<BR>Testing Tools. Recent works [73], [77], [82], [83], [90], [93] scaled automated system-level testing to distributed software systems by focusing the test effort on specific bug patterns. However, vicious cycles have yet to be covered.<BR>PCatch [77] detects performance cascading bugs by utilizing a causal relationships model [78]. Performance cascading bugs happen when a large user job delays another user job, but does not necessarily involve system degradation. In comparison, vicious cycles focus on the aggravating cycle between the system&#8217;s execution, including both user requests and internal requests, and system degradation.<BR>CrashTuner [82] utilizes log-guided fault injection [82] to automatically test for concurrency bugs that happen during failure recovery. Our study shows most vicious cycles are not caused by concurrency bugs.<BR>ScaleCheck [90] triggers scalability bugs by injecting sleep in CPU-intensive computations that involve scale-dependent data structures. Though some vicious cycles (e.g., Hadoop- 572) are easier to be triggered in a large-scale cluster, most vicious cycles do not have such a requirement.<BR>&nbsp;&nbsp;&nbsp; XI. Conclusion<BR>This paper provides the first in-depth analysis of vicious cy- cles in open-source distributed software systems. To overcome the challenge of collecting issues containing vicious cycles, we adopt a multi-round boosting strategy to expand our filtering criteria until it converges. We further analyze the symptoms, root causes, triggering conditions, and the fixing strategies of vicious cycles. Based on the findings, we present two approaches that can prevent vicious cycles caused by retries. Our study reveals 16 findings with concrete implications. In particular, we discuss implications and guidelines for runtime detection and prevention techniques, testing techniques, and good practices in fix and development strategies to avoid vicious cycles.