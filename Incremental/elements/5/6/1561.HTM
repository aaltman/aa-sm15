&nbsp;<BR>School of Information Technology and Engineering at the ADA University<BR>School of Engineering and Applied Science at the George Washington University 
<P></P>
<P>&nbsp;</P>
<P>RETRY POLICY REVIEW AND ANALYSIS</P>
<P><BR>A Thesis<BR>Presented to the Graduate Program of Computer Science and Data Analytics of the School of Information Technology and Engineering<BR>ADA University</P>
<P>&nbsp;</P>
<P>In Partial Fulfillment<BR>of the Requirements for the Degree<BR>Master of Science in Computer Science and Data Analytics ADA University</P>
<P>&nbsp;</P>
<P><BR>By Askarov Alakbar</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>April 2023<BR>&nbsp;&nbsp;&nbsp; 1 ABSTRACT.<BR>The distributed systems have a broad spectrum of use and run almost everywhere. The expansion of technology is now at a high rate. The applications based on distributed systems have high efficiency<BR>[22] and availability, but systems grow larger and more complex and need to handle failures. Retry is a ubiquitous mechanism for handling transient or short-lived failures by allowing the system to redo the failed request after a short period. One of the problems is happening due to the retry storms. It is a very unpredictable and hazardous application state with a long recovery time, or when the system falls to metastable failures, the retry storm self-sustains itself, and without intervention, the state will remain forever. This paper evaluates the retry storm situations and recovery techniques using different retry policies and will analyze and understand which is a relatively safer one based on the experiments done during the scope of this research. <FONT class=extract>To evaluate the retry policies, we have decided to create a testbed for the failed replication. We had to replicate the metastable failure at a small scale after building the testbed, and we were able to do so by using various testbed configurations to test. After reaching the two previous goals as a final contribution to the scope of this paper, we evaluated different retry policies and defined several outcomes. In conclusion, the study of retry storm situations and recovery techniques is of utmost importance to ensure the reliability and stability of distributed systems. Through our experiments and analysis, we have shown that the cancel retry policy is the most reliable and efficient approach for handling transient failures, while the simple and simple retry policies should be used with caution due to their potential to generate retry storms.</FONT> Our findings can serve as a valuable reference for system designers and developers when selecting appropriate retry policies for their distributed systems. As technology advances, it is crucial to continue researching and refining these techniques to ensure the robustness and resilience of distributed systems in the face of evolving challenges.<BR>Contents<BR>&nbsp;&nbsp;&nbsp; 1 Abstract&nbsp;2<BR>&nbsp;&nbsp;&nbsp; 2 Introduction.&nbsp;8<BR>&nbsp;&nbsp;&nbsp; 3 Background.&nbsp;10<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Exponential Backoff Retries&nbsp;11<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Adjustable Retry Policies&nbsp;11<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Simple Retry Policies&nbsp;13<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Retry Storm Antipattern&nbsp;14<BR>&nbsp;&nbsp;&nbsp; 4 Methods and Materials.&nbsp;14<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Testing environment&nbsp;14<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Front-end Environment&nbsp;16<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Workload Simulation Environment&nbsp;18<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4 Workload Simulation Structure&nbsp;18<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5 Workload Simulation Workflow&nbsp;19<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6 Workload Simulation Data&nbsp;21<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.7 Middleware Environment&nbsp;21<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.8 Database Environment&nbsp;21<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.9 Limitations of the system&nbsp;22<BR>&nbsp;&nbsp;&nbsp; 5 Research Results&nbsp;22<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Cancel Retry Policy&nbsp;24<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Simple Retry Policy&nbsp;25<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Simple Delay Retry Policy&nbsp;27<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4 Incremental Backoff Retry Policy&nbsp;28<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5 Exponential Backoff Retry Policy&nbsp;31<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.6 Fibonacci Backoff Retry Policy&nbsp;33<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.7 LILD Retry Policy&nbsp;35<BR>&nbsp;&nbsp;&nbsp; 6 Evaluation of Results&nbsp;37<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Simple Retry Policy vs Simple Delay Retry Policy&nbsp;37<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2 Incremental Backoff Retry Policy vs Exponential Backoff Retry Policy vs Fibonacci Backoff Retry Policy&nbsp;37<BR>&nbsp;&nbsp;&nbsp; 7 Future Work&nbsp;38<BR>&nbsp;&nbsp;&nbsp; 8 Conclusion&nbsp;40<BR>&nbsp;&nbsp;&nbsp; 9 References&nbsp;41<BR>LIST OF FIGURES<BR>No&nbsp;Figure Caption&nbsp;Page</P>
<P>&nbsp;&nbsp;&nbsp; 1 Metastable failures states. The figure is redrawn from &#8220;Metastable failure in the wild&#8221; [1]&nbsp;11<BR>&nbsp;&nbsp;&nbsp; 2 Timeseries of a core service under a peak load test at Twitter. Metrics are normalized&nbsp;11<BR>except for the success rate, which is scaled to show the trend dropping below the SLO. The figure is redrawn from &#8220;Metastable failure in the wild&#8221; [1]<BR>&nbsp;&nbsp;&nbsp; 3 The architecture of testing application.&nbsp;17<BR>&nbsp;&nbsp;&nbsp; 4 Retry Policy Testing Environment Front-end Part.&nbsp;18<BR>&nbsp;&nbsp;&nbsp; 5 Configuration settings&nbsp;19<BR>&nbsp;&nbsp;&nbsp; 6 Class diagram of Requester.&nbsp;21<BR>&nbsp;&nbsp;&nbsp; 7 Workflow diagram&nbsp;22<BR>&nbsp;&nbsp;&nbsp; 8 Timeline diagram&nbsp;22<BR>&nbsp;&nbsp;&nbsp; 9 A number of requests and retry requests, status codes, and average response time for the&nbsp;26<BR>requests for cancel retry policy. The loading period till the 120th second. The vulnerable period till the 150th second. The trigger started at the 150th second and finished at the 330th second.<BR>&nbsp;&nbsp;&nbsp; 10 Succes rate of the requests for cancel retry policy. The loading period till the 120th&nbsp;26<BR>second. The vulnerable period till the 150th second. The trigger started at the 150th second and finished at the 330th second.<BR>&nbsp;&nbsp;&nbsp; 11 A number of requests and retry requests, status codes, and average response time for the&nbsp;27<BR>requests for simple retry policy. The loading period till the 200th second. The vulnerable period till the 350th second. The trigger started at the 350th second and finished at the 410th second. The generation of new requests finished at the 600th second.<BR>&nbsp;&nbsp;&nbsp; 12 Succes rate of the requests for simple retry policy. The loading period till the 200th&nbsp;28<BR>second. The vulnerable period till the 350th second. The trigger started at the 350th second and finished at 410th second. The generation of new requests finished at the 600th second.<BR>&nbsp;&nbsp;&nbsp; 13 A number of requests and retry requests, status codes, and average response time for the&nbsp;29<BR>requests for simple delay retry policy. The loading period till the 300th second. The vulnerable period till the 400th second. The trigger started at the 500th second and finished at the 600th second. The generation of new requests finished at the 650th second.<BR>&nbsp;&nbsp;&nbsp; 14 Success rate of the requests for simple delay retry policy. The loading period till the 300th&nbsp;30<BR>second. The vulnerable period till the 400th second. The trigger started at the 500th second and finished at the 600th second. The generation of new requests finished at the 650th second.<BR>&nbsp;&nbsp;&nbsp; 15 A number of requests and retry requests, status codes, and average response time for the&nbsp;30<BR>requests for incremental backoff retry policy. The loading period till the 100th second. The vulnerable period till the 115th second. The trigger started at the 115th second and finished at the 450th second. The generation of new requests finished at the 650th second.<BR>&nbsp;&nbsp;&nbsp; 16 Succes rate of the requests for incremental backoff retry policy. The loading period till&nbsp;31<BR>100th second. The vulnerable period till the 115th second. The trigger started at the 115th second and finished at the 450th second. The generation of new requests finished at the 650th second.<BR>&nbsp;&nbsp;&nbsp; 17 A number of requests and retry requests, status codes, and average response time for the&nbsp;33<BR>requests for exponential backoff retry policy. The loading period till the 180th second. The vulnerable period till the 210th second. The trigger started at the 210th second and finished at the 400th second. The generation of new requests finished in the 750th second.<BR>&nbsp;&nbsp;&nbsp; 18 Succes rate of the requests for exponential backoff retry policy. The loading period till the&nbsp;34<BR>180th second. The vulnerable period till the 210th second. The trigger started at the 210th second and finished at the400th second. The generation of new requests finished in the 750th second.<BR>&nbsp;&nbsp;&nbsp; 19 A number of requests and retry requests, status codes, and average response time for the&nbsp;35<BR>requests for Fibonacci backoff retry policy. The loading period till the 120th second. The vulnerable period till the 140th second. The trigger started at the 140th second and finished at the 550th second. The generation of new requests finished at the 790th second.<BR>&nbsp;&nbsp;&nbsp; 20 Succes rate of the requests for exponential backoff retry policy. The loading period till the&nbsp;36<BR>120th second. The vulnerable period till the 140th second. The trigger started at the 140th second and finished at the 550th second. The generation of new requests finished at the 790th second.<BR>&nbsp;&nbsp;&nbsp; 21 A number of requests and retry requests, status codes, and average response time for the&nbsp;38<BR>requests for LILD retry policy. The loading period till the 110th second. The vulnerable period till the 130th second. The trigger started at the 130th second and finished at the 360th second. The generation of new requests finished in the 750th second.<BR>&nbsp;&nbsp;&nbsp; 22 Succes rate of the requests for LILD retry policy. The loading period till the 110th second.&nbsp;38 The vulnerable period till the 130th second. The trigger started at the 130th second and<BR>finished at the 360th second. The generation of new requests finished in the 750th second.<BR>&nbsp;&nbsp;&nbsp; 23 Simple Retry on the left vs Simple Delay Retry on the right.&nbsp;39<BR>&nbsp;&nbsp;&nbsp; 24 Circuit Breaker pattern work diagram. Retrieved from Microsoft [20]&nbsp;41<BR>LIST OF TABLES<BR>No&nbsp;Table Caption&nbsp;Page</P>
<P>&nbsp;&nbsp;&nbsp; 1 The number of requests generated in the time frame by incremental backoff policy.&nbsp;32<BR>&nbsp;&nbsp;&nbsp; 2 The number of requests generated in the time frame by exponential backoff policy.&nbsp;34<BR>&nbsp;&nbsp;&nbsp; 3 The number of requests generated in the time frame by Fibonacci backoff policy.&nbsp;37</P>
<P><BR>Abbreviation&nbsp;Explanation<BR>LIST OF ABBREVIATIONS</P>
<P>AWS&nbsp;Amazon Web Services<BR>SLO&nbsp;Service-level objective<BR>RPS&nbsp;Requests per second<BR>CPAN&nbsp;Comprehensive Perl Archive Network<BR>LILD&nbsp;Linear Increment Linear Decrement<BR>LIMD&nbsp;Linear Increment Multiplicative Decrement<BR>MILD&nbsp;Multiplicative Increment Linear Decrement MIMD&nbsp;Multiplicative Increment Multiplicative Decrement CPU&nbsp;Central Processing Unit<BR>DB&nbsp;Database<BR>SQL&nbsp;Structured Query Language<BR>MVC&nbsp;Model View Controller<BR>JVM&nbsp;Java Virtual Machine</P>
<P>&nbsp;&nbsp;&nbsp; 2 INTRODUCTION.<BR>The reliability of applications is crucial in the modern environment. Applications today frequently have numerous interdependent components, are huge in size, and rely heavily on cloud technology. For an app to function as intended, all these components must be in good working order.<BR>Unfortunately, despite our best efforts to produce dependable software, mistakes can and do happen. These errors can have serious repercussions for both consumers and organizations.<BR>One egregious instance of a reliability problem occurred in December 2021, when an Amazon Web Services (AWS) outage disrupted smart pet food dispensers, Roomba vacuum cleaners, and Amazon shopping. Numerous customers were irritated and inconvenienced by this outage, underscoring how crucial it is for programs to be dependable and robust [25].<BR>Furthermore, reliability problems are not just found in well-publicized outages like this one. Even minor, less well-known failures can have a big influence on users' day-to-day activities, resulting in missed output, diminished corporate profits, and even serious safety issues.<BR>Given the significance of application reliability, it is imperative that developers and companies give dependability top priority during the software development process. To lessen the effects of any failures that may occur, this includes rigorous testing and quality assurance procedures as well as methods for resilience and disaster recovery. The success, safety, and productivity of both enterprises and consumers depend on the dependability of the applications being used. This is not merely a matter of convenience or user experience.<BR>Building such systems has become a goal for most companies, and at the same time, these applications should be reliable. A lot of engineering and research work has gone into the reliability and fault tolerance domain. While cloud technology has undoubtedly brought many benefits to modern software development, it has also introduced new challenges, such as the risk of metastable failures which is the focus of this paper. The reasons for metastable failure could vary and are mostly related to optimization issues, including but not limited to persistent congestion, overload, cascading failures, retry storms, death spirals, and others [1]. In this paper, we make an accent on the retry storm antipattern. This failure pattern often occurs because of excessive retries on the overloaded system. This pattern, called metastable failures, was first introduced by Bronson et al. [20]. It is characterized by &#8220;permanent overload with an ultra-low goodput&#8221; [20] and defined in three metastable states:<BR>&nbsp;&nbsp;&nbsp; 1. Metastable failure state - the state of a permanent overload with an ultra-low goodput.<BR>&nbsp;&nbsp;&nbsp; 2. Stable state - the state when a system experiences a low enough load that it can successfully recover from temporary overloads.<BR>&nbsp;&nbsp;&nbsp; 3. Vulnerable state - the state when a system experiences a high load, but it can successfully handle that load in the absence of temporary overloads.</P>
<P>Figure 1: Metastable failures states. The figure is redrawn from &#8220;Metastable failure in the wild&#8221; [1]</P>
<P>The topic was further explored in &#8220;Metastable Failures in the Wild&#8221; [1] and focuses attention on real-world failures and formally defining metastable failures. The system in a vulnerable state could be triggered after it changes to a metastable failure state, and after removing the trigger cannot restore to the stable or vulnerable state and remains in the metastable failure one [1, 20].</P>
<P>Figure 2: Timeseries of a core service under a peak load test at Twitter. Metrics are normalized except for the success rate, which is scaled to show the trend dropping below the SLO. The figure is redrawn from &#8220;Metastable failure in the wild&#8221; [1]<BR>The main cause of the failure is sustaining effect loop, which could not be predicted easily; hardware or software could be the reason for it; hence they are unpredictable. However, the trigger could be identified and typically known beforehand. Due to efficiency and cost perspectives, most applications run in a vulnerable state all the time. The paper provided several cases of failures, some of which lasted for more than 6 hours. The failure starts when the system cannot handle a request rate, and after several of them fail, it leads to more failures, and eventually, all systems fail at an exponential rate. The problem is old but has a limited number of cases [1] and preventing the problem now while it has no major effect is crucial.<BR>Amazon, Google, and Microsoft [1] already have faced this problem, even with all optimization techniques and advanced technologies both on the software and hardware side. One of the problems the companies adjusted was a retry storm. A retry storm [5] is a special state of the application when due to the timeout, the clients start to retry the queries and artificially increase the load by filling up the queue with retry requests. This makes the normal requests per second (RPS) rate double on each iteration preventing normal operations from executing that leading them to<BR>retry. This feedback loop is hard to avoid when it happens and even harder to restore the system from it. For example, let&#8217;s imagine a situation where the max RPS of the system is 100 while using the caching with an 80% of hit rate could operate at an average of 300-400 RPS. After triggering the system, the hit rate of the cache decreased to 0 for a couple of seconds. What happens next is the requests will wait for much more than expected and, due to timeout, will retry them. This will increase the load from normal 300-400 RPS to 700-800 RPS which will lead to the point that a number of requests will overwhelm the operation rate of the system and decrease the goodput of the system to the bare minimum while this recursion normally will continue till the cut of all requests.<BR>Finding a method to prevent this behavior or at least finding the best retry policy for this approach will prevent companies from struggling and decrease the potential losses in profit terms since while these failures occur the system is not operatable for hours and preventing the client from using the application.<BR>Our research study aims to develop a testbed that can replicate metastable failures caused by retries in software systems. To that point, we have three goals. The first goal is to create a testbed to replicate the failures. The second goal is to replicate the retry-induced metastable failures at a small scale. And finally, the third contribution we make is studying different retry strategies and their impact on metastable failures.<BR>To create a testbed that generates a large number of retries, simulating a real-world scenario where retries occur due to various reasons, such as network congestion or system overload.<BR>To successfully replicate the metastable failure at a small scale by identifying the key factors that contribute to the failure and designing experiments to recreate those conditions in a controlled environment. This will help us gain a better understanding of the underlying causes of metastable failures and how they can be prevented or mitigated.<BR>To investigate different, retry policies and their impact on metastable failures. We aim to identify which policies are most effective at preventing these types of failures, providing valuable insights for software developers and system administrators in terms of optimizing their retry policies to minimize the risk of metastable failures.<BR>Our three goals are interconnected and represent a comprehensive approach to studying metastable failures caused by retries in software systems. By successfully achieving these goals, we will provide valuable insights into the nature of these failures and how to prevent them in the future.</P>
<P>&nbsp;&nbsp;&nbsp; 3 BACKGROUND.<BR>Retry policy is an algorithm that allows applications to retrieve answers in case of temporary server unavailability. For example, if the server was experiencing transient network failure for some moment or for several requests, then some clients&#8217; requests may timeout or fail. Retrying<BR>these requests may allow the clients to mask such transient failure. There is a lot of variation of retry policy algorithms that have a constant timeout, adjustable one, limited, etc.<BR>A wide variety of retry strategies, or as we call them, retry policies exist, catering to different usage patterns. Some are monotonic: retry happens after a constant interval of time. Some are scaling: calculates the last retries and adjusts the time depending on the failure request number. Most are compatible with different patterns: limiting the number of retries, using retry budgets, setting max retry time, etc. Each company has its own approach to the policy, some propose for technical stakeholders to use a specific pattern, and others propose to decide on the policy depending on the purposes of the application.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Exponential Backoff Retries<BR>Google and several other companies [2, 6, 9, 18, 21] propose an exponential backoff algorithm to run for stakeholders of applications.<BR>The exponential backoff algorithm uses some base timeout and doubles it for each next iteration to provide smooth working of the service. The algorithm runs as follows:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Make the first request.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. If failed wait 1 + random number of milliseconds.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. If failed wait 2 + random number of milliseconds.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. If failed wait 4 + random number of milliseconds.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5. Continue till reaches max backoff time limit.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6. Continue waiting and retrying up to some maximum number of retries, but do not increase the waiting period between retries.<BR>The wait time is &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;(((2^&#55349;&#56411;) + &#55349;&#56415;&#55349;&#56398;&#55349;&#56411;&#55349;&#56401;&#55349;&#56412;&#55349;&#56410;_&#55349;&#56411;&#55349;&#56418;&#55349;&#56410;&#55349;&#56399;&#55349;&#56402;&#55349;&#56415;_&#55349;&#56410;&#55349;&#56406;&#55349;&#56409;&#55349;&#56409;&#55349;&#56406;&#55349;&#56416;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56411;&#55349;&#56401;&#55349;&#56416;), &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;&#55349;&#56406;&#55349;&#56410;&#55349;&#56418;&#55349;&#56410;_&#55349;&#56399;&#55349;&#56398;&#55349;&#56400;&#55349;&#56408;&#55349;&#56412;&#55349;&#56403;&#55349;&#56403;), with n incremented by 1 for each iteration (request).<BR>&#55349;&#56415;&#55349;&#56398;&#55349;&#56411;&#55349;&#56401;&#55349;&#56412;&#55349;&#56410;_&#55349;&#56411;&#55349;&#56418;&#55349;&#56410;&#55349;&#56399;&#55349;&#56402;&#55349;&#56415;_&#55349;&#56410;&#55349;&#56406;&#55349;&#56409;&#55349;&#56409;&#55349;&#56406;&#55349;&#56416;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56411;&#55349;&#56401;&#55349;&#56416; is a random number of milliseconds less than or equal to 1000. This helps to avoid cases in which many clients are synchronized by some situation, and all retry at once, sending requests in synchronized waves. The value of &#55349;&#56415;&#55349;&#56398;&#55349;&#56411;&#55349;&#56401;&#55349;&#56412;&#55349;&#56410;_&#55349;&#56411;&#55349;&#56418;&#55349;&#56410;&#55349;&#56399;&#55349;&#56402;&#55349;&#56415;_&#55349;&#56410;&#55349;&#56406;&#55349;&#56409;&#55349;&#56409;&#55349;&#56406;&#55349;&#56416;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56411;&#55349;&#56401;&#55349;&#56416; is recalculated after each retry request. &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;&#55349;&#56406;&#55349;&#56410;&#55349;&#56418;&#55349;&#56410;_&#55349;&#56399;&#55349;&#56398;&#55349;&#56400;&#55349;&#56408;&#55349;&#56412;&#55349;&#56403;&#55349;&#56403; is typically 32 &#55349;&#56412;&#55349;&#56415; 64 seconds. The appropriate value depends on the use case. Most companies propose to add some limit to the applications not to spam themselves and have a chance to recover after a prolonged shutdown.<BR>The &#8220;&#55349;&#56400;&#55349;&#56398;&#55349;&#56417;&#55349;&#56416; &#8722; &#55349;&#56415;&#55349;&#56402;&#55349;&#56417;&#55349;&#56415;&#55349;&#56422;&#8221; and CPAN [18, 21] libraries, in addition to exponential backoff, also provide functionality for Fibonacci backoff algorithms that, as refers in its name, adjust the next delay time as the sum of the previous two and rise to some limit.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Adjustable Retry Policies<BR>The CPAN [21] also describes 4 types of backoff:<BR>LILD - Linear Increment, Linear Decrement backoff. Using predefined variables this backoff algorithm calculates the current wait time and sends this time to the clients depending on the sequence of the requests. For the failure request acts as follows:<BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;</P>
<P>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;(&#55349;&#56410;&#55349;&#56406;&#55349;&#56411;(&#55349;&#56375;1 + &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;), &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. ..<BR>For the success request:</P>
<P><BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421; ( &#55349;&#56375;1 + &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416;, &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;, &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422; )<BR>. ..<BR>&#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;, &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, and &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416; are required for the proper work of algorithms.<BR>LIMD - Linear Increment, Multiplicative Decrement backoff. As previously it calculates the delay time depending on previous requests but acts a little differently this time. Upon failure, this backoff algorithm calculates the next delay as:<BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;(&#55349;&#56375;1 + &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. . .<BR>Upon success, the next delay is calculated as:<BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;(&#55349;&#56375;1 &#8727; &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416;, &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. . .<BR>&#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;, &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, and &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416; are required.<BR>MILD - Multiplicative Increment, Linear Decrement backoff uses the same approach as the previous one but vise-versa. Upon failure, this backoff algorithm calculates the next delay as:<BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;(&#55349;&#56410;&#55349;&#56406;&#55349;&#56411;(&#55349;&#56375;1 &#8727; &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;), &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. . .<BR>Upon success, the next delay is calculated as:<BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;(&#55349;&#56375;1 + &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416;, &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;, &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. . .<BR>&#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;, &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, and &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56406;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416; are required.<BR>&#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422; and &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422; should be larger than zero; otherwise, the next delays will all be zero.<BR>MIMD - Multiplicative Increment, Multiplicative Decrement backoff uses multiple for both scenarios. Upon failure, this backoff algorithm calculates the next delay as:</P>
<P><BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;(&#55349;&#56410;&#55349;&#56406;&#55349;&#56411;(&#55349;&#56375;1 &#8727; &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;), &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. . .<BR>Upon success, the next delay is calculated as:</P>
<P><BR>&#55349;&#56375;1 = &#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;<BR>&#55349;&#56375;2 = &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;(&#55349;&#56410;&#55349;&#56406;&#55349;&#56411;(&#55349;&#56375;1 &#8727; &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416;, &#55349;&#56410;&#55349;&#56398;&#55349;&#56421;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;), &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;)<BR>. . .<BR>&#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;, &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56403;&#55349;&#56398;&#55349;&#56406;&#55349;&#56409;&#55349;&#56418;&#55349;&#56415;&#55349;&#56402;, and &#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422;_&#55349;&#56410;&#55349;&#56418;&#55349;&#56409;&#55349;&#56417;&#55349;&#56406;&#55349;&#56413;&#55349;&#56409;&#55349;&#56402;_&#55349;&#56412;&#55349;&#56411;_&#55349;&#56416;&#55349;&#56418;&#55349;&#56400;&#55349;&#56400;&#55349;&#56402;&#55349;&#56416;&#55349;&#56416; are required.<BR>&#55349;&#56406;&#55349;&#56411;&#55349;&#56406;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422; and &#55349;&#56410;&#55349;&#56406;&#55349;&#56411;_&#55349;&#56401;&#55349;&#56402;&#55349;&#56409;&#55349;&#56398;&#55349;&#56422; should be larger than zero; otherwise, the next delays will all be zero.<BR>Also, CPAN recommends adding a &#8220;jitter factor, e.g., 0.25 to add some randomness to avoid &#8216;thundering herd problem.&#8217;&#8221; and a maximum limit on attempts and total duration. [21]</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 <FONT class=extract>Simple Retry Policies<BR>In its documentation notes, Microsoft [4, 6] describes the general principles of the retry policy algorithms, why it is needed, and 3 types of retry policies.<BR>The cancel retry policy.<BR>The main area of policy usage is for failures that are not transient or predictable to be unchancy to succeed if repeated. The application suggested dropping this query and proceeding to the next one. An example of an operation could be dissatisfaction with the query requirement, such as a lack of data, improper data, or absence of credentials.<BR>The simple retry policy.<BR>In case of facing the unlike error occurring during the request processing or delivery, the application can instantly retry the request since it is mainly impossible to get the error twice, and most probably, the operation will provide success.<BR>The retry after delay policy.<BR>When a high load appears, or temporary server unavailability happens, the application can retry after some period to wait for the load to decrease or for the server to recover. However, such busy failures and connectivity errors require some time, so the policy should adjust the proper amount of time depending on the case.<BR>For requests after delay, Microsoft proposes configuring delays and the limit number of retries. For example, the configuration of adding delay could be incremental (1,2,3,4) or exponential (1,2,4,8).<BR>While discussing the issues, they propose to make the configurations fit business logic. If it fails after a considerable number of requests, as suggested in the documentation, close the system, and report the failure immediately. Moreover, they proposed to check for Circuit Breaker patterns.</FONT></P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 <FONT class=extract>Retry Storm Antipattern<BR>Retry Storm antipattern is well described in Microsoft [5] documentation. The retry storm antipattern is a kind of failure described previously in this paper and the primary purpose of the research. In this document, they describe the metastable failure caused by a retry storm and some assumptions on how to prevent it. They suggested not using a self-made retry policy but ready ones, checking Circuit Breaker patterns, adding something to shut down the system, delays between retries, and much more. They also described how to detect such kinds of errors using telemetry tools. Additionally, to avoid this behavior they propose using the randomization of retries, in general it is just simply adding some random number of milliseconds to the wait time so there are no cases when too many requests are performed at the same time.</FONT></P>
<P>&nbsp;&nbsp;&nbsp; 4 METHODS AND MATERIALS.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Testing environment<BR>Real-world production system troubleshooting can be very difficult and time-consuming, resulting in considerable delays and expenses. Additionally, reproducing failures at smaller scales is often difficult, as metastable failures should run for some period of time and have enough computational power to handle CPU load [1]. There should be a big enough RPS ratio as well as small enough computation power on the DB side. At scale and cloud, it is easy to achieve since you have full control of cloud resources. Therefore, we need a controlled testing environment that simulates a real-like stateful application to obtain accurate and reproducible test results.<BR>To achieve this goal, we have designed a simulation testbed [26] that can mimic real-world scenarios and enable testing of different retry policies. With the help of this testbed, we can compare various system designs, settings, and policies under diverse circumstances without<BR>endangering the reliability of the live system. Additionally, the testbed enables us to assess the system's functionality and spot any potential problems that might emerge in various scenarios. To create solid and dependable systems that can satisfy the needs of users and companies alike, a testbed system is essential.<BR>The testbed itself consists of 4 components as shown in Figure 2.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. VueJS front-end for control and real-time statistics observation.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Java Spring Boot based workload simulation client.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. Java Spring Boot based middleware for implementing server-side techniques and database communications.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Cassandra database running in Docker container.</P>
<P>Figure 3: The architecture of testing application.<BR>The environment works with table &#8216;Profiles&#8217; generated in the Cassandra DB and sends 5 types of requests:<BR>&nbsp;&nbsp;&nbsp; 1. Create request.<BR>&nbsp;&nbsp;&nbsp; 2. Update request.<BR>&nbsp;&nbsp;&nbsp; 3. Delete request.<BR>&nbsp;&nbsp;&nbsp; 4. Get request.<BR>&nbsp;&nbsp;&nbsp; 5. Count request.<BR>We attempted to simulate a real-world app with standard operations and conduct experiments that closely reflect reality by using this type of request.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Front-end Environment<BR>The VueJS front-end application is required for real-time control of the test process. The front end allows us to see the statistics and control parameters of the testing including testing configuration that consist of the limit of the database size, the number of requests per second, and base timeout for retry, as well as the chosen retry technique.</P>
<P>Figure 4. Retry Policy Testing Environment Front-end Part.<BR>The front part consists of 3 blocks:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Graphics<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Testing Configurations Control<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. Testing Control<BR>In the graphics, the researcher can observe 3 parameters:<BR>&nbsp;&nbsp;&nbsp; 1. The response codes of the request and the number of them<BR>&nbsp;&nbsp;&nbsp; 2. The average response time of requests made each second.<BR>&nbsp;&nbsp;&nbsp; 3. The success rate of the requests made in the second.<BR>Additionally, there is the total number of regular requests and retry requests in the top left corner. The front-end sends a request to the requester with debounce of 5 seconds to update statistics and<BR>refresh graphics. Response codes, average response time, and success rate calculated on the requester side and front just shows ready data.<BR>The testing configuration control has two controllers, one for creating Testing Configuration and the second for selecting a previously created configuration for a current test session.<BR>While creating testing configuration the researcher should specify several configuration metrics that include:<BR>&nbsp;&nbsp;&nbsp; 1. The &#8220;Request per Second&#8221; will set the load to the server.<BR>&nbsp;&nbsp;&nbsp; 2. the &#8220;Database Limit&#8221; is the limiting configuration responsible for the size of the database since the DB size affects the overall performance of the system and can slow down the speed of the operations.<BR>&nbsp;&nbsp;&nbsp; 3. The &#8220;Base Timeout&#8221; stands for the first retry timeout, and all others are calculated based on this value.<BR>&nbsp;&nbsp;&nbsp; 4. The &#8220;Retry Policy&#8221; is the most important param which means the Retry Policy is used during testing.</P>
<P>Figure 5: Configuration settings<BR>The requester receives all configurations and saves them to the database for future use. In the selection part, the requester displays a list of all created configurations, and for each configuration, there are two buttons: one to delete and another one to select it. When the user presses the second button, the front end sends the ID of the selected configuration to the requester, which then saves it as the working configuration locally.<BR>Testing control is just a set of buttons that allow start, stop, and pause testing any moment, but the researcher should choose at any retry policy beforehand.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Workload Simulation Environment<BR>The Spring Boot workload simulation client or requester is an asynchronous service that runs on a Java Machine and serves as a simulation of clients sending requests to the middleware back-end service that is as well the Java-based tool. During testbed development, the first decision was to include workload simulation in the front-end part, but the browser limits the number of outgoing requests and gives errors. Due to this limitation, we have decided to create a separate service for workload simulations. The workload simulation service also gathers statistics such as average time per query by second, percent of successful requests per second, and status codes returned as well as the number of normal requests and retry requests.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.4 Workload Simulation Structure<BR>During the writing testbed, we decided to stick to the MVC pattern for our project due to its simplicity and logical organization. In the requester, we write two controllers that are responsible for:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. The testing controller and stats gathering combined controller.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. The testing configuration controller.</P>
<P>Figure 6. Class diagram of Requester.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.5 Workload Simulation Workflow<BR>The core part of the workload environment happens in class which gives the command to start the simulation. The requester and middleware parts of workload simulation are running asynchronously to handle parallel connections and not to wait for the response of each operation. The service randomly sends requests to the middleware and subscribes for the response event. If the service catches a timeout exception, it sets an event with some delay that is stated in configurations to start retrying the request and add the record of the current request to stats as a failed attempt. The retry request behaves similarly, but changes delay depending on the selected retry politics.</P>
<P><BR>Figure 7. Workflow diagram</P>
<P>Figure 8. Timeline diagram<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.6 Workload Simulation Data<BR>The stats and configurations are stored and changed differently. Configuration is stored in the Singleton Instance that represents the class with static variables for global access. Additionally, the Singleton Instance stored the current state of the testbed: &#8216;START&#8217;, &#8216;PAUSE&#8217;, or &#8216;STOP&#8217; that is checked by the requester before each operation to proceed accordingly. Moreover, here is the list of all ids of the generated &#8216;Profiles&#8217; so the requester could select a random profile for operation.<BR>The stats itself is a bunch of statistics used to measure the performance and load of the system as well as goodput. The statistics consist of 3 parameters:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. The response codes of the request and the number of them<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. The average response time of requests made in the second.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. The success rate of the requests made in the second.<BR>We have three performance statistics to measure the performance of our middleware system. The first one is the total number of response codes for all requests. To simplify, we assigned the response code 200 for all possible results coming from the database, indicating that the middleware runs as expected without delays. However, if the middleware cannot proceed on time, it returns a 504 error code. There are still some unexpected errors, such as 405, indicating trouble in establishing a connection with the middleware.<BR>The second statistic is the average wait time for the requests sent in a second. This statistic helps us to see how much time it takes for requests and how the performance changes over time.<BR>The third statistic is the percentage of successful requests sent in a second out of all requests. We did not separate the stats by the type of request since both types affect the system. However, we counted the number of normal requests and the number of retry requests to get a better understanding of the overall picture.<BR>Overall, these three statistics provide us with valuable insights into the performance of our middleware system, helping us to identify potential issues and improve the system's performance.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.7 Middleware Environment<BR>The Spring Boot middleware service is used for applying back-end side techniques for preventing metastable failures and using methodologies for preventing retry storm antipattern such as retry budget that is described as &#8220;a mechanism that limits the number of retries that can be performed against a service as a percentage of original requests. This prevents retries from overwhelming your system.&#8221; as stated in Linkerd [13]. It is also responsible for database direct connection and interprets queries coming from the work loader to the language of SQL.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.8 Database Environment<BR>We use the Apache Cassandra database as our main database system since it is distributed, has a balance loader load-balanced, and is relatively easy to raise using on one machine. M. C. Narra<BR>[12] describes the way to raise distributed clusters and maintain them. Initially, we decided to raise<BR>the cluster with three nodes running inside, but due to the limitation of hardware, we decided to stick with just one Cassandra node.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.9 Limitations of the system<BR>During the creation of the testbed, we encountered challenges due to limited computational power. These challenges included difficulties in reaching the metastable failure state, as well as longer times required to find the optimal configuration for testing. Additionally, we faced issues such as the system experiencing a "Blue Screen of Death" (BSOD), which necessitated restarting the entire test from scratch. Despite these challenges, we were able to increase the number of threads and retries to load the system, albeit with a decrease in the overall request per second (RPS) to 18-20 RPS, which was not originally intended. This decrease in RPS may have made it harder to reach the metastable failure state, but with persistent efforts, we were able to achieve it.<BR>The occurrence of BSOD further added to the time required to restart the system and commence a new attempt at the test, causing delays and extending the overall test duration. Nevertheless, despite these limitations, we were able to conduct the tests and obtain valuable insights into the behavior of the retry policies under various conditions. These challenges highlight the importance of considering the limitations of the test environment and infrastructure when conducting performance and reliability testing and taking appropriate measures to mitigate any potential issues that may arise.</P>
<P>&nbsp;&nbsp;&nbsp; 5 RESEARCH RESULTS<BR>Next, we present the results of experiments including graphics, numbers, and the evaluation of each retry policy used. We conducted all tests using the same system and parameters, which included: Hardware, Software, and Test specs.<BR>The hardware specifications used for the tests were based on the laptop utilized throughout the experimentation. We employed an 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz CPU with<BR>16.0 GB RAM, a 64-bit operating system, an x64-based processor, and a 1 TB SSD to run the tests at a low scale with high-speed. However, at times, the laptop was unable to perform the tests due to the size and complexity of the system, leading to fatal errors. Therefore, we had to modify the testing specifications and run the system on a smaller scale than initially planned.<BR>Regarding the software specifications, we chose the operating system as Windows 11 Home Edition due to its compatibility with our laptop and the researcher's background. For the testbed, we opted for the Cassandra database for its ability to create distributed clusters and the ease of virtualization techniques it offers. As the core of the testbed, we selected the Java Spring Boot framework, using the 18.0.2 version of Java. For visualization purposes, we used the VueJS framework based on the 16.16.0 version of Node and the 8.11.0 version of NPM.<BR>By adopting these hardware and software specifications, we aimed to create an open and transparent environment for testing and evaluating different retry policies in distributed systems.<BR>While we faced some limitations with the laptop's performance, we believe that the selected software components and configurations provided a robust foundation for conducting the experiments and analyzing the outcomes.<BR>The test specifications used for evaluating the effectiveness of different retry policies in handling transient failures in distributed systems were comprehensive. The tests were performed at an operations rate of 85 requests per second, with a base timeout of 1000 milliseconds. The database limit was set at 30000, while the maximum number of retry requests allowed was 20. The maximum timeout was 64000 milliseconds.<BR>During the test, the database loading period was 100-140 seconds, with the CPU shared for Cassandra set at 0.5. The vulnerable state period lasted between 20-40 seconds, and the CPU shared for Cassandra was at 0.05. Similarly, during the trigger state period, the CPU shared for Cassandra was at 0.04. We tried to stick to these specifications, but they still differentiate policy by policy.<BR>Each retry policy's recovery period was calculated using the results of a specific test. The tests were stopped to measure the time needed for service recovery. Overall, these specifications provided a robust framework for testing and comparing different retry policies in the context of distributed systems.<BR>We tested such retry policies as:<BR>&nbsp;&nbsp;&nbsp; 1. Cancel<BR>&nbsp;&nbsp;&nbsp; 2. Simple<BR>&nbsp;&nbsp;&nbsp; 3. Simple Delay<BR>&nbsp;&nbsp;&nbsp; 4. Incremental backoff<BR>&nbsp;&nbsp;&nbsp; 5. Exponential backoff<BR>&nbsp;&nbsp;&nbsp; 6. Fibonacci backoff<BR>&nbsp;&nbsp;&nbsp; 7. LILD<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Cancel Retry Policy</P>
<P>&nbsp;</P>
<P>Figure 9: A number of requests and retry requests, status codes, and average response time for the requests for cancel retry policy. The loading period till the 120th second. The vulnerable period till the 150th second.<BR>The trigger started at the 150th second and finished at the 330th second.<BR>Figure 9 illustrates the experimental run conducted with the cancel-retry policy. The top right graph displays approximately 1000 failure requests, while the top left corner of the figure indicates 0 retry requests, as expected. In the bottom graph of Figure 9, we can observe that the average request time during the loading period was nearly 0 millisecond until the CPU level was lowered from 0.5 to 0.05, triggering the vulnerable state at the 120th second. At the 150th second, the CPU level was further reduced to 0.04, resulting in request failures and an increase in average response time. After a wait of approximately 25 seconds, the system started experiencing failures, and the average response time continued to degrade. However, at the 330th second, when the CPU level was raised back to 0.05, the system returned to a vulnerable state and recovered swiftly, as anticipated.</P>
<P><BR>Figure 10: Success rate of the requests for cancel retry policy. The loading period till the 120th second. The vulnerable period till the 150th second. The trigger started at the 150th second and finished at the 330th second.<BR>As we can observe in Figure 10, we can divide it into three periods:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Loading and Vulnerable states: where all the requests went successfully, and the requester service fed the database with the data (till 120th second) and then worked normally in the vulnerable state (till 150th second)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Failing state: after 25 seconds the first fail appeared, but after a little time it continued with about 100 percent of a successful request. After queuing a request for a hundred seconds it started with the continuous failure of all requests.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. Recovering state: after returning the CPU level to 0.05 the system recovered in about 10-15 seconds and never failed again.<BR>As the system recovered autonomously, it can be concluded that there were no instances of metastable failure observed during this test.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Simple Retry Policy</P>
<P><BR>Figure 11: A number of requests and retry requests, status codes, and average response time for the requests for simple retry policy. The loading period till the 200th second. The vulnerable period till the 350th second. The trigger started at the 350th second and finished at the 410th second. The generation of new requests finished at the 600th second.<BR>The Simple Retry policy was the first one we tested, and we experimented with different configurations to determine the optimal settings and first-time achieved metastable failure. Figure 11 shows that 26 thousand retry requests happened out of 11 thousand default requests.<BR>The test started with a loading period for the first 200 seconds, followed by a change in the CPU level to 0.05, causing temporary request failures. After the system adapted at the 350th second, we lowered the CPU level to 0.04, and within about 60 seconds, the system started to fail. We waited for another 60 seconds before returning the CPU level to 0.05, but the system did not recover fully, as seen in the increased average response time between the 500th and 600th seconds in Figure 11.<BR>This situation indicates that we have reached a metastable failure state.<BR>As per the definition of metastable failure, if the system fails to recover after removing the trigger, which in our case is lowering the CPU level, it falls into a constant failing state, or metastable failure [16]. At the 600th second, we decided to stop creating new requests, and the system started showing successful requests only after approximately 80 seconds, as depicted in Figure 12.</P>
<P><BR>Figure 12: Success rate of the requests for simple retry policy. The loading period till the 200th second. The vulnerable period till the 350th second. The trigger started at the 350th second and finished at 410th second.<BR>The generation of new requests finished at the 600th second.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Simple Delay Retry Policy</P>
<P><BR>Figure 13: A number of requests and retry requests, status codes, and average response time for the requests for simple delay retry policy. The loading period till the 300th second. The vulnerable period till the 400th second. The trigger started at the 500th second and finished at the 600th second. The generation of new requests finished at the 650th second.<BR>The Simple Delay Retry and Simple Retry policies exhibited similar performance. For the Simple Delay Retry, we had a loading period of approximately 300 seconds after changing the CPU level from 0.5 to 0.05 and then ran the system for 100 seconds before triggering the failure by lowering the CPU level to 0.04 at the 500th second. The system failed within 10-15 seconds and showed only occasional expected success responses, as the middleware service runs asynchronously and some successful requests may still occur.<BR>At the 600th second, we returned the CPU level to the normal value of 0.05, but there was no immediate change. Moreover, the number of retry requests continued to grow over time, and even after stopping new requests at the 650th second, the system took around 50-60 seconds to recover, as seen in Figure 13.</P>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P>Figure 14: Success of the requests for simple delay retry policy. The loading period till the 300th second. The vulnerable period till the 400th second. The trigger started at the 500th second and finished at the 600th second. The generation of new requests finished at the 650th second.</P>
<P>&nbsp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.4 Incremental Backoff Retry Policy</P>
<P><BR>Figure 15: A number of requests and retry requests, status codes, and average response time for the requests for incremental backoff retry policy. The loading period till the 100th second. The vulnerable period till the 115th second. The trigger started at the 115th second and finished at the 450th second. The generation of new requests finished at the 650th second.<BR>Figure 15 depicts the linear progression of the average response time in the bottom chart. In this test, we observed approximately 18 thousand retry requests, as shown in the figure.</P>
<P>Figure 16:Success rate of the requests for incremental backoff retry policy. The loading period till 100th second. The vulnerable period till the 115th second. The trigger started at the 115th second and finished at the 450th second. The generation of new requests finished at the 650th second.<BR>Figure 16 shows the test results where we loaded the system for 100 seconds with successful requests. At the 100th second, we triggered the vulnerable state by lowering the CPU level from 0.5 to 0.05 and ran the system for 15 seconds. Afterward, we changed the CPU level to 0.04 to trigger the failure. Within 10 seconds, the system started failing, and retries began to occur.<BR>The performance of simple and simple delay retry policies initially was similar, with occasional success requests. However, over time, the number of occasional success requests diminished due to stacking requests at the moment with reduced speed. In the simple delay retry policy, requests appeared after a one-second delay following the receipt of a fault request, and the number of requests sent per second grew briskly. For example, in the first second, there were 85 requests, and in the next second, there were 170 requests, and in the successive seconds, the number of requests increased exponentially. On the other hand, in the incremental retry policy, the number of retries is incremented based on the time increment for each subsequent request, as shown in Table 1.</P>
<P>&nbsp;</P>
<P>Table 1: The number of requests generated in the time frame by incremental backoff policy.</P>
<P>Seconds<BR>1<BR>2<BR>3<BR>4<BR>5<BR>6<BR>7<BR>8<BR>9<BR>Sum of requests<BR>1<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>2<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>170<BR>3<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>170<BR>4<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>5<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>255<BR>6<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>255<BR>7<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>340<BR>8<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>340<BR>9<BR>0<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>340</P>
<P>Table 1 provides the number of requests generated in each second, assuming that all requests result in failures, and it shows when the retry requests are generated according to the incremental backoff algorithm. The first row and first column of the table indicate the order number of the second. For example, in the second column, we can see that in the 1st second, the system sent 85 requests.<BR>When all requests failed, the requests were retried with a base timeout of 1 second, resulting in 85 retry requests in the 2nd second. After requester middleware generated new requests at the 2nd second, the total number of requests in the 2nd second became 170.<BR>As we move to the 3rd second, the situation changes compared to the simple retry policy. The first sequence of retries already has a 2-second delay and is waiting, so we only have the requests generated at the 3rd second and the retries generated after the failed requests at the 2nd second.<BR>Continuing through the table, we can observe a significant difference in the number of requests between the simple delay and incremental retry policies. For example, at the 9th second, the incremental retry policy has 340 parallel requests, while the simple delay policy has 765 requests. It's important to note that this sample is approximate and intended to demonstrate the difference between the two retry policies, and in a real system, there may be a 2-second wait time for responses, resulting in shifted timings.<BR>Continuing the review of Figure 16, we returned the CPU level to 0.05 at the 450th second and observed the system's behavior. After 200 seconds, there was no change in the graph, so we decided to stop generating new requests. The system could only recover after another 150 seconds, and during this time, there were no successful requests. Based on the specifications we used, which included limiting the number of retries, the system eventually recovered. However, without such limitations, it's possible that it would take much longer for the system to recover.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.5 Exponential Backoff Retry Policy</P>
<P>Figure 17: A number of requests and retry requests, status codes, and average response time for the requests for exponential backoff retry policy. The loading period till the 180th second. The vulnerable period till the 210th second. The trigger started at the 210th second and finished at the 400th second. The generation of new requests finished in the 750th second.<BR>In Figure 17, we can observe the significant number of retries, and the reason is a long time needed to recover the system. During this test, we intentionally loaded the system for a longer time. Based on other tests, we noticed that this retry policy takes more time to fail than other retry policies, so we decided to proceed with a longer loading time for testing purposes. Additionally, the incremental backoff algorithm used in this retry policy has an accumulating effect, meaning that during a prolonged period of continuous failures, the recovery time can be significantly longer due to the accumulation of errors. This is a vital factor to consider when evaluating the performance and effectiveness of the retry policy in real-world scenarios.</P>
<P><BR>Figure 18: Success rate of the requests for exponential backoff retry policy. The loading period till the 180th second. The vulnerable period till the 210th second. The trigger started at the 210th second and finished at the 400th second. The generation of new requests finished in the 750th second.<BR>In Figure 18, we can observe that after triggering the system failure, it took approximately 40-50 seconds for the system to fail, which is a relatively long time considering the prolonged loading period in this test. During the testing, we noticed occasional successful requests expected due to the exponential growth of retry timeouts in the incremental backoff algorithm. We removed the trigger at the 400th second, but there was no significant change in the system behavior, indicating that the system may have reached a metastable failure state. We then stopped generating new requests at the 750th second, and the system took a whopping 1250 seconds to finish processing all requests, which is a significantly longer recovery time compared to other retry policies. This long recovery time is attributed to the exponential growth of retry timeouts, where the timeout doubles with each subsequent retry in the retry chain as per the incremental backoff algorithm. This characteristic of the retry policy should be carefully considered when choosing the appropriate retry strategy for a system with specific requirements and constraints.<BR>Table 2: The number of requests generated in the time frame by exponential backoff policy.</P>
<P>Seconds&nbsp;1&nbsp;2&nbsp;3&nbsp;4&nbsp;5&nbsp;6&nbsp;7&nbsp;8&nbsp;9&nbsp;10&nbsp;11&nbsp;12&nbsp;13&nbsp;14&nbsp;15&nbsp;16&nbsp;Sum of<BR>requests</P>
<P><BR>1<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>2<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>170<BR>3<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>170<BR>4<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>5<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>6<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>7<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>8<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>9<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>10<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>11<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>12<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>340<BR>13<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>0<BR>340<BR>14<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>0<BR>340<BR>15<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>0<BR>340<BR>16<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>425</P>
<P>As we can notice in Table 2 the requests per second grow very slowly since the retry timeout grows exponentially. This causes the delayed accumulation of the requests and due to that longer recovery time.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.6 Fibonacci Backoff Retry Policy</P>
<P><BR>Figure 19: A number of requests and retry requests, status codes, and average response time for the requests for fibonacci backoff retry policy. The loading period till the 120th second. The vulnerable period till the 140th second. The trigger started at the 140th second and finished at the 550th second. The generation of new requests finished at the 790th second.<BR>In Figure 19, the test result for the Fibonacci backoff retry policy is depicted, showing a total of 15 thousand requests along with 23 thousand retry requests. Additionally, as observed in Figure 19, the average timeout for requests shows a dramatic increase towards the end of the test during the recovery period. This behavior was expected in the Fibonacci backoff algorithm, where the retry timeout is calculated based on Fibonacci numbers, leading to longer and irregularly increasing timeouts as the retries progress. This characteristic of the Fibonacci backoff retry policy should be considered when choosing it as a retry strategy, as it may affect the overall system behavior and recovery time in the chosen scenarios.</P>
<P><BR>Figure 20: Success rate of the requests for exponential backoff retry policy. The loading period till the 120th second. The vulnerable period till the 140th second. The trigger started at the 140th second and finished at the 550th second. The generation of new requests finished at the 790th second.</P>
<P>Figure 20 shows the loading period in which all requests were initially successful at the beginning of the test. The trigger set at the 140th second, and failures started occurring at the 160th second.<BR>Despite the Fibonacci backoff retry policy initially exhibiting a relatively high success rate, as depicted in Figure 20, it eventually failed entirely at the 500th second. Subsequently, the decision was made to remove the trigger and observe the outcome. However, even after removing the trigger at the 550th second, the system failed to recover within 240 seconds. At the 790th second,<BR>the generation of new requests ceased, and the system only completed all pending requests after a delay of 500 seconds. This illustrates the behavior of the Fibonacci backoff retry policy in this scenario, where the recovery time was longer compared to other retry policies due to the increasing and irregular timeouts based on Fibonacci numbers.<BR>Table 3: The number of requests generated in the time frame by Fibonacci backoff policy.</P>
<P>Seconds<BR>1<BR>2<BR>3<BR>4<BR>5<BR>6<BR>7<BR>8<BR>9<BR>10<BR>11<BR>12<BR>13<BR>Sum of requests<BR>1<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>85<BR>2<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>170<BR>3<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>4<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>255<BR>5<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>6<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>7<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>0<BR>340<BR>8<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>0<BR>425<BR>9<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>0<BR>425<BR>10<BR>0<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>0<BR>425<BR>11<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>0<BR>425<BR>12<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>0<BR>425<BR>13<BR>85<BR>0<BR>0<BR>0<BR>0<BR>85<BR>0<BR>0<BR>85<BR>0<BR>85<BR>85<BR>85<BR>510</P>
<P>Table 3 presents the number of retries at each second, with minimal differences in timeouts initially. However, after the 4th retry, the timeouts rapidly increase, exhibiting a pattern akin to exponential growth. Moreover, as the number of consecutive retries increases, there is a notable discrepancy in timeouts, with significantly longer timeouts observed. Overall, this Fibonacci backoff retry policy proves to be effective when employed for short durations with an active trigger but may experience longer recovery times and larger differences in timeouts with consecutive retries.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.7 LILD Retry Policy</P>
<P>&nbsp;</P>
<P>Figure 21: A number of requests and retry requests, status codes, and average response time for the requests for LILD retry policy. The loading period till the 110th second. The vulnerable period till the 130th second. The trigger started at the 130th second and finished at the 360th second. The generation of new requests finished in the 750th second.<BR>Figure 21 presents an intriguing retry policy characterized by an approximate 1-to-1 ratio between requests and retry requests. This policy exhibits a fast recovery time, with retries occurring at a similar rate as the initial requests. However, during a metastable failure state, the system fails to fully recover and retains a high ratio of successful responses, indicating that some requests may still fail despite retries. Further analysis may be required to understand the root cause of this behavior and potential optimizations for better recovery in such scenarios.</P>
<P>Figure 22: Success rate of the requests for LILD retry policy. The loading period till the 110th second. The vulnerable period till the 130th second. The trigger started at the 130th second and finished at the 360th second. The generation of new requests finished in the 750th second.<BR>Figure 22 depicts that the system experienced a rapid and complete failure after setting the trigger. However, upon removing the trigger, it exhibited a high rate of successful requests. This can be attributed to the retry policy, which calculates the retry timeout based not only on the current retry attempt but also on the overall retry rate in the system. This policy increases the retry timeout for failed requests and decreases it for successful requests, potentially allowing for successful requests to be retried more frequently. Despite this approach, the system experienced sustained failure and was unable to recover autonomously, as evidenced in Figure 22. After ceasing the generation of new requests at the 750th second, the system took an additional 180 seconds to complete all pending requests, indicating a significant delay in recovery time even after the trigger was removed. Further investigation may be required to identify potential improvements in the retry policy to enable more efficient recovery in such scenarios.</P>
<P>&nbsp;&nbsp;&nbsp; 6 EVALUATION OF RESULTS<BR>In this section, we evaluate and compare the retry policies between each other, and discuss the most interesting points and key features.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Simple Retry Policy vs Simple Delay Retry Policy</P>
<P>&nbsp;</P>
<P>Figure 23: Simple Retry on the left vs Simple Delay Retry on the right.</P>
<P>Figure 23 illustrates the number of requests and retry requests for the simple retry and simple delay retry policies. Despite having similar testing times and generating requests, there is a notable difference in the ratio of retries between these two policies. The simple retry policy has a higher ratio of retries and default requests, at 234 percent, compared to the simple delay retry policy, which has a ratio of 125 percent. This difference can be attributed to the fact that in the simple delay retry policy, each chain of retries introduces a delay of one second, as per the test configurations. This cumulative delay results in a sum of 20 additional seconds of wait time when the limit of retries is set to 20, whereas the simple retry policy does not have this additional delay.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2 Incremental Backoff Retry Policy vs Exponential Backoff Retry Policy vs Fibonacci Backoff Retry Policy<BR>The Incremental backoff, Exponential backoff, and Fibonacci backoff retry policies have long recovery times of 250 seconds, 1250 seconds, and 510 seconds, respectively. This correlates with the acceleration of timeouts of the policies. Comparing Table 1, Table 2, and Table 3, it is observed that the increase in the number of retries is correlated with the formulas of the policies. In the incremental retry, the growth happens at 1st, 2nd, 4th, 7th, 11th seconds due to the formula:<BR>&#55349;&#56403;(&#55349;&#56411;) = &#55349;&#56403;(&#55349;&#56411; &#8722; 1) + &#55349;&#56411; &#8727; &#55349;&#56399;&#55349;&#56398;&#55349;&#56416;&#55349;&#56402;_&#55349;&#56417;&#55349;&#56406;&#55349;&#56410;&#55349;&#56402;&#55349;&#56412;&#55349;&#56418;&#55349;&#56417;, &#55349;&#56403;(0) = &#55349;&#56399;&#55349;&#56398;&#55349;&#56416;&#55349;&#56402;_&#55349;&#56417;&#55349;&#56406;&#55349;&#56410;&#55349;&#56402;&#55349;&#56412;&#55349;&#56418;&#55349;&#56417;<BR>Where 'n' is the next iteration of the growth in RPS (requests per second) and 'f(n)' is the second when the growth happens.<BR>In the exponential backoff, the formula is:<BR>&#55349;&#56403;(&#55349;&#56411;) = &#55349;&#56403;(0) &#8727; 2&#55349;&#56411;, &#55349;&#56403;(0) = &#55349;&#56399;&#55349;&#56398;&#55349;&#56416;&#55349;&#56402;_&#55349;&#56417;&#55349;&#56406;&#55349;&#56410;&#55349;&#56402;&#55349;&#56412;&#55349;&#56418;&#55349;&#56417;<BR>Where 'n' is the next iteration of the growth in RPS, and 'f(n)' is the second when the growth happens.<BR>In the Fibonacci backoff, the formula is:<BR>&#55349;&#56403;(&#55349;&#56411;) = &#55349;&#56403;(&#55349;&#56411; &#8722; 1) &#8722; &#55349;&#56403;(&#55349;&#56411; &#8722; 2), &#55349;&#56403;(1) = &#55349;&#56399;&#55349;&#56398;&#55349;&#56416;&#55349;&#56402;_&#55349;&#56417;&#55349;&#56406;&#55349;&#56410;&#55349;&#56402;&#55349;&#56412;&#55349;&#56418;&#55349;&#56417;, &#55349;&#56403;(0) = 0<BR>Where 'n' is the next iteration of the growth in RPS, and 'f(n)' is the second when the growth happens.<BR>The acceleration of the policies relates as follows:<BR>&#55349;&#56380;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;&#55349;&#56398;&#55349;&#56409; &lt; &#55349;&#56377;&#55349;&#56406;&#55349;&#56399;&#55349;&#56412;&#55349;&#56411;&#55349;&#56398;&#55349;&#56400;&#55349;&#56400;&#55349;&#56406; &lt; &#55349;&#56376;&#55349;&#56421;&#55349;&#56413;&#55349;&#56412;&#55349;&#56411;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409;<BR>And this correlates with their recovery times:<BR>&#55349;&#56380;&#55349;&#56411;&#55349;&#56400;&#55349;&#56415;&#55349;&#56402;&#55349;&#56410;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;&#55349;&#56398;&#55349;&#56409; &#55349;&#56415;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56419;&#55349;&#56402;&#55349;&#56415;&#55349;&#56422; &#55349;&#56412;&#55349;&#56403; 250 &#55349;&#56416;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56411;&#55349;&#56401;&#55349;&#56416; &lt; &#55349;&#56377;&#55349;&#56406;&#55349;&#56399;&#55349;&#56412;&#55349;&#56411;&#55349;&#56398;&#55349;&#56400;&#55349;&#56400;&#55349;&#56406; &#55349;&#56415;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56419;&#55349;&#56402;&#55349;&#56415;&#55349;&#56422; &#55349;&#56412;&#55349;&#56403; 510 &#55349;&#56416;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56411;&#55349;&#56401;&#55349;&#56416;<BR>&lt; &#55349;&#56376;&#55349;&#56421;&#55349;&#56413;&#55349;&#56412;&#55349;&#56411;&#55349;&#56402;&#55349;&#56411;&#55349;&#56417;&#55349;&#56406;&#55349;&#56398;&#55349;&#56409; &#55349;&#56415;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56419;&#55349;&#56402;&#55349;&#56415;&#55349;&#56422; &#55349;&#56412;&#55349;&#56403; 1250 &#55349;&#56416;&#55349;&#56402;&#55349;&#56400;&#55349;&#56412;&#55349;&#56411;&#55349;&#56401;&#55349;&#56416;</P>
<P>&nbsp;&nbsp;&nbsp; 7 FUTURE WORK<BR>As the next step in our work, we are planning to move our testbed to the cloud. This step will improve system productivity and bring us closer to real-world systems. By moving our testbed to the cloud, we will take full advantage of the scalability and flexibility of cloud technology. One of the main advantages of moving our testbed to the cloud is scalability. Cloud providers offer on- demand resources that can be scaled up or down based on the needs of our system. By using the cloud, we can ensure that the service can work with many requests. We can also easily add or remove services as our needs change over time. This flexibility allows us to quickly adapt to the changes in the testbed and meet the needs in time.<BR>In addition to moving our testbed to the cloud, we are planning to implement several additional solutions in our testbed. One of these solutions is the Circuit Breaker Pattern. The Circuit Breaker Pattern [10] is a design pattern used to prevent cascading failures in distributed systems. By implementing this pattern in our testbed, we can ensure that any failure in one part of the system does not bring down the entire system.<BR><FONT class=extract>The Circuit Breaker Pattern works by monitoring the health of a service [20]. A circuit breaker has 3 states shown in Figure 24:<BR>&nbsp;&nbsp;&nbsp; 1. Closed: the proxy counts the number of unsuccessful requests, then after reaching some threshold, it changes the state to Open and puts timeout after what changes the state to Half-open. In this state, all requests are redirected to the operation server.<BR>&nbsp;&nbsp;&nbsp; 2. Open: catches all the requests and returns fail immediately<BR>&nbsp;&nbsp;&nbsp; 3. Half-open: passes a limited number of requests to the server and waits for a response. If any operation fails, revert the system to an Open state. If the operation never failed till reaching the threshold, assume that the problem causing errors is fixed and change the state to Closed.</FONT></P>
<P>Figure 24: Circuit Breaker pattern work diagram. Retrieved from Microsoft [20]<BR>Another solution that we plan to implement is Reddis with Caching [19]. Reddis is an in-memory data store that is commonly used for caching data. By using Reddis with Caching in our testbed, we can reduce the number of times that our system needs to access the database, which can improve system performance.<BR>Caching is the process of storing frequently accessed data in memory to be retrieved in the future. By using Reddis with caching, we store frequently accessed data, which can reduce the number of times that our system needs to access the database. Caching can improve system performance and reduce the load on the database.<BR>A clustered database works by distributing data across multiple servers or nodes. The technology ensures that: if one server or node fails the data is accessed from another server or node. The clustered database combined with caching technology improves the availability of the systems. We<BR>had to refuse to use the solution in this paper because of hardware limitations, even though it was intended to be implemented.<BR>Finally, we plan to add several middleware service types to our testbed. Middleware services are software components that provide additional functionality to our system. We can improve system functionality and performance by adding these components to our testbed.<BR>Middleware services are commonly used in distributed systems to handle communication between different components of the system. They provide a layer of abstraction that allows different components to communicate with each other without the details of how the communication is happening. Additional middleware can improve system flexibility and scalability.<BR>One middleware service that we are considering is a message queue. A message queue is a software component that allows different services of the system to communicate with each other asynchronously. A message queue can improve system performance and scalability by allowing various components to work independently.<BR>Another middleware service that we are considering is a load balancer. A load balancer is a software component that distributes incoming traffic across multiple servers or nodes. This software can improve system performance by ensuring even distribution of the workload across all servers or nodes. This middleware can also improve system availability by ensuring that if one server or node fails, the workload can automatically shift to another server or node.<BR>Overall, our plans for our testbed include moving it to the cloud and implementing several additional solutions, including the Circuit Breaker Pattern, Reddis with Caching, a Clustered Database, and several middleware clients. These solutions will improve system productivity, scalability, and functionality and bring us closer to the real systems we aim to emulate. We believe that these improvements will allow us to test and develop software systems and stay ahead of the competition.</P>
<P>&nbsp;&nbsp;&nbsp; 8 CONCLUSION<BR>In conclusion, the results of this research highlight the significance of retry policies in mitigating failures and promoting system resilience. The findings indicate that the choice of retry policy can greatly impact the recovery outcomes of a system. The cancel retry policy demonstrated swift recovery from failures, while the simple retry policy resulted in metastable failure. The simple delay retry and incremental backoff retry policies showed delayed recovery with occasional success requests after removing the trigger. These findings emphasize the importance of carefully selecting an appropriate retry policy based on the specific requirements and characteristics of the system.<BR>Furthermore, this research underscores the need for further analysis and optimization of retry policies to enhance system resilience and performance. Future research could explore additional factors that may influence the effectiveness of retry policies, such as system workload, failure<BR>patterns, and resource constraints. Additionally, the integration of other fault tolerance techniques, such as replication or redundancy, in conjunction with retry policies could be investigated to further improve system reliability.<BR>Overall, the findings of this research contribute to the body of knowledge on retry policies and their impact on system resilience. This research can inform system designers, developers, and practitioners in selecting appropriate retry policies for effectively handling failures and promoting robustness in distributed systems. Further research in this area can help advance the field of fault tolerance and enhance the reliability and performance of complex distributed systems in various application domains.</P>
<P>&nbsp;&nbsp;&nbsp; 9 REFERENCES:<BR>&nbsp;&nbsp;&nbsp; 1. Huang , L., Magnusson , M., Muralikrishna , A. B., Estyak , S., Isaacs , R., Aghayev , A., Zhu , T., &amp; Charapko , A. (2022, September 13). Metastable failures in the wild. USENIX. Retrieved December 5, 2022, from <A href="https://www.usenix.org/publications/loginonline/metastable-failures-wild">https://www.usenix.org/publications/loginonline/metastable-failures-wild</A></P>
<P>&nbsp;&nbsp;&nbsp; 2. Google. (n.d.). Implementing exponential backoff | cloud IOT core documentation | google cloud. Google. Retrieved&nbsp; December&nbsp; 5,&nbsp; 2022,&nbsp; from&nbsp; <A href="https://cloud.google.com/iot/docs/how-tos/exponential-backoff">https://cloud.google.com/iot/docs/how-tos/exponential-backoff</A></P>
<P>&nbsp;&nbsp;&nbsp; 3. What is a retry policy?: Temporal documentation. Temporal Documentation RSS. (n.d.). Retrieved December 5, 2022,&nbsp;from&nbsp;<A href="https://docs.temporal.io/concepts/what-is-a-retry">https://docs.temporal.io/concepts/what-is-a-retry</A>- policy#:~:text=A%20Retry%20Policy%20is%20a,%2C%20which%20always%20retry%20indefinitely.)</P>
<P>&nbsp;&nbsp;&nbsp; 4. EdPrice-MSFT. (n.d.). Retry pattern - azure architecture center. Azure Architecture Center | Microsoft Learn. Retrieved&nbsp; December&nbsp; 5,&nbsp; 2022,&nbsp; from&nbsp; <A href="https://learn.microsoft.com/en-us/azure/architecture/patterns/retry">https://learn.microsoft.com/en-us/azure/architecture/patterns/retry</A></P>
<P>&nbsp;&nbsp;&nbsp; 5. Johndowns. (n.d.). Retry storm antipattern - performance antipatterns for cloud apps. Performance antipatterns for cloud apps | Microsoft Learn. Retrieved December 5, 2022, from <A href="https://learn.microsoft.com/en">https://learn.microsoft.com/en</A>- us/azure/architecture/antipatterns/retry-storm/</P>
<P>&nbsp;&nbsp;&nbsp; 6. EdPrice-MSFT. (n.d.). Azure service retry guidance - best practices for cloud applications. Azure service retry guidance - Best practices for cloud applications | Microsoft Learn. Retrieved December 5, 2022, from <A href="https://learn.microsoft.com/en-us/azure/architecture/best-practices/retry-service-specific">https://learn.microsoft.com/en-us/azure/architecture/best-practices/retry-service-specific</A></P>
<P>&nbsp;&nbsp;&nbsp; 7. Retries. Retries - Boto3 Docs 1.26.22 documentation. (n.d.). Retrieved December 5, 2022, from <A href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html">https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html</A></P>
<P>&nbsp;&nbsp;&nbsp; 8. Retrypolicy (AWS SDK for Java - 1.12.339) - docs.aws.amazon.com. Amazon. (n.d.). Retrieved December 6, 2022, from&nbsp;<A href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/retry/RetryPolicy.html">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/retry/RetryPolicy.html</A><BR>&nbsp;&nbsp;&nbsp; 9. Product&nbsp;documentation.&nbsp;ServiceNow.&nbsp;(n.d.).&nbsp;Retrieved&nbsp;December&nbsp;5,&nbsp;2022,&nbsp;from <A href="https://docs.servicenow.com/bundle/quebec-servicenow-platform/page/administer/flow-designer/concept/retry">https://docs.servicenow.com/bundle/quebec-servicenow-platform/page/administer/flow-designer/concept/retry</A>- policy.html</P>
<P>&nbsp;&nbsp;&nbsp; 10. Brooker, M. (n.d.). [web log]. Retrieved December 5, 2022, from <A href="https://brooker.co.za/blog/">https://brooker.co.za/blog/</A>.</P>
<P>&nbsp;&nbsp;&nbsp; 11. Portworx. (2017). The Expert&#8217;s Guide to Running Cassandra in Containers. Portworx_Cassandra_Guide_10-17-<BR>17.&nbsp;Retrieved&nbsp;December&nbsp;5,&nbsp;2022,&nbsp;from&nbsp;<A href="https://portworx.com/wp">https://portworx.com/wp</A>- content/uploads/2017/10/Portworx_Cassandra_Guide_10-17-17.pdf.</P>
<P>&nbsp;&nbsp;&nbsp; 12. &nbsp;(<A href="https://www.linkedin.com/in/narramadan">https://www.linkedin.com/in/narramadan</A>), M. N. (2021, October 19). Setting up Cassandra with docker. 2much2learn.com. Retrieved December 5, 2022, from <A href="https://2much2learn.com/setting-up-cassandra-with-docker/">https://2much2learn.com/setting-up-cassandra-with-docker/</A></P>
<P>&nbsp;&nbsp;&nbsp; 13. Configuring retries. Linkerd. (n.d.). Retrieved December 5, 2022, from <A href="https://linkerd.io/2.12/tasks/configuring">https://linkerd.io/2.12/tasks/configuring</A>- retries/#:~:text=A%20retry%20budget%20is%20a,f ree%E2%80%9D%20retries%20per%20second</P>
<P>&nbsp;&nbsp;&nbsp; 14. Meiklejohn, C., Stark, L., Celozzi, C., Ranney, M., &amp; Miller, H. (2022). Method overloading the circuit. Proceedings of&nbsp;&nbsp; the&nbsp;&nbsp; 13th&nbsp;&nbsp; Symposium&nbsp;&nbsp; on&nbsp;&nbsp; Cloud&nbsp;&nbsp; Computing.&nbsp;&nbsp; <A href="https://doi.org/10.1145/3542929.3563466">https://doi.org/10.1145/3542929.3563466</A></P>
<P>&nbsp;&nbsp;&nbsp; 15. Mendonca, N. C., Aderaldo, C. M., Camara, J., &amp; Garlan, D. (2020). Model-based analysis of Microservice resiliency patterns. 2020 IEEE International Conference on Software Architecture (ICSA). <A href="https://doi.org/10.1109/icsa47634.2020.00019">https://doi.org/10.1109/icsa47634.2020.00019</A></P>
<P>&nbsp;&nbsp;&nbsp; 16. Saleh Sedghpour, M. R., Klein, C., &amp; Tordsson, J. (2022). An empirical study of service mesh traffic management policies for microservices. Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering.&nbsp;<A href="https://doi.org/10.1145/3489525.3511686">https://doi.org/10.1145/3489525.3511686</A></P>
<P>&nbsp;&nbsp;&nbsp; 17. Kwak, B.-J., Song, N.-O., &amp; Miller, L. E. (2005). Performance analysis of exponential backoff. IEEE/ACM Transactions&nbsp;&nbsp; on&nbsp;&nbsp; Networking,&nbsp;&nbsp; 13(2),&nbsp;&nbsp; 343&#8211;355.&nbsp;&nbsp; <A href="https://doi.org/10.1109/tnet.2005.845533">https://doi.org/10.1109/tnet.2005.845533</A></P>
<P>&nbsp;&nbsp;&nbsp; 18. Birchall, C. (n.d.). Retry policies. cats-retry. Retrieved December 5, 2022, from <A href="https://cb372.github.io/cats">https://cb372.github.io/cats</A>- retry/docs/policies.html</P>
<P>&nbsp;&nbsp;&nbsp; 19. Lee, Y.-H., &amp; Shin, K. G. (1988). Optimal design and use of retry in fault-Tolerant Computer Systems. Journal of the ACM, 35(1), 45&#8211;69. <A href="https://doi.org/10.1145/42267.42269">https://doi.org/10.1145/42267.42269</A></P>
<P>&nbsp;&nbsp;&nbsp; 20. Bronson, N., Aghayev, A., Charapko, A., &amp; Zhu, T. (2021). Metastable failures in Distributed Systems. Proceedings of&nbsp; the&nbsp; Workshop&nbsp; on&nbsp; Hot&nbsp; Topics&nbsp; in&nbsp; Operating&nbsp; Systems.&nbsp; <A href="https://doi.org/10.1145/3458336.3465286">https://doi.org/10.1145/3458336.3465286</A></P>
<P>&nbsp;&nbsp;&nbsp; 21. Algorithm-backoff. MetaCPAN. (n.d.). Retrieved December 5, 2022, from <A href="https://metacpan.org/dist/Algorithm">https://metacpan.org/dist/Algorithm</A>- Backoff</P>
<P>&nbsp;&nbsp;&nbsp; 22. Gillis, A. S. (2022, August 17). What is distributed computing? WhatIs.com. Retrieved December 5, 2022, from <A href="https://www.techtarget.com/whatis/definition/distributed-computing">https://www.techtarget.com/whatis/definition/distributed-computing</A></P>
<P>&nbsp;&nbsp;&nbsp; 23. Gos.&nbsp;izd-vo&nbsp;selkhoz&nbsp;lit-ry.&nbsp;(1961).&nbsp;Redis.&nbsp;Amazon.&nbsp;Retrieved&nbsp;April&nbsp;15,&nbsp;2023,&nbsp;from <A href="https://aws.amazon.com/redis/?nc1=h_ls">https://aws.amazon.com/redis/?nc1=h_ls</A></P>
<P>&nbsp;&nbsp;&nbsp; 24. Circuit breaker pattern - azure architecture center. Azure Architecture Center | Microsoft Learn. (n.d.). Retrieved April 15, 2023, from <A href="https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker">https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker</A></P>
<P>&nbsp;&nbsp;&nbsp; 25. Vibraye, T. de., &amp; Druet, R. (1981).&nbsp;Message.&nbsp; Amazon.&nbsp; Retrieved April 15, 2023, from <A href="https://aws.amazon.com/message/12721/">https://aws.amazon.com/message/12721/</A></P>
<P>&nbsp;&nbsp;&nbsp; 26. Askarov,&nbsp;A.&nbsp;(n.d.).&nbsp;Suspicio/RetryPolicyResearch.&nbsp;GitHub.&nbsp;Retrieved&nbsp;April&nbsp;15,&nbsp;2023,&nbsp;from <A href="https://github.com/suspicio/RetryPolicyResearch">https://github.com/suspicio/RetryPolicyResearch</A>