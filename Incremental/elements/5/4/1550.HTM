ABSTRACT<BR>Many distributed databases employ consensus protocols to ensure that data is replicated in a strongly-consistent manner on multiple machines despite failures and concurrency. Unfortunately, these protocols show widely varying performance under different network, workload, and deployment conditions, and no previous study offers a comprehensive dissection and comparison of their performance. To fill this gap, we study single-leader, multi-leader, hierarchical multi-leader, and leaderless (opportunistic leader) consensus protocols, and present a comprehensive evaluation of their performance in local area networks (LANs) and wide area networks (WANs). We take a two-pronged systematic approach. We present an analytic modeling of the protocols using queuing theory and show simulations under varying controlled parameters. To cross-validate the analytic model, we also present empirical results from our prototyping and evaluation framework, Paxi. We distill our findings to simple throughput and latency formulas over the most significant parameters. These formulas enable the developers to decide which category of protocols would be most suitable under given deployment conditions.<BR>ACM Reference Format:<BR>Ailidani Ailijiang, Aleksey Charapko, and Murat Demirbas. 2019. Dissecting the Performance of Strongly-Consistent Replication Pro- tocols. In 2019 International Conference on Management of Data (SIG- MOD &#8217;19), June 30-July 5, 2019, Amsterdam, Netherlands. ACM, New York, NY, USA, 15 pages. <A href="https://doi.org/10.1145/3299869.3319893">https://doi.org/10.1145/3299869.3319893</A>
<P></P>
<P>&#8727;Work completed at University at Buffalo, SUNY.<BR>&#8224;Also with Microsoft, Redmond, WA.</P>
<P>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from <A href="mailto:permissions@acm.org">permissions@acm.org</A>.<BR>SIGMOD &#8217;19, June 30-July 5, 2019, Amsterdam, Netherlands<BR>&#169; 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-5643-5/19/06. . . $15.00<BR><A href="https://doi.org/10.1145/3299869.3319893">https://doi.org/10.1145/3299869.3319893</A><BR>&nbsp;&nbsp;&nbsp; 1 INTRODUCTION<BR>Coordination services and protocols play a key role in mod- ern distributed systems and databases. Many distributed databases and datastores [4, 7&#8211;10, 12, 13, 16, 18, 23, 24, 31, 40] use consensus to ensure that data is replicated in a strongly- consistent manner on multiple machines despite failures and concurrency.<BR>Fault-tolerant distributed consensus problem is addressed by the Paxos [25] protocol and its numerous variations and extensions [1, 19&#8211;21, 26, 30, 33&#8211;35]. The performance of these protocols become important for the overall perfor- mance of the distributed databases. These protocols show widely varying performance under different conditions: net- work (latency and bandwidth), workload (command interfer- ence and locality), deployment size and topology (LAN/WAN, quorum sizes), and failures (leader and replica crash and re- covery). Unfortunately, there has been no study that offers a comprehensive comparison across consensus protocols, and that dissects and explains their performance.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Contributions<BR>We present a comprehensive evaluation of consensus proto- cols in local area networks (LANs) and wide area networks (WANs) and investigate many single-leader, multi-leader, hi- erarchical multi-leader and leaderless (opportunistic leader) consensus protocols. We take a two-pronged systematic ap- proach and study the performance of these protocols both analytically and empirically.<BR>For the analytic part, we devise a queuing theory based model to study the protocols controlling for workload and deployment characteristics and present high-fidelity sim- ulations of the protocols. Our model captures parameters impacting throughput, such as inter-node latencies, node processing speed, network bandwidth, and workload char- acteristics. We made the Python implementations of our analytical models available as opensource.<BR>For our empirical study, we developed Paxi, a prototyp- ing and evaluation framework for consensus and replication protocols. Paxi provides a leveled playground for protocol evaluation and comparison. The protocols are implemented
<P>&nbsp;
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Results<BR>Armed with both the simulation results from the analyti- cal model and the experimental results obtained from the Paxi platform implementations, we distill the performance results into simple throughput and latency formulas and present these in Section 6. These formulas provide a simple unified theory of strongly-consistent replication in terms of throughput &#8212;Formula 3: L/(1 + c)(Q + L &#8722; 2)&#8212; and latency<BR>&#8212;Formula 7:(1+c)&#8727;((1&#8722;l)&#8727;(DL +DQ )+l &#8727;DQ ). As such, these<BR>formulas enable developers to perform back-of-the-envelope<BR>performance forecasting. In Section 6 we discuss these re- sults in detail and provide a flowchart to serve as a guideline to identify which consensus protocols would be suitable for a given deployment environment. Here we highlight some significant corollaries from these formulas.
<P>Protocol parameters<BR>L<BR>number of leaders
<P>Q<BR>quorum size<BR>Workload parameters<BR>c<BR>conflict probability
<P>l<BR>locality<BR>Deployment parameters<BR>DL<BR>latency to leader
<P>DQ<BR>latency to quorum<BR>Considering protocol parameters, an effective protocol- level revision for improving throughput and latency is to increase the number of leaders in the protocol, while trying to avoid an increase on the number of conflicts. Increasing the number of leaders is also good for availability: In Paxos, failure of the single leader leads to unavailability until a new leader is elected, but in multi-leader protocols most requests do not experience any disruption in availability, as the failed leader is not in their critical path. Another protocol revision that helps to improve throughput and latency is to reduce Q, the quorum size, provided that fault-tolerance requirements are still met.<BR>As workload parameters are concerned, reducing conflict probability and increasing locality (in the presence of mul- tiple leaders) are beneficial. However, there is an interplay between the number of leaders and probability of conflicts:<BR>Figure 1: State transitions for two-phase coordination
<P>increasing the number of leaders (which helps for throughput and latency) may cause an increase on conflicts (which hurts throughput and latency). EPaxos [30] protocol suffers from this problem. Multi-leader protocols that learn and adapt to locality, such as WPaxos [1] and WanKeeper [2], are less susceptible to this problem.<BR>Finally, the deployment parameters, distance to the leader and distance from leader to the quorum number of nodes, also have a big effect on the latency in WAN deployments. Note that these deployment parameters shadow the protocol parameters, the number or leaders and the quorum size. In WANs, other factors also affect latency. The asymmetric distances between datacenters, the access pattern locality, and unbalanced quorum distances complicate forecasting the performance WAN deployments.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Outline of the rest of the paper<BR>In Section 2 we briefly introduce the protocols we study. We discuss our analytical model in Section 3 and our proto- typing/evaluation framework in Section 4. We present the evaluation in Section 5, discussion of the findings in Section 6, and conclude the paper in Section 7.<BR>&nbsp;&nbsp;&nbsp; 2 PROTOCOLS<BR>Many coordination and replication protocols share a similar state transition pattern as shown in Figure 1. These protocols typically operate in two phases. In the phase-1, some node establishes itself as a leader by announcing itself to other nodes and gaining common consent. During this stage, an incumbent leader also acquires information related to any prior unfinished commands in order to recover them in the next phase. The phase-2 is responsible for replicating the state/commands from the leader to the nodes.<BR>Leveraging this two phase pattern, we give brief descrip- tions of the protocols in our study below. These protocols provide strong consistency guarantees for data replication in distributed databases.<BR>Paxos. The Paxos consensus protocol [25] is typically em- ployed for realizing a fault-tolerant replicated state machine (RSM), where each node executes the same commands in the
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>Leader
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P><BR>Phase:
<P>self-appoint as command leader
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P><BR>Propose
<P>wait for majority
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>Promise
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>Accept
<P>wait for majority
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>Accepted
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P>Commit<BR>Flexible Paxos. Flexible quorums Paxos, or FPaxos [20], observes that the majority quorum is not necessary in phase- 1 and phase-2. Instead, FPaxos shows that the Paxos prop- erties hold as long as all quorums in phase-1 intersect with all quorums in phase-2. This result enables deploying multi- decree Paxos protocols with a smaller quorum in phase-2, providing better performance at the cost of reduced fault tolerance.<BR>Vertical Paxos. Vertical Paxos (VPaxos) [26] separates<BR>Figure 2: Overview of Paxos algorithm
<P>&nbsp;
<P>same order to arrive to identical states. Paxos achieves this in 3 distinct phases: propose (phase-1), accept (phase-2), and commit as shown in Figure 2. During phase-1, a node tries to become the leader for a command by proposing it with a ballot number. The other nodes acknowledge this node to lead the proposal only if they have not seen a higher ballot before. Upon reaching a majority quorum of acks in the pro- pose phase, the &#8220;leader&#8221; advances to phase-2 and tells the followers to accept the command. The command is either a new one proposed by the leader, or an older one learned in phase-1. (If the leader learns some older pending/uncom- mitted commands in phase-1, it must instruct the followers to accept the pending commands with the highest ballot numbers.) Once the majority of followers acknowledge the acceptance of a command, the command becomes anchored and cannot be lost. Upon receiving the acks, the leader sends a commit message that allows the followers to commit and execute the command in their respected state machines.<BR>Several optimizations are adopted over this basic scheme. The commit phase is typically piggybacked to the next mes- sage broadcasted from the leader, alleviating the need for an extra message. Another popular optimization, known as multi-Paxos or multi-decree Paxos [37], reduces the need for extra messages further by allowing the same leading node to instruct multiple commands to be accepted in different slots without re-running the propose phase, as long as its ballot number remains the highest the followers have seen. In the rest of the paper, we use Paxos to refer to the multi-Paxos implementation.<BR>As examples of databases that uses Paxos, FaunaDB [16] uses Raft [33] (a Paxos implementation) to achieve consensus, Gaios [7] uses Paxos to implement a storage service, WAN- Disco [8] uses Paxos for active-active replication, Bizur [19] key-value store uses Paxos for reaching consensus indepen- dently on independent keys, pg_paxos [13] adopts Paxos for fault-tolerant, consistent table replication in PostgreSQL, and Clustrix [10] distributed SQL database uses Paxos for distributed transaction resolution.<BR>the control plane from the data plane. VPaxos imposes a master Paxos cluster above some Paxos groups in order to control any configuration changes, and enables a quick and safe transition between configurations without incurring any stop time: one Paxos group finishes the commands proposed in the previous configuration, while another Paxos group starts on the commands in the new configuration. The ability to safely switch configurations is useful in geo-replicated datastores, since it allows for relocating/assigning data/ob- jects to a different leader node in order to adjust to changes in access locality. Spanner [12] and CockroachDB [24] are examples of databases that uses Paxos groups to work on partitions/shards with another solution on top for relocat- ing/assigning data to another Paxos group.<BR>WanKeeper. WanKeeper [2] is a hierarchical protocol composed of two consensus/Paxos layers. It employs a to- ken broker architecture to control where the commands take place across a WAN deployment. The master resides at level-2 and controls all token movement, while the level- 1 Paxos groups located in different datacenters across the globe, execute commands only if they have a token for the corresponding objects. When multiple level-1 Paxos groups require access to the same object, the master layer at level-2 retracts the token from the lower level and performs com- mands itself. Once the access locality settles to a single re- gion, the master can pass the token back to that level-1 Paxos group to improve latency.<BR>WPaxos. WPaxos [1] is a multi-leader Paxos variant de- signed for WANs. It takes advantage of flexible quorums idea to improve WAN performance, especially in the presence of access locality. In WPaxos, every node can own some objects and operate on these objects independently. Unlike Vertical Paxos, WPaxos does not consider changing the object own- ership as a reconfiguration operation and does not require an external consensus group. Instead WPaxos performs ob- ject migration between leaders by carrying out a phase-1 across the WAN, and commands are committed via phase-2 within the region or neighboring regions. Since the process of moving ownership between leaders is performed using the core Paxos protocol, WPaxos operates safely without requir- ing an external master as in vertical Paxos or WanKeeper. FleetDB [9] adopts WPaxos for implementing distributed transactions over multiple datacenter deployments in WAN.
<P>Egalitarian Paxos. Egalitarian Paxos [30], or EPaxos, is a leaderless solution, where every node can opportunistically become a leader for some command and commit it. When a command does not interfere with other concurrent com- mands, it commits in a single round after receiving the acks from a fast quorum, which is approximately 3/4ths of all nodes. In a sense, EPaxos compacts phase-2 to be part of phase-1, when there are no conflicts. However, if the fast quorum detects a conflict between the commands, EPaxos defaults back to the traditional Paxos mode proceeds with a second phase to establish order on the conflicting commands. The main advantage of EPaxos is the ability to commit non- interfering commands in just a single round trip time. This
<P><BR>AWS Latency : &#956; = 0.4271, &#963; = 0.0476
<P>10
<P>&nbsp;
<P>8
<P>&nbsp;
<P>6
<P>&nbsp;
<P>4
<P>&nbsp;
<P>2<BR>works well when the cluster operates on many objects, and the probability of any two nodes operating on the same objects is small. As an example of a leaderless approach like EPaxos, MDCC [23] has used Fast Paxos and General- ized Paxos (before EPaxos) to implement multi-datacenter strongly consistent replication.
<P>&nbsp;&nbsp;&nbsp; 3 PERFORMANCE MODEL<BR>Modeling the performance of protocols provides an easy way to test different configurations or ideas. In this section, we develop our models for estimating latency and maximum throughput of the strong consistency replication protocols. Our models leverage queueing theory [3] and k-order statis- tics to account for various delays and overheads due to the message exchange and processing.
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Assumptions<BR>We make a number of assumptions about the machines and network in order to constrain the scope of our models and keep them simple and easy to understand.<BR>We assume the round-trip communication latency (RTT) in the network between any two nodes to be normally dis- tributed. This assumption simplifies the reasoning about the effects of network latency on the consensus performance. Exponential distribution is often used to model network la- tencies [6], however, our experiments conducted in AWS EC2 point to an approximately normal latency distribution in local area in AWS cluster, as shown in Figure 3.<BR>We assume all nodes to have stable, uniform network band- width. Our models do not account for bandwidth variations among the nodes. For simplicity, we assume all nodes in the modeled cluster to be of identical performance. In particular, we assume identical CPU and network interface card (NIC) performance. We only consider the case of a single process- ing pipeline &#8211; our modeled machines have a single network card and CPU.<BR>0<BR>0.30&nbsp;0.35&nbsp;0.40&nbsp;0.45&nbsp;0.50&nbsp;0.55&nbsp;0.60<BR>Local Ping Latency (ms)
<P><BR>Figure 3: Histogram of local area RTTs within Amazon AWS EC2 region over a course of a few minutes
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Simple Queueing Models<BR>Queueing theory [3] serves as the basis for our analytical study of consensus protocols. We treat each node in a system as a single processing queue consisting of both NIC and CPU. The protocols operate by exchanging the messages that go through the queue and use the machine&#8217;s resources. Sending a message out from a node requires some time to process at the CPU and some time to transmit through the NIC. Similarly, an incoming message first needs to clear the NIC before being deserialized and processed by a CPU. When a message enters a queue, it needs to wait for any prior messages to clear and resources to become available.<BR>Queueing models enable estimating the average waiting time spent in the queue before the resources become avail- able. To make such estimates, queueing models require just two parameters: service time and inter-arrival time. Service time describes how long it takes to process each item in the queue once it is ready to be consumed. Inter-arrival time controls the rate at which items enter the queue. With high inter-arrival time, new items come in rarely, keeping the queue rather empty. Low inter-arrival time causes the queue to fill up faster, increasing the chance of items having to wait for predecessors to exit the queue.<BR>Since in our model messages immediately transition from CPU to NIC (or vice-versa) and have no possibility of leaving the system without bypassing either the CPU or NIC, we treat these two components as a single queue. This simpli- fies the &#8220;queueing network&#8221; significantly and facilitates our modeling.<BR>For our purposes we have considered four different types of queue approximations: M/M/1, M/D/1, M/G/1 and G/G/1, where the first letter represents the inter-arrival assumption,
<P><BR>Request Arrival<BR>Service Time<BR>Wq<BR>M/M/1<BR>Poisson Process rate &#955;<BR>Exponential Distribution rate &#181;<BR>&#961; 2<BR>&#955;(1&#8722;&#961; )<BR>M/D/1<BR>Poisson Process<BR>Constant s rate &#181; = 1/s<BR>&#961;<BR>2&#181; (1&#8722;&#961; )<BR>M/G/1<BR>Poisson Process<BR>General Distribution<BR>&#955;2&#963; 2+&#961; 2<BR>2&#955;(1&#8722;&#961; )<BR>G/G/1<BR>General Distribution<BR>General Distribution<BR>&#8776; &#961; 2(1+Cs )(Ca +&#961; 2Cs )<BR>2&#955;(1&#8722;&#961; )(1+&#961; 2Cs )<BR>Table 1: Queue types and assumptions
<P>second letter describes service time, and the number tells how many queues are in the system. The simplest model,<BR>M/M/1 assumes both inter-arrival and service time to be approximated by a Poisson process. M/D/1 model makes the service time to be constant, and M/G/1 queue assumes service time to follow a general distribution. The most gen- eral model we have considered is G/G/1. It assumes both the service-time and inter-arrival time as any given distributed<BR>random variables. We summarize and compare each queue- ing type in Table 1.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Modeling Consensus Performance<BR>To model consensus performance, we are interested in esti- mating the average latency of a consensus round as perceived by the client. For such latency estimates, we consider the parameters outlined in Table 2. The average latency is com- prised of a few different components: round&#8217;s queue waiting<BR>Table 2: Model parameters
<P>N<BR>Number of nodes participating in a Paxos phase<BR>Q<BR>Quorum size. For a majority quorum Q = &#8970; N &#8971; + 1<BR>2<BR>DL<BR>RTT between client and the leader node.<BR>DQ<BR>RTT between the leader and (Q &#8722; 1)th follower<BR>b<BR>Network bandwidth available at the node<BR>sm<BR>Message size<BR>ti<BR>Processing time for incoming message<BR>to<BR>Processing time for outgoing message<BR>&#181;<BR>Service time. For Paxos: &#181; = 2to + Nti + 2N sm<BR>b<BR>R<BR>Throughput in rounds per second<BR>&#955;<BR>Inter-arrival between rounds. &#955; = 1<BR>R<BR>&#961;<BR>Queue utilization. For M/D/1 queue: &#961; = &#955;<BR>&#181;<BR>wQ<BR>Queue wait time for a round. For M/D/1 queue: wQ = 2&#181; &#961;<BR>(1&#8722;&#961; )
<P>simplicity, we assume that each message, incoming and out- going, needs to be processed by both the NIC and CPU.<BR>time wQ , round&#8217;s service time &#181;, and network delays DL and<BR>DQ :<BR>The round&#8217;s service time &#181; is then a sum of t<BR>N IC<BR>and t<BR>CPU :
<P>Latency = wQ + &#181; + DL + DQ<BR>For Paxos, the network delays consist of a round-trip time (RTT) between the client and the leader DL, and between the leader and some follower that sends the message forming a quorum of replies at the leader DQ . For the network de- lays, we assume only the time-in-transit for messages, since the time to clear NIC is accounted in our service time com- putations. To calculate DQ in Paxos, we need to consider the quorum size Q of the deployment. For a cluster with N<BR>nodes, the quorum size is Q = &#8970; N &#8971; + 1, making a self-voting<BR>&#181; = tN IC + tCPU . We compute tN IC as the time required to push all round&#8217;s messages of size sm through the network of<BR>some bandwidth b: tN IC = 2N sm . CPU portion of the service time is estimated as the sumbof costs for processing incom-<BR>ing messages ti and outgoing messages to. For Paxos, each<BR>phase-2 round requires the leader to receive a message from the client (ti ), broadcast one outgoing message (to because the CPU serializes the broadcast message once), receive N &#8722;1<BR>messages from the followers ((N &#8722; 1) &#8727; ti ), and reply back to<BR>the client (to). As a result, we have tCPU = 2to + N &#8727; ti . Note that only service time impacts the maximum through-<BR>put of the system, since it alone governs how much of the<BR>leader wait for Q &#8722;<BR>2<BR>1 follower messages before reaching a<BR>processing resources are consumed for each round. The pro-<BR>majority quorum. The RTT for this (Q &#8722; 1)th follower reply is DQ . In LAN we assume the RTTs between all nodes to be drawn from the same Normal distribution, therefore we use a Monte Carlo method approximation of k-order-statistics [14] to compute the RTT for (Q &#8722;1)th reply. In WAN, however, the RTTs between different nodes may be significantly different,<BR>therefore we pick the (Q &#8722; 1)th smallest RTT between the leader and its followers.<BR>Round service time &#181; is a measure of how long it takes the leader to process all messages for a given round. For<BR>tocol reaches its peak throughput when it fully saturates the queue and leaves no unused resources in it. The maximum throughput is the reciprocal of service time:<BR>1<BR>Rmax = &#181;<BR>Finally, we estimate the queueing costs wQ by computing average queue wait time.<BR>To select the most appropriate queue approximation, we used all four queuing models described earlier to estimate
<P>6
<P>5
<P>4
<P>3
<P>2
<P>1
<P>0<BR>3000&nbsp;4000&nbsp;5000&nbsp;6000&nbsp;7000&nbsp;8000<BR>Throughput (requests/sec)
<P>Figure 4: Comparison of different queueing models to a reference implementation in Paxi framework
<P>the performance of Paxos protocol in LAN. We used empir- ically obtained values for all relevant parameters, such as network RTTs, serialization/deserialization costs, etc. Figure 4 shows the comparison between the models and a real Paxos implementation in Paxi. Based on our findings, M/D/1 and<BR>M/G/1 models perform nearly identical and most similar to<BR>our reference Paxos implementation. Since M/D/1 model is simpler, we use it for all our further analysis.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Expanding Models Beyond Paxos and LANs<BR>The other protocols, albeit being more complicated, share the same modeling components with the Paxos described above. Thus, we rely on the same queuing model and round latency computation principles for them as well.<BR>Multi-leader protocols add a few additional parameters to consider. In such protocols a node can both lead the round and participate in concurrent rounds as a follower, there- fore we account for the messages processed as a follower in addition to the messages processed as round&#8217;s leader to estimate queue waiting time. The total number of requests coming to the system is spread out evenly (we assume uni- form workload at each leader for simplicity) across all leaders. This allows us to compute per-leader latency based on each leader&#8217;s processing queue.<BR>The performance of leaderless protocols, such as EPaxos, varies with respect to the command conflict ratios. A conflict typically arises when two or more replicas try to run a com- mand against the same conflict domain (e.g., the same key) concurrently. The conflict must be resolved by the protocol, which leads to extra message exchanges. Therefore, we intro- duce a conflict probability parameter c for EPaxos to compute different latencies for conflicting and non-conflicting com- mands. The overall average latency of a round accounts for<BR>the ratio of conflicting to non-conflicting commands as fol- lows: Latency = c(Latencyconflict) + (1 &#8722; c)(Latencynonconflict).
<P>Figure 5: Paxi modules usage where user implements Messages and Replica type (shaded)
<P>&nbsp;
<P>WAN modeling also requires changes over the LAN model. In particular, communication latency between nodes in WAN cannot be drawn from the same distribution, since the dat- acenters are not uniformly distanced from each other. For example, in a 3-node Paxos configuration with replicas in Eastern U.S., Ireland and Japan, the communication latency between US and Ireland is significantly smaller than that of Ireland and Japan or US and Japan. For that reason, our WAN modeling no longer assumes the same Normal distribution of latency for all nodes, and instead we use a different distribu- tion for communication between every pair of datacenters.
<P>&nbsp;&nbsp;&nbsp; 4 PAXI FRAMEWORK<BR>In order to provide empirical comparisons of different pro- tocols in the same platform under the same conditions, we developed a prototyping framework for coordination/repli- cation protocols called Paxi. Leveraging our observation that strongly consistent replication protocols share com- mon parts, Paxi provides implementations for these shared components. Each component resides in its own loosely cou- pled module and exposes a well-designed API for interaction across modules. Modules can be extended or replaced easily, provided that a new version a module follows the interface. We show the architectural overview of Paxi framework in Figure 5. The developer can easily prototype a distributed coordination/replication protocol by filling in the missing components shown as the shaded blocks. Often the engi- neers only need to specify message structures and write the replica code to handle client requests.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Components<BR>Configurations. Paxi is a highly configurable framework both for the nodes and benchmarks. A configuration in Paxi provides various vital information about the protocol under examination and its environment: list of peers with their reachable addresses, quorum configurations, buffer sizes, replication factor, message serialization and networking pa- rameters, and other configurable settings. The framework allows to manage configuration in two distinct ways: via a JSON file distributed to every node, or via a central master that distributes the configuration parameters to all nodes.<BR>Quorum systems. A quorum system is a key abstrac- tion for ensuring consistency in fault-tolerant distributed computing. A wide variety of distributed coordination algo- rithms rely on quorum systems. Typical examples include consensus algorithms, atomic storage, mutual exclusion, and replicated databases. Paxi implements and provides multiple types of quorum systems, like simple majority, fast quorum, grid quorum, flexible grid and group quorums. The quorum system module only needs two simple interfaces, ack() and quorum satisfied(). By offering different types of quorum systems out of the box, Paxi enables users to easily probe the design space without changing their code.<BR>Networking. When designing Paxi framework, we re- frained from any blocking primitives such as remote proce- dure calls (RPC), and implemented a simple message passing model that allows us to express any algorithmic logic as a set of event handlers [36, 38]. The networking module transpar- ently handles message encoding, decoding, and transmission with a simple Send(), Recv(), Broadcast() and Multicast() in- terface. The transport layer instantiates TCP, UDP, or Go channel for communication without any changes to the caller from an upper layer. Paxi supports both TCP and UDP to eliminate to eliminate any bias for algorithms that benefit from different transport protocols. For example, the single leader approach may benefit from TCP as messages are reli- ably ordered from leader to followers. Whereas conflict-free updates in small messages gain nothing from ordered de- livery and pay the latency penalty in congestion control. Such system might perform better on UDP. Paxi also imple- ments the intra-process communication transport layer by Go channels for a cluster simulation, where all nodes run concurrently within a single process. The simulation mode simplifies the debugging of Paxi protocols since it avoids a cluster deployment step.<BR>Data store. Many protocols separate the protocol-level outputs (e.g. operation commits) from the application state outputs (e.g operation execution result) for versatility and performance reasons. Therefore, evaluating for either one is insufficient. Paxi can be used to measure both protocol and application state performance. To this end, our framework
<P>Parameter<BR>Default Value<BR>Description<BR>T<BR>60<BR>Run for T seconds<BR>N<BR>0<BR>Run for N operations (if N&gt;0)<BR>K<BR>1000<BR>Total number of keys<BR>W<BR>0.5<BR>Write ratio<BR>Concurrency<BR>1<BR>Number of concurrent clients<BR>LinearizabilityCheck<BR>true<BR>Check linearizability at the end of benchmark<BR>Distribution<BR>"uniform"<BR>Name of distribution used for key generation<BR>include uniform, normal and zipfian<BR>Min<BR>0<BR>Random: minimum key number<BR>Conflicts<BR>100<BR>Random: percentage of conflicting keys<BR>Mu<BR>0<BR>Normal: Mean<BR>Sigma<BR>60<BR>Normal: Standard Deviation<BR>Move<BR>false<BR>Normal: Moving average (mu)<BR>Speed<BR>500<BR>Normal: Moving speed in milliseconds<BR>Zipfian_s<BR>2<BR>Zipfian: s parameter<BR>Zipfian_v<BR>1<BR>Zipfian: v parameter<BR>Table 3: Benchmark parameters
<P><BR>comes with an in-memory multi-version key-value datastore that is private to every node. The datastore is used as a deterministic state machine abstraction commonly used by coordination protocols. Any other data model can be used for this purpose as long as Paxi node can query current state, submit state transform operation and generate directed acyclic graph (DAG) of past states.<BR>RESTful client. The Paxi client library uses a RESTful API to interact with any system node for read and write requests. This allows users to run any benchmark (e.g. YCSB [11]) or testing tools (e.g. Jepsen [22]) against their imple- mentation in Paxi without porting the client library to other programming languages.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Paxi Benchmark Components<BR>Benchmarker. The benchmarker component of Paxi serves as the core of our benchmarking capabilities. The bench- marker both generates tunable workloads with rich features including access locality and dynamicity and collects per- formance data for these workloads. Similar to YCSB [11], Paxi provides a simple read/write interface to interact with the client library. The workload generator reads the con- figuration to load the workload definition parameters, as summarized in Table 3.<BR>Benchmark component can generate a variety of work- loads by tuning the read-to-write ratios, creating hot ob- jects, conflicting objects and locality of access. The locality characteristic in workload is especially important in WAN distributed protocols as each region has a set of keys it is more likely to access. In order to simulate workloads with tunable access locality patterns across regions, Paxi uses a normal distribution to control the probability of generating a request on each key, and denotes a pool of K common keys with the probability function of each region, as shown in Figure 6. In other words, Paxi introduces locality to the evaluation by drawing the conflicting keys from a Normal
<P><BR>Uniform
<P>&nbsp;
<P>&nbsp;
<P><BR>0&nbsp;1&nbsp;2&nbsp;3&nbsp;&#8230;&nbsp;K<BR>Exponential
<P>&nbsp;
<P>3
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P><BR>3
<P>&nbsp;
<P>&nbsp;
<P>&nbsp;
<P><BR>0&nbsp;1&nbsp;2&nbsp;3&nbsp;&#8230;&nbsp;K
<P><BR>Figure 6: Probability distributions with total number of data records equal to K
<P>distribution N(&#181;, &#963; 2), where &#181; can be varied for different re- gions to control the locality, and &#963; is shared between regions. The locality can be visualized as the non-overlapping area under the probability density functions in Figure 6.<BR>Paxi benchmarker is capable of testing and evaluating four different aspects of coordination/replication protocols behav- ior: performance, scalability, availability, and consistency.<BR>Performance. Paxi measures performance via the latency and throughput metrics. The latency of every individual request is stored and output to a file for future analysis. Since it is important to show how a protocol perform in terms of tail latency under stress, Paxi supports this by increasing the benchmark throughput (via increasing the concurrency level of the workload generator) until the system is saturated and throughput stops increasing or latency starts to climb. The user may conduct this benchmark tier with different workloads to understand how a protocol performs or use the same workload with increasing throughput to find a bottleneck of the protocol.<BR>Scalability. One of the most desirable properties for cloud native protocols is the elastic scalability: when the applica- tion grows/shrinks, the underlining protocol should be able to adapt to load by expanding/reducing number of servers. In Paxi, we support benchmarking scalability by adding more nodes into system configuration and by increasing the size of dataset (K).<BR>Availability. High availability is an indispensable require- ment for distributed protocols, as they are expected to main- tain progress under a reasonable number of failures. While testing for availability seems straightforward, it requires la- borious manual work to simulate all combinations of failures. Many failure types are hard to produce in an uncontrolled runtime environments without utilizing third party tools specific to their operating systems. Typical examples include asymmetric network partition, out of order messages and random message drop/delay, to name only a few. Several projects have automated fault injection procedures, but with limitations. For instance, Jepsen [22] issues "tc" (traffic con- trol) commands to manipulate network packets on every node, but can only run on Linux systems. ChaosMonkey [32] is a resiliency tool that only randomly terminates virtual machine instances and containers. Paxi, being a prototyping framework, can make it easy to simulate any node or net- work failure. We provide four special commands in the Paxi client library and realize those in the networking modules:
<P>&nbsp;&nbsp;&nbsp; &#8226; Crash(t ) will &#8220;freeze&#8221; the node for t seconds.<BR>&nbsp;&nbsp;&nbsp; &#8226; Drop(i, j, t ) function drops every message send from node i to node j.<BR>&nbsp;&nbsp;&nbsp; &#8226; Slow(i, j, t ) function delays messages for a random pe- riod.<BR>&nbsp;&nbsp;&nbsp; &#8226; Flaky(i, j, t ) function drops messages by chance.
<P>Consistency. For the consistency checker component of Paxi, we implement the simple offiine read/write lineariz- ability checker from the Facebook TAO system [28]. Our linearizability checker takes a list of all the operations per record sorted by invocation time as an input. The output of the checker is a list of all anomalous reads, i.e., read oper- ations that returned results they would not be able to in a linearizable system. Our checker maintains a graph whose vertices are read or write operations, and edges are con- straints. It checks for cycles as it adds operations to the graph. If the graph is acyclic, then there exists at least one total order over the operations with all constraints satisfied. Else linearizability violation is reported.<BR>Unlike the linearizability checker, our consensus checker validates whether the consensus for every state transition has been reached among the nodes in a replicated state machine. External, client-observed, linearizability can be reached with- out having a consensus among the state machines, however, satisfying consensus is vital for consensus algorithms, such as Paxos. To test for consensus, Paxi includes a multi-version datastore as the state machine. We implement a special com- mand in client library to collect entire history of some data record Hr from every system node, then verify if all history Hr from node i shares a common prefix.
<P>8<BR>7<BR>6<BR>5<BR>4<BR>3<BR>2<BR>1<BR>0<BR>0&nbsp;2000&nbsp;4000&nbsp;6000&nbsp;8000&nbsp;10000<BR>Throughput (ops/s)<BR>allowing us to directly run Paxi benchmark against etcd without any changes.<BR>Without considering reconfiguration and recovery differ- ences, Paxos and Raft are essentially the same protocol with a single stable leader driving the command replication. As a result, they should exhibit similar performance in the normal case. We ran each system with 9 replicas within the same availability zone. For a fair comparison with Paxi, we dis- abled persistent logging and snapshots and increased the maximum number of inflight messages to 10,000 in etcd. The client request is replied only after the request is committed in Raft. In Figure 7, we show that both systems converge to<BR>Figure 7: Single leader consensus protocol imple- mented in Paxi versus etcd
<P>&nbsp;&nbsp;&nbsp; 5 EVALUATION OF THE PROTOCOLS<BR>We perform protocol evaluations using both simulated ex- periments with our model and deployed experiments with Paxi framework in LANs and WANs. In Paxi, we carry out all experiments on AWS EC2 [5] m5.large instances with 2 vCPUs and 8 GB of RAM. For WAN evaluations, we use AWS datacenter topology with the N.Virginia (VA), Ohio (OH), California (CA), Ireland (IR) and Japan (JP) regions. Our modeling effort is calibrated to CPU speed of the m5.large instances and uses the communication delays corresponding to latencies within an AWS region and across regions.<BR>In WPaxos, we limit the number of nodes that can act as leaders to just one per region; in a 9-node 3-region cluster WPaxos will have only 3 leaders. This gives WPaxos similar and comparable deployment to WanKeeper that is restricted by its design to have just a single leader in each region. In EPaxos, every node can become an opportunistic leader. Ad- ditionally, EPaxos model penalizes the message processing to account for extra resources required to compute depen- dencies and resolve conflicts. For all protocols we assume full-replication scheme in which a leader node replicates the commands to all other nodes.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Paxi Performance<BR>The Paxi framework serves the primary goal of comparing many different consensus and replication protocols against each other under the same framework with the same imple- mentation conditions. However, to show that our Paxi imple- mentation of protocols are representative of Paxos variant implementations used in other real-world production grade systems, we compared the Paxos protocol implemented in<BR>similar maximum throughput around 8000 operations per second due to the single leader bottleneck, but Paxi exhibits lower latency when the system is not saturated. The latency difference is likely due to etcd&#8217;s use of http for inter-node communication instead of TCP sockets and differences in message serialization.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Protocol Comparisons in LANs<BR>We perform a set of experiments studying the performance of the consensus protocols in LANs. Figure 8 shows the re- sults obtained from our model for LANs. We present the Paxi evaluation in Figure 9 using uniformly random work- load against Paxi&#8217;s internal key-value store on 1000 objects with 50% read operations. Both figures show how latency of protocols change as the throughput increases.<BR>Single leader bottleneck. Both the model and Paxi eval- uations point to the scalability limitation of the single leader solutions. Several papers [20, 27, 29, 39] observed&#8212;but failed to analyze further&#8212; that the single leader becomes a bottle- neck in Paxos protocol, having to do with sending/receiving N messages and the CPU utilization at the leader. The bot- tleneck is due to the leader getting overwhelmed with the messages it needs to process for each round. For example, from our modeling effort, we estimate a Paxos leader to han-<BR>dle N incoming messages, one outgoing message and one outgoing broadcast2, for a total of N + 2 messages per round. At the same time, the follower nodes only process 2 mes- sages per round, assuming phase-3 is piggybacked to the next phase-2 message. For a cluster of 9 nodes, this trans- lates in 11 messages per round at the leader against just 2 messages at the replicas, making the leader the bottleneck.<BR>Multi-leader protocols, such as WPaxos and WanKeeper perform better than single leader protocols. Their advantage comes from the ability to process commands for indepen- dent objects in parallel at different nodes. While multi leader<BR>Paxi against Raft [33] implemented in etcd [15]. The etcd&nbsp;&nbsp;<BR>project provides a standalone sample code in Go1 that uses Raft and exposes a simple REST API for a key-value store,
<P>1https://github.com/etcd-io/etcd/tree/master/contrib/raftexample<BR>2For the broadcast, the CPU is involved for serialization just once, and then the message is send to the other nodes individually by the NIC (amounting to N-1 transmission). Since NIC is much faster than the CPU processing, the NIC cost becomes negligible for small messages.
<P><BR>&nbsp;MultiPaxos
<P>6
<P>5
<P>4
<P>3
<P>2
<P>1
<P>0<BR>EPaxos
<P>&nbsp;
<P>200<BR>175<BR>150<BR>125<BR>100<BR>75<BR>50<BR>25<BR>0&nbsp;2000&nbsp;4000&nbsp;6000&nbsp;8000&nbsp; 10000&nbsp; 12000&nbsp; 14000<BR>Throughput (rounds/sec)
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Max throughput<BR>0<BR>0&nbsp;5000&nbsp;10000&nbsp;15000&nbsp;20000&nbsp;25000<BR>Aggregate Throughput (rounds/sec)
<P><BR>MultiPaxos
<P><BR>EPaxos<BR>Figure 10: Modeled performance in WANs
<P>2.00
<P>1.75
<P>1.50
<P>1.25
<P>1.00
<P>0.75
<P>0.50
<P>0.25
<P>0.00
<P><BR>0&nbsp;1000&nbsp;2000&nbsp;3000&nbsp;4000&nbsp;5000&nbsp;6000&nbsp;7000&nbsp;8000<BR>Throughput (rounds/sec)
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Latency at lower throughput
<P>The figures also show that the 3-leader WanKeeper per- forms better than the 3-leader WPaxos. By taking a hierarchi- cal approach, WanKeeper reduces the number of messages each leader processes, alleviating the leader bottleneck fur- ther.<BR>Leaderless solutions suffer from conflict. EPaxos is an example of a leaderless consensus, where each node can act as an opportunistic leader for commands, as long as there are no collisions between the commands from different nodes. Command conflicts present a big problem for such<BR>Figure 8: Modeled performance in LANs
<P>8<BR>7
<P>6<BR>5<BR>4<BR>3<BR>2<BR>1<BR>0<BR>0&nbsp;5000&nbsp;10000&nbsp;15000<BR>Throughput (op/s)
<P>Figure 9: Experimental performance in LAN
<P><BR>solutions can take advantage of the capacity left unused at the single-leader protocol replicas, they still suffer from the relatively high number of messages to process, so they don&#8217;t scale linearly: the 3-leader WPaxos does not perform 3 times better than Paxos. Our model showed a roughly 55% improvement in maximum throughput in WPaxos, which is consistent with our experimental observations in Paxi.<BR>opportunistic approaches. The ability to identify and resolve conflicts increases the message size and processing capacity needed for messages. The conflict resolution mechanism also requires an additional phase, making conflicting rounds default to a two-phase Paxos implementation. These result in EPaxos performing the worst in Paxi LAN experiments. It is worth mentioning that EPaxos shows better throughput (but not latency) than Paxos in our model even with 100% conflict, since it does not have a single-leader bottleneck problem. However, when we add message processing penalty to account for extra weight of finding and resolving conflict, EPaxos&#8217; performance degrades greatly.<BR>Small flexible quorums benefit. Our model results show a modest average latency improvement of just 0.03 ms due to using single-leader flexible quorums solution (FPaxos). Our Paxi evaluation shows a slightly bigger improvement for going from Paxos to FPaxos.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.3 Protocol Comparison in WANs<BR>In this set of experiments, we compare protocols deployed across the WANs. WAN deployments present many new chal- lenges for consensus and replication protocols originating from large and non-uniform latencies between datacenters and nodes: the inter-datacenter latency can vary from a few to a few hundred milliseconds.<BR>In Figure 10 we show the modeled throughput and latency results for different consensus algorithms.
<P>Unlike LAN models, wide area network models differ greatly in the average latency, with more than a 100 ms differ- ence in latency between the slowest (Paxos) and the fastest (WPaxos) protocols. Flexible quorums make a great differ- ence in latency in this environment &#8211; they allow WPaxos to commit many commands with near local latency, and reduce the overall quorum wait time for FPaxos.<BR>To combat the adversarial effects of WAN, many protocols try to optimize for various common aspects of operation in such environment: some assume the objects over which a consensus is needed get rarely accessed from many places<BR>WPaxos (fz=0)&nbsp; WPaxos(fz=1)&nbsp; WanKeeper EPaxos&nbsp; VPaxos&nbsp; Paxos<BR>30<BR>25<BR>20<BR>15<BR>10<BR>5<BR>0<BR>0%&nbsp;20%&nbsp;40%&nbsp;60%&nbsp;80%&nbsp;100%<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) Virginia<BR>at once, while others may go even further and assume a strict placement of the objects in certain regions for better locality of access. When these assumptions break, algorithms&#8217; performance often degrades. We designed our experiments to test these conflict and locality assumptions.<BR>Conflict Experiments. We define conflict as commands accessing the same object from different region. To study the protocol performance under the conflict workload, we create one &#8220;hot&#8221; conflicting key that will be accessed by all clients. We control the ratio of conflicting requests and make every conflicting request perform an operation against the designated conflict objects. For instance, 20% conflict mean that 20% of requests issues by the benchmarker clients target the same object.<BR>The results of our conflict experiments, shown in Figure 11, reveal the following observations:<BR>&nbsp;&nbsp;&nbsp; (1) The protocols that does not tolerate entire region fail- ure (WPaxos fz = 0, WanKeeper, VPaxos) exhibit the same performance in every location. This is because when fz = 0, the non-interfering commands are able to commit by a quo-<BR>30<BR>25<BR>20<BR>15<BR>10<BR>5<BR>0
<P><BR>100<BR>80<BR>60<BR>40<BR>20<BR>0
<P><BR>0%&nbsp;20%&nbsp;40%&nbsp;60%&nbsp;80%&nbsp;100%<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Ohio
<P>0%&nbsp;20%&nbsp;40%&nbsp;60%&nbsp;80%&nbsp;100%<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) California<BR>rum in the same region. The interfering command is for- warded to the object&#8217;s current leader region.<BR>&nbsp;&nbsp;&nbsp; (2) When the protocol embraces a leader/owner concept, the leader&#8217;s region of the conflicting object is at an advantage and experience optimal latency for its local quorum. In this case, the Ohio region is the leader of conflicting object, and thus have a low steady latency. On the contrary, EPaxos, a leaderless approach, experiences latency due to interfering commands even in the Ohio region.<BR>&nbsp;&nbsp;&nbsp; (3) Among the protocols which can tolerate entire region failure, including Paxos, EPaxos, and WPaxos fz = 1, WPaxos performs best until 100% interfering commands where it starts to provide the same latency as Paxos.<BR>&nbsp;&nbsp;&nbsp; (4) Unlike other protocols, EPaxos average latency is a non- linear function of conflicting ratio. This is because even with low conflict rate like 20%, the previous conflicting command may not have been committed yet when the new requests arrive, leading to more rounds of RTT delays to resolve the conflict. The situation gets worse when the region is far from other regions, such as California region in our experiment.<BR>Figure 11: Comparison of protocols under a conflict workload
<P><BR>20000
<P>18000
<P>16000
<P>14000
<P>12000
<P>10000
<P>8000<BR>0&nbsp;20&nbsp;40&nbsp;60&nbsp;80&nbsp;100<BR>Conflict %
<P>Figure 12: Model of EPaxos maximum throughput in 5-nodes/regions deployment
<P>Our model also suggests the maximum throughput of EPaxos is severely affected by the conflict ratio, with as much
<P>300<BR>250<BR>200<BR>150<BR>100<BR>50
<P><BR>Name<BR>Protocols<BR>L<BR>Leaders<BR>EPaxos. WPaxos<BR>c<BR>Conflicts<BR>Generalized Paxos, EPaxos<BR>Q<BR>Quorum<BR>FPaxos, WPaxos<BR>l<BR>Locality<BR>VPaxos, WPaxos, WanKeeper<BR>Table 4: Parameters explored<BR>0
<P>&nbsp;
<P>&nbsp;
<P>1.0
<P>0.8
<P>0.6
<P>0.4
<P>0.2
<P>0.0
<P>T&nbsp;C&nbsp;O&nbsp;V&nbsp;I
<P>&nbsp;&nbsp;&nbsp; (a) Average latency per region<BR>LoFDliWy WorkloDd
<P>0&nbsp;50&nbsp;100&nbsp;150&nbsp;200&nbsp;250&nbsp;300<BR>LDWenFy (PV)<BR>&nbsp;&nbsp;&nbsp; (b) Latency distribution
<P><BR>almost identical CDFs in Figure 6.4. When we look at all re- quests globally, WanKeeper experience more WAN latencies than WPaxos and VPaxos.<BR>&nbsp;&nbsp;&nbsp; 6 DISCUSSION<BR>In this section, we examine the evaluation results from Sec- tion 5 and distill those performance results into simple through- put and latency formulas for uniformly distributed work- loads. These formulas present a simple unified theory of strongly-consistent replication throughput in Section 6.1, and latency in Section 6.2. Finally, in Section 6.3, we demon- strate how these formulas allow us to perform back-of-the- envelope performance forecasting of the protocols.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Load and Capacity<BR>Figure 13: WPaxos, WanKeeper and Vertical Paxos in WAN with locality workload
<P>&nbsp;
<P><BR>as 40% degradation in capacity between no conflict and full conflict scenarios, as illustrated in Figure 12.<BR>Locality Experiments. For these experiments, we study the performance of protocols that take advantage of locality (WPaxos, WanKeeper, and our augmented version of Verti- cal Paxos) across a WAN with a locality workload. We use a locality workload with object access locality controlled by a normal distribution, as described earlier. We start the experi- ment by initially placing all objects in the Ohio region and then running the locality workload for 60 seconds. All three protocols are deployed with fault-tolerance level fz = 0 and the simple three-consecutive access policy to adapt to opti- mal performance. Figure 13a compares per-region latency, while Figure 13b shows a CDF for operation latencies.<BR>The capacity of the system Cap(S) is the highest request pro- cessing rate that system S can handle. As we have observed from Paxi experiments, the capacity of a given protocol is determined by the busiest node in the system with load L, such that<BR>Cap(S) = 1/L(S).&nbsp;(1)<BR>Definition 6.1. Load of the system L(S) is the minimum number of operations invoked on the busiest node for every<BR>request on average, where an operation is the work required to handle round-trip communication between any two nodes. For example, for every quorum access, the leader must han- dle Q number of outgoing and incoming messages, which correspond to a total of Q operations. Whereas in single- leader protocols, the busiest node is typically the leader, multi-leader algorithms have more than one busiest node. In such protocols, nodes that have the leader capabilities tend to be under greater load than others.<BR>In WanKeeper, the Ohio region is the higher level region&nbsp;1&nbsp;1<BR>that will keep the tokens for any objects shares access from another region. Thus, Ohio shows the best average latency<BR>L(S) =&nbsp; (1 + c)(Q &#8722; 1) + (1 &#8722; )(1 + c)&nbsp;(2)<BR>L&nbsp;L<BR>(1 + c)(Q + L &#8722; 2)<BR>close to local area RTT, at the cost of suffering at the other<BR>=&nbsp;(3)<BR>L<BR>two regions. WPaxos and VPaxos are more balanced and share very similar performance in this deployment. This is because when stabilized, both protocols balance the num- ber objects in each region in the same way, as confirmed by<BR>where 0 &#8804; c &#8804; 1 is probability of conflicting operations, Q is the quorum size chosen by leader, and L is the number of operation leaders. The derivation is as follows. There is a 1/L chance the node is the leader of a request, which induces one
<P>&nbsp;
<P>Figure 14: Flowchart for identifying the suitable consensus protocol
<P>round of quorum access with Q &#8722;1 communication, plus extra round of quorum access if there is conflict. The probability of the node being a follower is 1 &#8722; 1/L, where it only handles one received message in the best case. From the simplified<BR>form, it is easy to see that the protocols that utilize more leaders reduce the load (and hence increase capacity) because the user requests are shared between multiple leaders. On the other hand, this also increases the chance of extra round to resolve any conflicts between leaders. Equation 3 uses thrifty optimization where leader only communicates with minimum number of nodes to reach the quorum of size Q.<BR>In the general case, however, the leader communicates with all N &#8722; 1 followers, making Q = N &#8722; 1 for the purpose of this equation.<BR>In the below equations, we present the simplified form of load for three protocols, and calculate the result for N = 9 nodes. The protocols perform better as the load decreases.
<P>L(Paxos) = &#8970; N &#8971;&nbsp;= 4&nbsp;(4)<BR>L(EPaxos) = (1 + c)(&#8970; N &#8971; + N &#8722; 1)/N&nbsp;= 3 (1 + c) (5)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.2 Latency<BR>The expected latency of a protocol in WAN is determined by the location, minimum quorum latency DQ , and the locality l of the requests.
<P>Latency(S) = (1 + c) &#8727; ((1 &#8722; l) &#8727; (DL + DQ ) + l &#8727; DQ ) (7)<BR>where DL is the distance from where the request is gener- ated and the operation leader. When a request is local with probability l, it only requires time of the quorum access with closest neighbors DQ . For non-local requests occurred with probability of 1 &#8722; l, a round trip of distance to leader DL also contributes to the average latency. For EPaxos l = 1 but<BR>c is workload specific, in contrast for the other consensus protocols we study, we have c = 0 and l is workload specific.
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6.3 Comparing the Protocols<BR>By using the two formulas 3 and 7 we present in the previous subsections, we discuss how these protocols compare with each other and how we can provide back-of-the-envelope performance forecasting of these protocols.<BR>L(WPaxos) = (&nbsp; + L &#8722; 2)/L&nbsp;=<BR>(6)<BR>EPaxos and Generalized Paxos try to make single leader
<P><BR>L&nbsp;3&nbsp;Paxos more scalable by extending the leadership to every<BR>In the single leader Paxos protocol (Equation 4) with N nodes, c = 0 as the conflicting operation is serialized by the single leader, and L = 1, and quorum size Q = &#8970;N /2&#8971; +<BR>1. In contrast, EPaxos (Equation 5) uses every node as an<BR>opportunistic leader, and uses L = N . WPaxos (Equation 6) utilizes a flexible grid quorum system, such that every leader only accesses its own phase-2 quorum with size N /L.<BR>In a 3 &#215; 3 grid, the load of WPaxos is only 4/3, giving it the<BR>smallest load (and as a result the highest capacity) among<BR>the three consensus protocols.<BR>replica therefore sharing the load; this increases L to its max- imum value of all nodes in the system and reduces L(S). But it comes with a complication: any conflicting commands from two replicas require extra round of quorum acknowl- edgement for order resolution before the requests can be<BR>executed and replied. This extra round puts more load to the system, reducing throughput and increasing latency. Our evaluations show that the problem becomes even worse in WANs: since requests take much longer time to finish in WANs, that also contributes to an increase in the probability
<P>of contention. In the worst case, even with 25% of conflicting requests, system can experience c = 100% load.<BR>Flexible-Paxos and WPaxos benefits from flexible quorums such that both Q and DQ are reduced for phase-2.<BR>The three WAN protocols we evaluated in this work ex- ploit the locality l in the workload to optimize latency for wide area. When the locality is static, and an optimal policy is used for placing the data close to most requests, these protocols will experience the same WAN latency.<BR>In Table 4, we show the parameters each protocol aims to explore. Given that each protocol emphasizes a couple of these parameters and trades them off with others, there is no one protocol that fits all needs/expectations. Our results and formulas are also useful for deciding which category of Paxos protocols would be optimal under given deployment conditions. In Figure 14, we give a flowchart to serve as a guideline to identify which consensus protocol would be suitable for a given deployment environment.<BR>&nbsp;&nbsp;&nbsp; 7 CONCLUDING REMARKS<BR>We presented a two pronged approach to analyze the strongly- consistent replication protocols. We distilled the throughput and latency performance to simple formulas that generalize over Paxos protocols, uniting them, as well as emphasizing the different design decisions and tradeoffs they take.<BR>We anticipate that the simple exposition and analysis we provide will lead the way to the development of new proto- cols, especially WAN coordination protocols. The unbalanced topology with respect to obtaining a quorum causes compli- cations in WAN deployments, and achieving good locality as well as load balancing remains an open problem for efficient strongly-consistent WAN replication. In addition, as part of future work, we aim to extend our analytical model to cover replication protocols with relaxed consistency guarantees, such as bounded-consistency and session consistency.<BR>ACKNOWLEDGMENTS<BR>This project is in part sponsored by the National Science Foundation (NSF) under award number CNS-1527629 and XPS-1533870.<BR>REFERENCES<BR>&nbsp;&nbsp;&nbsp; [1] Ailidani Ailijiang, Aleksey Charapko, Murat Demirbas, and Tevfik Kosar. 2017. Multileader WAN Paxos: Ruling the Archipelago with Fast Consensus. arXiv preprint arXiv:1703.08905 (2017).<BR>&nbsp;&nbsp;&nbsp; [2] &nbsp;Ailidani Ailijiang, Aleksey Charapko, Murat Demirbas, Bekir Oguz Turkkan, and Tevfik Kosar. 2017. Efficient Distributed Coordination at WAN-scale. In Distributed Computing Systems (ICDCS), 2017 37th International Conference on. IEEE.<BR>&nbsp;&nbsp;&nbsp; [3] &nbsp;Arnold O. Allen. 2014. Probability, statistics, and queueing theory. Academic Press.<BR>&nbsp;&nbsp;&nbsp; [4] Deniz Altinbuken and Emin Gun Sirer. 2012. Commodifying replicated state machines with openreplica. Technical Report.<BR>&nbsp;&nbsp;&nbsp; [5] Amazon Inc. 2008. El