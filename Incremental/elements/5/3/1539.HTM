ABSTRACT<BR>Strongly consistent replication helps keep application logic simple and provides significant benefits for correctness and manageabil- ity. Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and per- formance. To alleviate the leader bottleneck in strongly-consistent replication protocols, we introduce Pig, an in-protocol communi- cation aggregation and piggybacking technique. Pig employs ran- domly selected nodes from follower subgroups to relay the leader&#8217;s message to the rest of the followers in the subgroup, and to perform in-network aggregation of acknowledgments back from these fol- lowers. By randomly alternating the relay nodes across replication operations, Pig shields the relay nodes as well as the leader from becoming hotspots and improves throughput scalability.<BR>We showcase Pig in the context of classical Paxos protocols em- ployed for strongly consistent replication by many cloud computing services and databases. We implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols under various workloads over clusters of size 5 to 25 nodes. We show that the aggregation at the relay has little latency overhead, and PigPaxos can provide more than 3 folds improved throughput over Paxos and EPaxos with little latency deterioration. We support our experimental observations with the analytical modeling of the bottlenecks and show that the communication bottlenecks are minimized when employing only one randomly rotating relay node.<BR>ACM Reference Format:<BR>Aleksey Charapko, Ailidani Ailijiang, and Murat Demirbas. 2021. PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus. In Proceedings of the 2021 International Conference on Management of Data (SIGMOD &#8217;21), June 20&#8211;25, 2021, Virtual Event, China. ACM, New York, NY, USA, 13 pages. <A href="https://doi.org/10.1145/3448016.3452834">https://doi.org/10.1145/3448016.3452834</A><BR>1&nbsp;INTRODUCTION<BR>Strongly consistent replication ensures that each operation accesses the latest committed database state, providing the illusion of a single copy centralized system [27]. This helps to keep application logic simple and provides significant benefits for correctness and man- ageability. Without strong consistency, the developers are forced to</P>
<P>&#8727;Work partially completed at the University at Buffalo, SUNY.</P>
<P>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from <A href="mailto:permissions@acm.org">permissions@acm.org</A>.<BR>SIGMOD &#8217;21, June 20&#8211;25, 2021, Virtual Event, China<BR>&#169; 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8343-1/21/06. . . $15.00<BR><A href="https://doi.org/10.1145/3448016.3452834">https://doi.org/10.1145/3448016.3452834</A><BR>write a lot of peripheral code to handle side-effects and corner-cases arising from concurrent eventually-consistent operations. More- over, an unanticipated combination of faults conspiring with the concurrency of operations is likely to leave those systems in states that are hard to debug and repair [24, 32, 37].<BR>Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and per- formance. In strongly consistent replication, all the communica- tion drains through a single node &#8211; the primary (a.k.a. the leader), which constitutes a throughput bottleneck [2]. The leader performs a disproportionately large amount of work compared to its &#55349;&#56385; 1 followers. For each consensus instance, the followers receive one message from the leader and send one message back. In contrast, the leader needs to send &#55349;&#56385; 1 messages to the followers, and re- ceive at least a quorum of messages back from these followers, not counting the interaction with clients. For instance, in a cluster of &#55349;&#56385; = 5 nodes, the leader handles up to 4 times more messages than a follower. This puts a practical limit on the cluster size and prevents adding mode replicas to scale weaker-consistency reads or improve fault-tolerance. Therefore, scaling strongly consistent systems requires reducing the disproportionate load on the leader. To address the single leader bottleneck and improve the through- put of strongly-consistent replication protocols, we introduce Pig, an in-protocol communication aggregation and piggybacking tech- nique that aims to decouple the decision-making from the commu-<BR>nication at the leader.<BR>Pig replaces the direct communication between the leader and followers with a relay-/aggregate-based message flow. In particular, Pig employs randomly selected nodes from one or more follower subgroups to relay the leader&#8217;s message to the rest of the follow- ers in each subgroup, and to perform an aggregation of responses back from these followers. This overlay communication pattern allows the leader to only exchange messages with a small num- ber of relay nodes. Moreover, the random alternation of the relay nodes across replication operations shields these relay nodes from becoming hotspots themselves: the extra traffic load a relay node incurs in one round is offset in consecutive rounds when the node no longer serves as a relay. These two properties combined improve the throughput scalability of the system.<BR>Although aggregation-based approaches have been known and employed in the context of weak-consistency replication proto- cols [47], Pig demonstrates how they can be effectively applied to and integrated with strongly consistent consensus-based replica- tion protocols. In particular, our experiments and analysis show that the rotation of the relay nodes in the Pig provides the most benefit for reducing the bottlenecks in strongly-consistent replicated state machines and improving throughput. Another novel and somewhat counterintuitive finding from our experiments and analysis is that