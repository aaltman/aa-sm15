Abstract<BR>Many distributed systems/databases rely on Paxos for pro- viding linearizable reads. Linearizable reads in Paxos are achieved either through running a full read round with fol- lowers, or via reading from a stable leader which holds leases on followers. We introduce a third method for performing linearizable reads by eschewing the leader and only reading from a quorum of followers. For guaranteeing linearizability, a bare quorum read is insufficient and it needs to be amended with a rinse phase to account for pending update operations. We present our Paxos Quorum Read (PQR) protocol that im- plements this. Our evaluations show that PQR significantly improves throughput compared to the other methods. The evaluations also show that PQR achieves comparable latency to the read from stable Paxos leader optimization.</P>
<P>&nbsp;&nbsp;&nbsp; 1 Introduction<BR>The Paxos consensus protocol [15] is used for implement- ing a fault-tolerant replicated state machine (RSM), where each node executes commands in the same order and main- tain identical states. Many distributed systems use Paxos as the backbone for strongly consistent distributed replica- tion [7, 8, 10, 12, 26]. To guarantee strong constraints on the safe interleaving of updates and provide ease of reason- ing/development, these distributed systems often provide lin- earizability to prohibit stale reads: once an operation is com- plete, every read must see that state or some later state. In other words, every read sees some current state between invo- cation and completion, but not a state prior to the read.<BR>A straightforward way to serve a linearizable read oper- ation in Paxos is to run this command through a majority of followers. But this is a costly solution because executing a Paxos round for a command that does not change state is wasteful. Furthermore, read operations occur more frequently than write operations, and read-to-write ratios greater than 100:1 are not uncommon in the industry [21].<BR>To reduce the cost of linearizable read operations, read from the leader optimization is performed [9, 20]. This approach<BR>requires the leader to be stable. To assert this condition, the leader employs leases with the follower nodes which prevents the election of another leader during the lease period.<BR>Both of these methods involve the leader, which is already burdened by serving every write operation [2]. Making the leader responsible for all read traffic puts even more strain on it, while the follower replicas perform no client-facing work and exist solely for the purpose of fault-tolerance and redundancy.<BR>Contributions. We present a general client-driven, strongly consistent quorum read protocol, called Paxos Quo- rum Reads (PQR). PQR relieves the Paxos leader from serv- ing read operations and achieves more uniform resource uti- lization among the nodes in the system. PQR also offloads the burden of orchestrating the read to the client, and provides better scalability for high throughput read/write workloads in Paxos-based databases.<BR>Providing linearizable quorum reads in Paxos is not an easy problem. The naive approach of simply reading the commit- ted value from the majority of followers may result in stale reads and read-your-write violations: A client that receives an ack for a successful write may not see its own write upon a subsequent read from a quorum of followers. This is because the followers do not have the most up-to-date information about the committed writes. The followers know only about the dirty writes that they "accept", but not about the "commit" finalized by the leader upon seeing a quorum of accepts.<BR>In order to avoid this problem in PQR, after reading ac- cepted values from a quorum of follower replicas, the client is then required to rinse a dirty read by waiting for the highest "accepted" slot to become "committed". While this solution might appear slower than a point-read from the leader node, and not very advantageous at a first glance, it provides value by alleviating the need for leader-leases and by ensuring a more even load distribution among the nodes in the cluster for intensive write and read workloads. Moreover, as we show in Section 4.1, the rinse phase becomes unnecessary and is omitted in many cases.<BR>Our prototype implementation of PQR achieves up to 12%